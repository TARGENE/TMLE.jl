mutable struct Fluctuation <: MLJBase.Supervised
    Ψ::StatisticalCMCompositeEstimand
    initial_factors::MLCMRelevantFactors
    tol::Union{Nothing, Float64}
    max_iter::Int
    ps_lowerbound::Float64
    weighted::Bool
    cache::Bool
end

Fluctuation(Ψ, initial_factors; tol=nothing, max_iter=1, ps_lowerbound=1e-8, weighted=false, cache=false) =
    Fluctuation(Ψ, initial_factors, tol, max_iter, ps_lowerbound, weighted, cache)

one_dimensional_path(target_scitype::Type{T}) where T <: AbstractVector{<:MLJBase.Continuous} = LinearRegressor(fit_intercept=false, offsetcol = :offset)
one_dimensional_path(target_scitype::Type{T}) where T <: AbstractVector{<:Finite} = LinearBinaryClassifier(fit_intercept=false, offsetcol = :offset)

same_type_nt(covariate::AbstractVector{T}, offset::AbstractVector{T}) where T = (covariate=covariate, offset=offset)

"""

The GLM models require inputs of the same type, which sometimes is not the case
"""
same_type_nt(covariate::AbstractVector{T1}, offset::AbstractVector{T2}) where {T1, T2} = 
    (covariate=covariate, offset=convert(Vector{T1}, offset))

function fluctuation_input(covariate, ŷ)
    offset = compute_offset(ŷ)
    return same_type_nt(covariate, offset)
end

hasconverged(gradient, tol) = abs(mean(gradient)) < tol

"""
    hasconverged(gradient, tol::Nothing)

Tolerance defaults to `1/n_samples`
"""
hasconverged(gradient, tol::Nothing) = hasconverged(gradient, 1/length(gradient))

"""
    initialize_observed_cache(model, X, y)

For the observed data, we store:

- The clever covariate
- The weights
- The observed predictions
- The observed outcome in floating point representation
"""
function initialize_observed_cache(model, X, y)
    Q⁰ = model.initial_factors.outcome_mean
    G⁰ = model.initial_factors.propensity_score
    H, w = clever_covariate_and_weights(
        model.Ψ, G⁰, X;
        ps_lowerbound=model.ps_lowerbound,
        weighted_fluctuation=model.weighted
    )
    ŷ = MLJBase.predict(Q⁰, X)
    return Dict{Symbol, Any}(:H => H, :w => w, :ŷ => ŷ, :y => float(y))
end

"""
    initialize_counterfactual_cache(model, X)

For each counterfactual generated by the indicator functions, we store:

- The counterfactual predictions
- The counterfactual sign
- The counterfactual clever covariate

These are used to evaluate the gradient and estimate
"""
function initialize_counterfactual_cache(model, X)
    Q⁰ = model.initial_factors.outcome_mean
    G⁰ = model.initial_factors.propensity_score
    Ψ = model.Ψ

    counterfactual_cache = (predictions=[], signs=[], covariates=[], weights=[])
    X = Tables.columntable(X)
    Ttemplate = selectcols(X, treatments(Ψ))
    for (vals, sign) in indicator_fns(Ψ)
        T_ct = counterfactualTreatment(vals, Ttemplate)
        X_ct = merge(X, T_ct)
        covariates_ct, w_ct = clever_covariate_and_weights(Ψ, 
            G⁰,
            X_ct; 
            ps_lowerbound=model.ps_lowerbound, 
            weighted_fluctuation=model.weighted
        )
        predictions_ct = MLJBase.predict(Q⁰, X_ct)
        push!(
            counterfactual_cache.predictions, 
            predictions_ct
        )
        push!(
            counterfactual_cache.signs, 
            sign
        )
        push!(
            counterfactual_cache.covariates, 
            covariates_ct
        )
        push!(
            counterfactual_cache.weights, 
            w_ct
        )
    end
    return counterfactual_cache
end

function compute_counterfactual_aggregate!(counterfactual_cache, Q)
    ct_aggregate = zeros(length(first(counterfactual_cache.predictions)))
    for (idx, (ct_ŷ, sign, ct_covariates)) in enumerate(zip(
            counterfactual_cache.predictions, 
            counterfactual_cache.signs, 
            counterfactual_cache.covariates
        ))
        # Compute new counterfactual predictions
        Xfluct = fluctuation_input(ct_covariates, ct_ŷ)
        new_ct_ŷ = MLJBase.predict(Q, Xfluct)
        # Update the counterfactual aggregate for this step
        ct_aggregate .+= sign .* expected_value(new_ct_ŷ)
        # Update the cache with the new counterfactual predictions
        counterfactual_cache.predictions[idx] = new_ct_ŷ
    end
    return ct_aggregate
end

function compute_gradient_and_estimate_from_caches!(
    observed_cache, 
    counterfactual_cache, 
    Q, 
    Xfluct
    )
    # Update predictions
    observed_cache[:ŷ] = MLJBase.predict(Q, Xfluct)
    # Compute gradient
    Ey = expected_value(observed_cache[:ŷ])
    ct_aggregate = compute_counterfactual_aggregate!(counterfactual_cache, Q)
    Ψ̂ = plugin_estimate(ct_aggregate)
    gradient = ∇YX(observed_cache[:H], observed_cache[:y], Ey, observed_cache[:w]) .+ ∇W(ct_aggregate, Ψ̂)
    return gradient, Ψ̂
end

function update_report!(report, Ψ̂, gradient, epsilon)
    push!(report.estimates, Ψ̂)
    push!(report.gradients, gradient)
    push!(report.epsilons, epsilon)
end

"""
    MLJBase.fit(model::Fluctuation, verbosity, X, y)

Iteratively fits a one dimensional path through the previously fitted model (initially the untargeted factors) 
for at most `model.max_iter` or until convergence. Convergence is reached when the mean of the gradient is below `model.tol`. 
If `model.tol=nothing` then the tolerance is set to `1/n_samples`.

# Mathematical aspects

For binary outcomes this is given by:

ϵ = argmin(ϵ) ∑ logit(Q(T, W)) + ϵ ⋅ H(T, W)

For continuous outcomes this is given by:

ϵ = argmin(ϵ) ∑ Q(T, W) + ϵ ⋅ H(T, W)

where H(T, W) is the clever covariate.

# Technical aspects

Both a `counterfactual_cache` and an `observed_cache` are initialized and updated to save computations. 
The former stores the counterfactual predictions, signs and covariates. 
The latter stores the clever covariate, weights, predictions and the outcome in floating point representation.
"""
function MLJBase.fit(model::Fluctuation, verbosity, X, y)
    Q = model.initial_factors.outcome_mean
    fluctuation_model = one_dimensional_path(scitype(y))
    observed_cache = initialize_observed_cache(model, X, y)
    counterfactual_cache = initialize_counterfactual_cache(model, X)
    report = (estimates = [], gradients = [], epsilons = [])
    machines = []
    for iter in 1:model.max_iter
        verbosity >= 0 && @info(string("TMLE step: ", iter, "."))
        # Fit new fluctuation using observed data
        Xfluct = fluctuation_input(observed_cache[:H], observed_cache[:ŷ])
        Q = machine(
            fluctuation_model, 
            Xfluct, 
            y,
            observed_cache[:w],
            cache=model.cache
        )
        fit!(Q, verbosity=verbosity-1)
        push!(machines, Q)
        # Compute the estimate, gradient and update caches
        gradient, Ψ̂ = compute_gradient_and_estimate_from_caches!(
            observed_cache, 
            counterfactual_cache, 
            Q, 
            Xfluct
        )
        update_report!(report, Ψ̂, gradient, fitted_params(Q).coef)
        if hasconverged(gradient, model.tol)
            verbosity >= 0 && @info("Convergence criterion reached.")
            return machines, nothing, report
        end
    end
    verbosity >= 0 && @info("Convergence criterion not reached.")
    return machines, nothing, report
end

"""
    MLJBase.predict(model::Fluctuation, machines, X)

Generates initial predictions and iteratively predicts from the fitted fluctuations.
"""
function MLJBase.predict(model::Fluctuation, machines, X) 
    covariate, _ = clever_covariate_and_weights(
        model.Ψ, model.initial_factors.propensity_score, X;
        ps_lowerbound=model.ps_lowerbound,
        weighted_fluctuation=model.weighted
    )
    ŷ = MLJBase.predict(model.initial_factors.outcome_mean, X)
    for mach in machines
        Xfluct = fluctuation_input(covariate, ŷ)
        ŷ = MLJBase.predict(mach, Xfluct)
    end
    return ŷ
end