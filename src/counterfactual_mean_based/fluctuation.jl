mutable struct Fluctuation <: MLJBase.Supervised
    Ψ::StatisticalCMCompositeEstimand
    initial_factors::MLCMRelevantFactors
    tol::Union{Nothing, Float64}
    max_iter::Int
    ps_lowerbound::Float64
    weighted::Bool
    cache::Bool
    prevalence_weights::Union{Nothing, Vector{Float64}}
end

Fluctuation(Ψ, initial_factors; tol=nothing, max_iter=1, ps_lowerbound=1e-8, weighted=false, cache=false, prevalence_weights=nothing) =
    Fluctuation(Ψ, initial_factors, tol, max_iter, ps_lowerbound, weighted, cache, prevalence_weights)

one_dimensional_path(target_scitype::Type{T}) where T <: AbstractVector{<:MLJBase.Continuous} = LinearRegressor(fit_intercept=false, offsetcol = :offset)
one_dimensional_path(target_scitype::Type{T}) where T <: AbstractVector{<:Finite} = LinearBinaryClassifier(fit_intercept=false, offsetcol = :offset)

same_type_df(covariate::AbstractVector{T}, offset::AbstractVector{T}) where T = DataFrame(covariate=covariate, offset=offset)

"""

The GLM models require inputs of the same type, which sometimes is not the case
"""
same_type_df(covariate::AbstractVector{T1}, offset::AbstractVector{T2}) where {T1, T2} = 
    DataFrame(covariate=covariate, offset=convert(Vector{T1}, offset))

function fluctuation_input(covariate, ŷ)
    offset = compute_offset(ŷ)
    return same_type_df(covariate, offset)
end

hasconverged(gradient, tol) = abs(mean(gradient)) < tol

"""
    hasconverged(gradient, tol::Nothing)

Tolerance defaults to `1/n_samples`
"""
hasconverged(gradient, tol::Nothing) = hasconverged(gradient, 1/length(gradient))

"""
    initialize_observed_cache(model, X, y)

For the observed data, we store:

- The clever covariate
- The weights
- The observed predictions
- The observed outcome in floating point representation

If prevalence weights are provided, they are applied to the weights and normalized.
"""
function initialize_observed_cache(model, X, y)
    Q⁰ = model.initial_factors.outcome_mean
    G⁰ = model.initial_factors.propensity_score
    H, w = clever_covariate_and_weights(
        model.Ψ, G⁰, X;
        ps_lowerbound=model.ps_lowerbound,
        weighted_fluctuation=model.weighted
    )
    ŷ = MLJBase.predict(Q⁰, X)
    return Dict{Symbol, Any}(:H => H, :w => w, :ŷ => ŷ, :y => float(y))
end

"""
    initialize_counterfactual_cache(model, X)

For each counterfactual generated by the indicator functions, we store:

- The counterfactual predictions
- The counterfactual sign
- The counterfactual clever covariate

These are used to evaluate the gradient and estimate
"""
function initialize_counterfactual_cache(model, X)
    Q⁰ = model.initial_factors.outcome_mean
    G⁰ = model.initial_factors.propensity_score
    Ψ = model.Ψ
    counterfactual_cache = (predictions=[], signs=[], covariates=[])
    Ttemplate = selectcols(X, treatments(Ψ))
    
    for (vals, sign) in indicator_fns(Ψ)
        T_ct = counterfactualTreatment(vals, Ttemplate)
        X_ct = DataFrame((;(Symbol(colname) => colname ∈ names(T_ct) ? T_ct[!, colname] : X[!, colname] for colname in names(X))...))
        
        covariates_ct, w_ct = clever_covariate_and_weights(Ψ, 
            G⁰,
            X_ct; 
            ps_lowerbound=model.ps_lowerbound, 
            weighted_fluctuation=model.weighted
        )
        predictions_ct = MLJBase.predict(Q⁰, X_ct)
        push!(
            counterfactual_cache.predictions, 
            predictions_ct
        )
        push!(
            counterfactual_cache.signs, 
            sign
        )
        push!(
            counterfactual_cache.covariates, 
            covariates_ct
        )
    end
    return counterfactual_cache
end

function compute_counterfactual_aggregate!(counterfactual_cache, Q)
    ct_aggregate = zeros(length(first(counterfactual_cache.predictions)))
    for (idx, (ct_ŷ, sign, ct_covariates)) in enumerate(zip(
            counterfactual_cache.predictions, 
            counterfactual_cache.signs, 
            counterfactual_cache.covariates
        ))
        # Compute new counterfactual predictions
        Xfluct = fluctuation_input(ct_covariates, ct_ŷ)
        new_ct_ŷ = MLJBase.predict(Q, Xfluct)
        # Update the counterfactual aggregate for this step
        ct_aggregate .+= sign .* expected_value(new_ct_ŷ)
        # Update the cache with the new counterfactual predictions
        counterfactual_cache.predictions[idx] = new_ct_ŷ
    end
    return ct_aggregate
end

function compute_gradient_and_estimate_from_caches!(
    observed_cache, 
    counterfactual_cache, 
    Q, 
    Xfluct,
    prevalence_weights
    )
    # Update predictions
    observed_cache[:ŷ] = MLJBase.predict(Q, Xfluct)
    # Compute gradient
    Ey = expected_value(observed_cache[:ŷ])
    ct_aggregate = compute_counterfactual_aggregate!(counterfactual_cache, Q)
    gradient_Y_X = ∇YX(observed_cache[:H], observed_cache[:y], Ey, observed_cache[:w])
    gradient, Ψ̂ =  gradient_and_estimate(ct_aggregate, gradient_Y_X, observed_cache[:y], prevalence_weights)
    return gradient, Ψ̂
end


"""
Computes the classic gradient and TMLE point estimate.
"""
function gradient_and_estimate(ct_aggregate, gradient_Y_X, y, weights::Nothing)
    Ψ̂ = plugin_estimate(ct_aggregate)
    gradient = gradient_Y_X .+ ∇W(ct_aggregate, Ψ̂)
    return gradient, Ψ̂
end


@doc raw"""
Computes the case-control weighted gradient and TMLE point estimate.

See: https://www.degruyterbrill.com/document/doi/10.2202/1557-4679.1114/html and https://biostats.bepress.com/cgi/viewcontent.cgi?article=1238&context=ucbbiostat

In the case-control setting with prevalence weights (``q_0``) and ``J`` controls per case, the estimator for the counterfactual mean is:

```math
\hat{CM}_{q_0,T=1} = \frac{1}{Ncases} \sum_{i=1}^{Ncases}\bigl(q_0 \cdot Q(1, W_{1, i}) + \frac{1-q_0}{J}\sum_{j=1}^J Q(1, W_{0,i}^j)\bigr)
```

and the case control gradient can be expressed in terms of the original gradient is:

```math
D_{q_0}(g^*, Q^*)(O_i) = q_0 D(g^*, Q^*)(O_{1, i}) + \frac{1-q_0}{J} \sum_{j=1}^J D(g^*, Q^*)(O_{0, i}^j)
```

where, ``O_{1, i}`` is the case observation ``i`` and ``O_{0, i}^j`` one of the associated control observations 
(in this implementation, the controls are not matchd but simply randomly assigned). In the above, we also have:

- ``g^*``: is the MLE weighted propensity score
- ``Q^* = (Q_Y^*, Q_W^*)``: where ``Q_Y^*`` is the weighted TMLE and ``Q_W^*`` the weighted empirical mean over ``W``.

In particular, the point estimate to be used in the case control gradient is the case control point estimate. This point estimate is not yet available 
but it can be noticed that if we let ``D(g^*, Q^*) = \bar{D}(g^*, Q^*) - \hat{\Psi}_{q_0}`` then:

D_{q_0}(g^*, Q^*)(O_i) = q_0 \bar{D}(g^*, Q^*)(O_{1, i}) + \frac{1-q_0}{J} \sum_{j=1}^J \bar{D}(g^*, Q^*)(O_{0, i}^j) - \hat{\Psi}_{q_0}

In the below implementation we:

1. Batch control observations for each case
2. Compute the pseudo gradient ``\bar{D} = gradient_Y_X + ct_aggregate`` for each case and its associated controls
3. Compute the `point_estimate` using ``ct_aggregate`` because the estimands considered here are lienar in ``CM``
4. Finalise the `point estimate` by taking the mean over cases
5. Finalise the `gradient` by subtracting the `point_estimate`
"""
function gradient_and_estimate(ct_aggregate, gradient_Y_X, y, weights)
    # Retrieve case and control indices
    idx_cases = findall(y .== 1)
    idx_ctls  = findall(y .== 0)
    # Retrieve nC, nCo, J
    nC  = length(idx_cases)
    nCo = length(idx_ctls)
    J = nCo ÷ nC
    # gradient_Y_X: cases and controls
    gradient_Y_X_cases = gradient_Y_X[idx_cases]
    gradient_Y_X_controls = gradient_Y_X[idx_ctls]
    # Counterfactual aggregate: cases and controls
    ct_aggregate_cases = ct_aggregate[idx_cases]
    ct_aggregate_controls = ct_aggregate[idx_ctls]
    # Weights: cases and controls
    q₀ = weights[first(idx_cases)]
    q̄₀_over_J = weights[first(idx_ctls)]
    # Pool J controls with each case
    gradient = Vector{Float64}(undef, nC)
    point_estimate = 0.0
    control_batches = collect(Iterators.partition(1:nCo, J)) ## leads to dropping surplus controls --> could incorporate them in the last batch
    # Assign exactly J controls to each case and compute gradient and estimate (zip will effectively terminate when nC is reached dropping the surplus)
    @inbounds for (case_id, control_batch) in zip(1:nC, control_batches)
        ct_aggregate_case = ct_aggregate_cases[case_id]
        ct_aggregate_controls_batch = ct_aggregate_controls[control_batch]
        point_estimate += q₀ * ct_aggregate_case + q̄₀_over_J * sum(ct_aggregate_controls_batch)
        ct_aggregate_controls_batch .+= gradient_Y_X_controls[control_batch]
        ctl_sum = q̄₀_over_J * sum(ct_aggregate_controls_batch)
        gradient[case_id] = q₀ * (gradient_Y_X_cases[case_id] + ct_aggregate_case) + ctl_sum
    end
    point_estimate /= nC
    gradient .-= point_estimate

    return gradient, point_estimate

end

function update_report!(report, Ψ̂, gradient, epsilon)
    push!(report.estimates, Ψ̂)
    push!(report.gradients, gradient)
    push!(report.epsilons, epsilon)
end

get_fluctuation_weights(prevalence_weights::Nothing, clever_covariate_weights) = clever_covariate_weights

get_fluctuation_weights(prevalence_weights, clever_covariate_weights) = clever_covariate_weights .* prevalence_weights

"""
    MLJBase.fit(model::Fluctuation, verbosity, X, y)

Iteratively fits a one dimensional path through the previously fitted model (initially the untargeted factors) 
for at most `model.max_iter` or until convergence. Convergence is reached when the mean of the gradient is below `model.tol`. 
If `model.tol=nothing` then the tolerance is set to `1/n_samples`.

# Mathematical aspects

For binary outcomes this is given by:

ϵ = argmin(ϵ) ∑ logit(Q(T, W)) + ϵ ⋅ H(T, W)

For continuous outcomes this is given by:

ϵ = argmin(ϵ) ∑ Q(T, W) + ϵ ⋅ H(T, W)

where H(T, W) is the clever covariate.

# Technical aspects

Both a `counterfactual_cache` and an `observed_cache` are initialized and updated to save computations. 
The former stores the counterfactual predictions, signs and covariates. 
The latter stores the clever covariate, weights, predictions and the outcome in floating point representation.
"""
function MLJBase.fit(model::Fluctuation, verbosity, X, y)
    Q = model.initial_factors.outcome_mean
    fluctuation_model = one_dimensional_path(scitype(y))
    observed_cache = initialize_observed_cache(model, X, y)
    counterfactual_cache = initialize_counterfactual_cache(model, X)
    report = (estimates = [], gradients = [], epsilons = [])
    machines = []
    for iter in 1:model.max_iter
        verbosity > 0 && @info(string("TMLE step: ", iter, "."))
        # Fit new fluctuation using observed data
        Xfluct = fluctuation_input(observed_cache[:H], observed_cache[:ŷ])
        w_fluct = get_fluctuation_weights(model.prevalence_weights, observed_cache[:w])
        Q = machine(
            fluctuation_model, 
            Xfluct, 
            y,
            w_fluct,
            cache=model.cache
        )
        MLJBase.fit!(Q, verbosity=verbosity-1)
        push!(machines, Q)
        # Compute the estimate, gradient and update caches
        gradient, Ψ̂ = compute_gradient_and_estimate_from_caches!(
            observed_cache, 
            counterfactual_cache, 
            Q, 
            Xfluct,
            model.prevalence_weights
        )
        # store the full data IC in report
        update_report!(report, Ψ̂, gradient, fitted_params(Q).coef)
        if hasconverged(gradient, model.tol)
            verbosity > 0 && @info("Convergence criterion reached.")
            return machines, nothing, report
        end
    end
    verbosity > 0 && @info("Convergence criterion not reached.")
    return machines, nothing, report
end

"""
    MLJBase.predict(model::Fluctuation, machines, X)

Generates initial predictions and iteratively predicts from the fitted fluctuations.
"""
function MLJBase.predict(model::Fluctuation, machines, X) 
    covariate, _ = clever_covariate_and_weights(
        model.Ψ, model.initial_factors.propensity_score, X;
        ps_lowerbound=model.ps_lowerbound,
        weighted_fluctuation=model.weighted
    )
    ŷ = MLJBase.predict(model.initial_factors.outcome_mean, X)
    for mach in machines
        Xfluct = fluctuation_input(covariate, ŷ)
        ŷ = MLJBase.predict(mach, Xfluct)
    end
    return ŷ
end