var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [TMLE]\nPrivate = false","category":"page"},{"location":"api/#TMLE.ATE","page":"API Reference","title":"TMLE.ATE","text":"ATE: Average Treatment Effect\n\nMathematical definition: \n\nEₓ[E[Y|T=case, X]] - Eₓ[E[Y|T=control, X]]\n\nCausal graph:\n\n T  ←  W  \n  ↘   ↙ \n    Y  ← C\n\nNotation:\n\nY: target\nT: treatment\nW: confounders\nC: covariates\nX = (W, C, T) \n\nFields:\n\n- target: A symbol identifying the target variable of interest\n- treatment: A NamedTuple linking each treatment variable to case/control values\n- confounders: Confounding variables affecting both the target and the treatment\n- covariates: Optional extra variables affecting the target only\n\nExamples:\n\nATE₁ = ATE(\n    target=:Y₁,\n    treatment=(T₁=(case=1, control=0),),\n    confounders=[:W₁, :W₂],\n    covariates=[:C₁]\n)\n\nATE₂ = ATE(\n    target=:Y₂,\n    treatment=(T₁=(case=1, control=0), T₂=(case=\"A\", control=\"B\")),\n    confounders=[:W₁],\n)\n\n\n\n\n\n","category":"type"},{"location":"api/#TMLE.CM","page":"API Reference","title":"TMLE.CM","text":"CM: Conditional Mean\n\nMathematical definition: \n\nEₓ[E[Y|T=t, X]]\n\nCausal graph:\n\n T  ←  W  \n  ↘   ↙ \n    Y  ← C\n\nNotation:\n\nY: target\nT: treatment\nW: confounders\nC: covariates\nX = (W, C, T) \n\nFields:\n\n- target: A symbol identifying the target variable of interest\n- treatment: A NamedTuple linking each treatment variable to a value\n- confounders: Confounding variables affecting both the target and the treatment\n- covariates: Optional extra variables affecting the target only\n\nExamples:\n\nCM₁ = CM(\n    target=:Y₁,\n    treatment=(T₁=1,),\n    confounders=[:W₁, :W₂],\n    covariates=[:C₁]\n)\n\nCM₂ = CM(\n    target=:Y₂,\n    treatment=(T₁=1, T₂=\"A\"),\n    confounders=[:W₁],\n)\n\n\n\n\n\n","category":"type"},{"location":"api/#TMLE.IATE","page":"API Reference","title":"TMLE.IATE","text":"IATE: Interaction Average Treatment Effect\n\nMathematical definition for pairwise interaction:\n\nEₓ[E[Y|T₁=1, T₂=1, X]] - Eₓ[E[Y|T₁=1, T₂=0, X]] - Eₓ[E[Y|T₁=0, T₂=1, X]] + Eₓ[E[Y|T₁=0, T₂=0, X]]\n\nCausal graph:\n\n T  ←  W  \n  ↘   ↙ \n    Y  ← C\n\nNotation:\n\nY: target\nT: treatment\nW: confounders\nC: covariates\nX = (W, C, T) \n\nFields:\n\n- target: A symbol identifying the target variable of interest\n- treatment: A NamedTuple linking each treatment variable to case/control values\n- confounders: Confounding variables affecting both the target and the treatment\n- covariates: Optional extra variables affecting the target only\n\nExamples:\n\nIATE₁ = IATE(\n    target=:Y₁,\n    treatment=(T₁=(case=1, control=0), T₂=(case=\"A\", control=\"B\")),\n    confounders=[:W₁],\n)\n\n\n\n\n\n","category":"type"},{"location":"api/#TMLE.NuisanceSpec-Tuple{Any, Any}","page":"API Reference","title":"TMLE.NuisanceSpec","text":"NuisanceSpec(Q, G; H=encoder(), F=Q_model(target_scitype(Q)))\n\nSpecification of the nuisance parameters to be learnt.\n\nArguments:\n\nQ: For the estimation of E₀[Y|T=case, X]\nG: For the estimation of P₀(T|W)\nH: The TreatmentTransformer` to deal with categorical treatments\nF: The generalized linear model used to fluctuate the initial Q\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.compose-Union{Tuple{N}, Tuple{Any, Vararg{TMLE.PointTMLE, N}}} where N","page":"API Reference","title":"TMLE.compose","text":"compose(f, estimation_results::Vararg{PointTMLE, N}) where N\n\nProvides an estimator of f(estimation_results...).\n\nMathematical details\n\nThe following is a summary from Asymptotic Statistics, A. W. van der Vaart.\n\nConsider k TMLEs computed from a dataset of size n and embodied by Tₙ = (T₁,ₙ, ..., Tₖ,ₙ).  Since each of them is asymptotically normal, the multivariate CLT provides the joint  distribution:\n\n√n(Tₙ - Ψ₀) ↝ N(0, Σ),\n\nwhere Σ is the covariance matrix of the TMLEs influence curves.\n\nLet f:ℜᵏ→ℜᵐ, be a differentiable map at Ψ₀. Then, the delta method provides the limiting distribution of √n(f(Tₙ) - f(Ψ₀)). Because Tₙ is normal, the result is:\n\n√n(f(Tₙ) - f(Ψ₀)) ↝ N(0, ∇f(Ψ₀) ̇Σ ̇(∇f(Ψ₀))ᵀ),\n\nwhere ∇f(Ψ₀):ℜᵏ→ℜᵐ is a linear map such that by abusing notations and identifying the  function with the multiplication matrix: ∇f(Ψ₀):h ↦ ∇f(Ψ₀) ̇h. And the matrix ∇f(Ψ₀) is  the jacobian of f at Ψ₀.\n\nHence, the only thing we need to do is:\n\nCompute the covariance matrix Σ\nCompute the jacobian ∇f, which can be done using Julia's automatic differentiation facilities.\nThe final estimator is normal with mean f₀=f(Ψ₀) and variance σ₀=∇f(Ψ₀) ̇Σ ̇(∇f(Ψ₀))ᵀ\n\nArguments\n\nf: An array-input differentiable map.\nestimation_results: 1 or more PointTMLE structs.\n\nExamples\n\nAssuming res₁ and res₂ are TMLEs:\n\nf(x, y) = [x^2 - y, y - 3x]\ncompose(f, res₁, res₂)\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.parameters_from_yaml-Tuple{Any}","page":"API Reference","title":"TMLE.parameters_from_yaml","text":"parameters_from_yaml(path)\n\nInstantiate parameters described in the provided YAML file.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.tmle!-Tuple{TMLE.TMLECache, NuisanceSpec, TMLE.Parameter}","page":"API Reference","title":"TMLE.tmle!","text":"tmle!(cache::TMLECache, η_spec::NuisanceSpec, Ψ::Parameter; verbosity=1, threshold=1e-8)\n\nRuns the TMLE procedure for the new parameter Ψ and the new nuisance parameters specification η_spec  while potentially reusing cached nuisance parameters.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.tmle!-Tuple{TMLE.TMLECache, NuisanceSpec}","page":"API Reference","title":"TMLE.tmle!","text":"tmle!(cache::TMLECache, η_spec::NuisanceSpec; verbosity=1, threshold=1e-8)\n\nRuns the TMLE procedure for the new nuisance parameters specification η_spec while potentially reusing cached nuisance parameters.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.tmle!-Tuple{TMLE.TMLECache, TMLE.Parameter, NuisanceSpec}","page":"API Reference","title":"TMLE.tmle!","text":"tmle!(cache::TMLECache, Ψ::Parameter, η_spec::NuisanceSpec; verbosity=1, threshold=1e-8)\n\nRuns the TMLE procedure for the new parameter Ψ and the new nuisance parameters specification η_spec  while potentially reusing cached nuisance parameters.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.tmle!-Tuple{TMLE.TMLECache, TMLE.Parameter}","page":"API Reference","title":"TMLE.tmle!","text":"tmle!(cache::TMLECache, Ψ::Parameter; verbosity=1, threshold=1e-8)\n\nRuns the TMLE procedure for the new parameter Ψ while potentially reusing cached nuisance parameters.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.tmle-Tuple{TMLE.Parameter, NuisanceSpec, Any}","page":"API Reference","title":"TMLE.tmle","text":"tmle(Ψ::Parameter, η_spec::NuisanceSpec, dataset; verbosity=1, threshold=1e-8)\n\nMain entrypoint to run the TMLE procedure.\n\nArguments\n\nΨ: The parameter of interest\nηspec: The specification for learning `Q0andG_0`\ndataset: A tabular dataset respecting the Table.jl interface\nverbosity: The logging level\nthreshold: To avoid small values of Ĝ to cause the \"clever covariate\" to explode\nmach_cache: Whether underlying MLJ.machines will cache data or not\n\n\n\n\n\n","category":"method"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"EditURL = \"https://github.com/olivierlabayle/TMLE.jl/blob/main/examples/introduction_to_targeted_learning.jl\"","category":"page"},{"location":"examples/introduction_to_targeted_learning/#Introduction-to-Targeted-Learning","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"","category":"section"},{"location":"examples/introduction_to_targeted_learning/#Taller-is-better?","page":"Introduction to Targeted Learning","title":"Taller is better?","text":"","category":"section"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"One is often interested in the effect of a given variable (treatment) on an outcome. For instance, one could be interested in the effect of an individual's characteristics on their climbing ability. In this tutorial we investigate this question using the famous 8anu-climbing-logbook dataset. More precisely, we will investigate the effect of height on the average and maximum climbing grades reached by individuals.","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"using SQLite\nusing DataFrames\nusing CategoricalArrays\nusing CairoMakie\nusing Statistics\n\nfunction bucketize(v; buckets=[160., 170., 180., 190., 200.])\n    bucketized = Vector{Int}()\n    max_val = maximum(v)\n    buckets = copy(buckets)\n    if max_val != buckets[end]\n        append!(buckets, max_val)\n    end\n    for index in eachindex(v)\n        x = v[index]\n        for bucket_limit in buckets\n            if x <= bucket_limit\n                push!(bucketized, bucket_limit)\n                break\n            end\n        end\n    end\n    return categorical(bucketized)\nend\n\nfunction load_dataset(;db_path = \"/Users/olivierlabayle/Documents/database.sqlite\", height_buckets = [160., 170., 180., 190., 200.])\n    db = SQLite.DB(db_path)\n    dataset = DBInterface.execute(\n        db,\n        \"\"\"SELECT CAST(height AS float) AS height,\n                  CAST(sex AS float) AS sex,\n                  CAST(max_score AS FLOAT) AS max_score,\n                  mean_score FROM\n            (SELECT user_id, MAX(score) as max_score, AVG(score) as mean_score FROM ascent\n                INNER JOIN grade\n                ON ascent.grade_id = grade.id\n                GROUP BY user_id) as score_table\n                INNER JOIN user\n                ON score_table.user_id = user.id\n                WHERE user.height > 150 AND user.height < 220;\"\"\"\n        ) |> DataFrame\n\n    dataset.categorical_height = bucketize(dataset.height; buckets=height_buckets)\n\n    for height in height_buckets\n        dataset[!, \"counterfactual_height_$height\"] .= height\n    end\n\n    return dataset\nend\n\nfunction plot_climbing_data(dataset)\n    overall = combine(groupby(dataset, :height), :mean_score => mean, :max_score => mean)\n    bysex = combine(groupby(dataset, [:sex, :height]), :mean_score => mean, :max_score => mean)\n    females = filter(x -> x.sex == 1, bysex)\n    males = filter(x -> x.sex == 0, bysex)\n    fig = Figure()\n    axis₁ = Axis(fig[1, 1], xlabel=\"Height\", ylabel=\"Mean mean Score\")\n    scatter!(axis₁, overall.height, overall.mean_score_mean, label=\"Overall\")\n    scatter!(axis₁, females.height, females.mean_score_mean, label=\"Females\")\n    scatter!(axis₁, males.height, males.mean_score_mean, label=\"Males\")\n    axislegend()\n    axis₂ = Axis(fig[1, 2], xlabel=\"Height\", ylabel=\"Mean max Score\")\n    scatter!(axis₂, overall.height, overall.max_score_mean, label=\"Overall\")\n    scatter!(axis₂, females.height, females.max_score_mean, label=\"Females\")\n    scatter!(axis₂, males.height, males.max_score_mean, label=\"Males\")\n    axislegend()\n    fig\nend\n\nheight_buckets = [160., 170., 180., 190., 200., 220.]\ndataset = load_dataset(height_buckets=height_buckets)\nplot_climbing_data(dataset)","category":"page"},{"location":"examples/introduction_to_targeted_learning/#The-causal-model","page":"Introduction to Targeted Learning","title":"The causal model","text":"","category":"section"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"In order to draw causal conclusions we must first come up with a causal model of the problem. If you are new to causal inference, you can have a look at Judea Pearl's primer, if you are not interested in causal inference but only on the targeted approach you can jump to the next section. Causal models are typically represented by directed acyclic graphs like the following one:","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"<div style=\"text-align:center\">\n<img src=\"assets/climbing_graph.png\" alt=\"Causal Model of Climing Performance\" style=\"width:400px;\"/>\n</div>","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"One of the major result in causal inference, the so-called backdoor-criterion, tells us that a causal effect of a treatment on an outcome can be obtained by \"adjusting\" for all back-door paths into the treatment and outcome variables. In our example it seems natural to assume that an individual's sex can be influencing both their height and climbing performance and is thus one such variable. There could be many other such variables, and we should adjust for all of them otherwise our effect size won't be carrying a causal interpretation. When we say \"adjust\" for variables, we formally mean integrate over those variables:","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"p^do(height=h)(score=s) = int_g p(score=s  height=h gender=g)","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"For the sake of this tutorial we will assume that gender is the only confounding variable and can now move to the statistical estimation procedure.","category":"page"},{"location":"examples/introduction_to_targeted_learning/#The-conceptual-shift:-from-statistical-models-to-parameters","page":"Introduction to Targeted Learning","title":"The conceptual shift: from statistical models to parameters","text":"","category":"section"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"Traditional estimation methods start from a conveniently chosen parametric statistical model, proceed with maximum likelihood estimation and finally interpret one of the model's parameter as the seeked effect size. For instance, a linear model is often assumed and in our scenario could be formalized as :","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"Y =  alpha + beta cdot X + epsilon  epsilon sim mathcalN(0 sigma^2)","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"where alpha, beta and sigma^2 are parameters to be estimated. In this case, beta would be understood as the effect of X on Y. The problem with this approach is that if the model is wrong there is no guarantee that beta will actually correspond to any effect size at all. We refer to such approaches as model-based because the model is the starting point of the analysis. Targeted Learning on the other hand, can be considered parameter-based because the starting point of the analysis is the question of interest. But how do you formulate a question without a model? The reality is that it requires some mathematical abstraction, which can be intimidating at first and keep you astray. Please do not, in fact you don't need to understand the mathematical details to use this package, and if you are trying to estimate some standard effect size, there is a high chance the parameter you are looking for is the Average Treatment Effect (ATE). Conceptually, you can think of the observed data as being generated by some complicated process which we will denote by P_0. The 0 subscript reminds us that this the ground truth, some unknown process that we can only witness through the observed data. A parameter, or question of interest is simply a summary of this highly complicated P_0. In our climbing example, we could ask the following question: \"What average improvement in climbing grades would someone see after a year if they started climbing 3 times a week as compared to climbing only once a week\". As you can see, the question is quite precise and the answer is expected to be a single number, for instance it could be 0, 1, 2, 3 grades... Formally this is represented by the ATE as follows:","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"ATE_0 C=3 rightarrow C=4 =  mathbfE_0mathbfE_0YC=4 W - mathbfE_0mathbfE_0YC=3 W","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"where the mathbfE_0 symbol is the expecation (average) operator, the inner one is taken over Y and the outer one over W. In essence, we are only looking at the average difference in outcomes for two different groups C=4 and C=3, and the presence of the extra W is due to our causal understanding of climbing progression. However you can notice that no particular model is assumed to define the ATE, it is simply depending on P_0 via the average mathbfE_0. Now that we have defined our target quantity, we can proceed to estimation.","category":"page"},{"location":"examples/introduction_to_targeted_learning/#The-naive-estimator","page":"Introduction to Targeted Learning","title":"The naive estimator","text":"","category":"section"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"The most common estimation strategy, and that which is used by Targeted Learning is the so-called plugin-estimation. The idea is simple, because we have defined our quantity of interest (the ATE) as a function of P_0, we only need to come up with an estimate hatP_n for P_0 and then compute the ATE on this estimated hatP_n. In fact, in most cases, we don't even need to come up with a complete estimate for P_0 but only the relevant parts of it. By looking at the ATE's formula, we can see that each term consists of two nested expectations, there are thus two such relevant parts that we need to estimate.","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"The first one is the conditional mean of the outcome given the climbing frequency and the confounders:","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"mathcalQ_0(c w) = mathbfE_0YC=c W=w","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"The second one is the mean of mathcalQ_0(c w) over w:","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"mu_0 W(c) = mathbfE_0mathcalQ_0(c W)","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"Using MLJ, we can define an estimator for mathcalQ_0 as follows:","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"using MLJ\nusing MLJLinearModels\nusing EvoTrees\nusing MLJModels\nusing NearestNeighborModels\n\nQ̂_spec = Stack(\n    metalearner = LinearRegressor(),\n    lr = LinearRegressor(),\n    evo = EvoTreeRegressor(),\n    knn = KNNRegressor(),\n    constant = DeterministicConstantRegressor()\n)","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"and compute the naive estimate by using the empirical distribution for mu_0 W, which is computing the empirical mean. We can do this for both the mean_score and max_score outcomes and with or without confounding adjustment to see the difference.","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"naive_estimate(mach, X_high, X_low) =\n    mean(predict(mach, X_high) .- predict(mach, X_low))\n\nfunction counterfactualX(dataset, features, height)\n    if :sex in features\n        return dataset[!, [\"counterfactual_height_$height\", \"sex\"]]\n    else\n        return dataset[!, [\"counterfactual_height_$height\"]]\n    end\nend\n\nresults = DataFrame(height_high=[], height_low=[], target=[], features=[], estimate=[])\nfor target in (:mean_score, :max_score)\n    for features in [[:height, :sex], [:height]]\n        mach = machine(\n            Q̂_spec,\n            dataset[!, features],\n            dataset[!, target]\n        )\n        fit!(mach, verbosity=0)\n        for (low, high) in zip(height_buckets[1:end-1], height_buckets[2:end])\n            X_high =  counterfactualX(dataset, features, high)\n            X_low = counterfactualX(dataset, features, low)\n            estimate = naive_estimate(mach, X_high, X_low)\n            push!(results, (high, low, target, join(features, :_), estimate))\n        end\n    end\nend\nresults","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"Now the estimate is only...","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"The problem with the naive approach is that the use of the data is targeted towards the estimation of mathcalQ_0. However, we are not interested in mathcalQ_0 but in the ATE, this is where the targeted step comes in, it will shift our initial estimator of mathcalQ_0 to reduce the bias of our ATE estimator.","category":"page"},{"location":"examples/introduction_to_targeted_learning/#Targeting-the-naive-estimator","page":"Introduction to Targeted Learning","title":"Targeting the naive estimator","text":"","category":"section"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"","category":"page"},{"location":"examples/introduction_to_targeted_learning/","page":"Introduction to Targeted Learning","title":"Introduction to Targeted Learning","text":"This page was generated using Literate.jl.","category":"page"},{"location":"user_guide/#User-Guide","page":"User Guide","title":"User Guide","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"CurrentModule = TMLE","category":"page"},{"location":"user_guide/#The-Dataset","page":"User Guide","title":"The Dataset","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"TMLE.jl should be compatible with any dataset respecting the Tables.jl interface, that is, a structure like a NamedTuple or a DataFrame from DataFrames.jl should work. In the remainder of this section, we will be working with the same dataset and see that we can ask very many questions (Parameters) from it.","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"using Random\nusing Distributions\nusing DataFrames\nusing StableRNGs\nusing CategoricalArrays\nusing TMLE\nusing LogExpFunctions\n\nfunction make_dataset(;n=1000)\n    rng = StableRNG(123)\n    # Confounders\n    W₁ = rand(rng, Uniform(), n)\n    W₂ = rand(rng, Uniform(), n)\n    # Covariates\n    C₁ = rand(rng, Uniform(), n)\n    # Treatment | Confounders\n    T₁ = rand(rng, Uniform(), n) .< logistic.(0.5sin.(W₁) .- 1.5W₂)\n    T₂ = rand(rng, Uniform(), n) .< logistic.(-3W₁ - 1.5W₂)\n    # Target | Confounders, Covariates, Treatments\n    Y = 1 .+ 2W₁ .+ 3W₂ .- 4C₁.*T₁ .- 2T₂.*T₁.*W₂ .+ rand(rng, Normal(0, 0.1), n)\n    return DataFrame(\n        W₁ = W₁, \n        W₂ = W₂,\n        C₁ = C₁,\n        T₁ = categorical(T₁),\n        T₂ = categorical(T₂),\n        Y  = Y\n        )\nend\ndataset = make_dataset()\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"note: Note on Treatment variables\nIt should be noted that the treatment variables must be categorical. Since the treatment is also used as an input to the Q_0 learner, a OneHotEncoder is used by default (see The Nuisance Parameters section). If a numerical representation is more appropriate (ordinal variables), use the keyword ordered=true when constructing a categorical vector, the OneHotEncoder will ignore those variables and their floating point representation will be used.","category":"page"},{"location":"user_guide/#The-Nuisance-Parameters","page":"User Guide","title":"The Nuisance Parameters","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"As described in the Mathematical setting section, we need to provide an estimation strategy for both Q_0 and G_0. For illustration purposes, we here consider a simple strategy where both models are assumed to be generalized linear models. However this is not the recommended practice since there is little chance those functions are actually linear, and theoretical guarantees associated with TMLE may fail to hold. We recommend instead the use of Super Learning which is exemplified in The benefits of Super Learning.","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"using MLJLinearModels\n\nη_spec = NuisanceSpec(\n    LinearRegressor(), # Q model\n    LogisticClassifier(lambda=0) # G model\n)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"note: Practical note on $Q_0$ and $G_0$\nThe models chosen for the nuisance parameters should be adapted to the outcome they target:Q_0 = mathbfE_0YT=t W=w C=c, Y can be either continuous or categorical, in our example it is continuous and a LinearRegressor is a correct choice.\nG_0 = P_0(TW), T are always categorical variables. If T is a single treatment with only 2 levels, a logistic regression will work, if T has more than two levels a multinomial regression for instance would be suitable. If there are more than 2 treatment variables (with potentially more than 2 levels), then, the joint distribution is learnt and a multinomial regression would also work. In any case, the LogisticClassifier from MLJLinearModels is a suitable choice.For more information on available models and their uses, we refer to the MLJ documentation","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"The NuisanceSpec struct also holds a specification for the OneHotEncoder necessary for the encoding of treatment variables and a generalized linear model for the fluctuation model. Unless you know what you are doing, there is little chance you need to modify those.","category":"page"},{"location":"user_guide/#Parameters","page":"User Guide","title":"Parameters","text":"","category":"section"},{"location":"user_guide/#The-Conditional-mean","page":"User Guide","title":"The Conditional mean","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"We are now ready to move to the definition of the parameters of interest. The most basic type of parameter is the conditional mean of the target given the treatment:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"CM_t(P) = mathbbEmathbbEYT=t W","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"The treatment does not have to be restricted to a single variable, we can define for instance CM_T_1=1 T_2=1:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Ψ = CM(\n    target      = :Y,\n    treatment   = (T₁=1, T₂=1),\n    confounders = [:W₁, :W₂]\n)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"In this case, we can compute the exact value of the parameter:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"CM_T_1=1 T_2=1 = 1 + 2mathbbEW₁ + 3mathbbEW₂ - 4mathbbEC₁ - 2mathbbEW₂ = 05","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Running a targeted estimation procedure should then be as simple as:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"cm_result₁₁, _, _ = tmle(Ψ, η_spec, dataset, verbosity=0)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"For now, let's ignore the two _ outputs and focus on the result of type PointTMLE, it represents a point estimator of CM_T_1=1 T_2=0. As such, we can have a look at the value and variance of the estimator, since the estimator is asymptotically normal, a 95% confidence interval can be rougly constructed via:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Ψ̂ = TMLE.estimate(cm_result₁₁)\nσ² = var(cm_result₁₁)\nΨ̂ - 1.96√σ² <= 0.5 <= Ψ̂ + 1.96√σ²","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"In fact, we can easily be more rigorous here and perform a standard T test:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"OneSampleTTest(cm_result₁₁)","category":"page"},{"location":"user_guide/#The-Average-Treatment-Effect","page":"User Guide","title":"The Average Treatment Effect","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Let's now turn our attention to the Average Treatment Effect:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"ATE_t_1 rightarrow t_2(P) = mathbbEmathbbEYT=t_2 W - mathbbEmathbbEYT=t_1 W","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Again, from our dataset, there are many ATEs we may be interested in, let's assume we are interested in ATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1. Since we know the generating process, this can be computed exactly and we have:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"beginaligned\nATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 = (1 + 2mathbbEW₁ + 3mathbbEW₂ - 4mathbbEC₁ - 2mathbbEW₂) - (1 + 2mathbbEW₁ + 3mathbbEW₂) \n                                               = -3\nendaligned                                    ","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Let's see what the TMLE tells us:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Ψ = ATE(\n    target      = :Y,\n    treatment   = (T₁=(case=1, control=0), T₂=(case=1, control=0)),\n    confounders = [:W₁, :W₂]\n)\n\nate_result, _, _ = tmle(Ψ, η_spec, dataset, verbosity=0)\n\nOneSampleTTest(ate_result)","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"As expected.","category":"page"},{"location":"user_guide/#The-Interaction-Average-Treatment-Effect","page":"User Guide","title":"The Interaction Average Treatment Effect","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Finally let us look at the most interesting case of interactions, we compute here the IATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 since this is the highest order in our data generating process. That being said, you could go after any higher-order (3, 4, ...) interaction if you wanted. However you will inevitably decrease power and encounter positivity violations as you climb the interaction-order ladder.","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Let us provide the ground truth for this pairwise interaction, you can check that:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"beginaligned\nIATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 = 1 + 2W₁ + 3W₂ - 4C₁*T₁ - 2T₂*T₁*W₂ \n                                                = - 1\nendaligned","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"and run:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Ψ = IATE(\n    target      = :Y,\n    treatment   = (T₁=(case=1, control=0), T₂=(case=1, control=0)),\n    confounders = [:W₁, :W₂]\n)\n\niate_result, _, _ = tmle(Ψ, η_spec, dataset, verbosity=0)\n\nOneSampleTTest(iate_result)","category":"page"},{"location":"user_guide/#Composing-Parameters","page":"User Guide","title":"Composing Parameters","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"By leveraging the multivariate Central Limit Theorem and Julia's automatic differentiation facilities, we can actually compute any new parameter estimate from a set of already estimated parameters. By default, TMLE.jl will use Zygote but since we are using AbstractDifferentiation.jl you can change the backend to your favorite AD system.","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"For instance, by definition of the ATE, we should be able to retrieve ATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 by composing CM_T_1=1 T_2=1 - CM_T_1=0 T_2=0. We already have almost all of the pieces, we just need an estimate for CM_T_1=0 T_2=0, let's get it.","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Ψ = CM(\n    target      = :Y,\n    treatment   = (T₁=0, T₂=0),\n    confounders = [:W₁, :W₂]\n)\ncm_result₀₀, _, _ = tmle(Ψ, η_spec, dataset, verbosity=0)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"composed_ate_result = compose(-, cm_result₁₁, cm_result₀₀)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"We can compare the estimate value, which is simply obtained by applying the function to the arguments:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"TMLE.estimate(composed_ate_result), TMLE.estimate(ate_result)","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"and the variance:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"var(composed_ate_result), var(ate_result)","category":"page"},{"location":"user_guide/#Reading-Parameters-from-YAML-files","page":"User Guide","title":"Reading Parameters from YAML files","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"It may be useful to declare a list of parameter files for a given causal model from a file. We provide this functionality using the YAML format and the parameters_from_yaml function. A parameters configuration file contains 4 mandatory and 1 optional sections. The variables sections: T, Y, W, C are lists of variables corresponding to treatments, targets, confounders and covariates (optional) respectively. The Parameters section is a list of parameters to be generated, with the causal model specified by the variables sections. The name subsection identifies the type of the parameter and the other subsections describe the exact treatment values specifications. Since a parameter corresponds to only one target, if multiple targets are present in the parameter file, as many parameters are generated for each target.","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"T:\n  - T1\n  - T2\n\nC: # Optional\n  - C1\n\nW:\n  - W1\n\nY:\n  - Y1\n  - Y2\n\nParameters:\n  - name: IATE\n    T1:\n      case: 2\n      control: 1\n    T2:\n      case: \"AC\"\n      control: \"CC\"\n  - name: ATE\n    T1:\n      case: 2\n      control: 0\n    T2:\n      case: \"AC\"\n      control: \"CC\"\n  - name: CM\n    T1: 0\n    T2: 0","category":"page"},{"location":"user_guide/#Using-the-cache","page":"User Guide","title":"Using the cache","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Oftentimes, we are interested in multiple parameters, or would like to investigate how our estimator is affected by changes in the nuisance parameters specification. In many cases, as long as the dataset under study is the same, it is possible to save some computational time by caching the previously learnt nuisance parameters. We describe below how TMLE.jl proposes to do that in some common scenarios. For that purpose let us add a new target variable (which is simply random noise) to our dataset:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"dataset.Ynew = rand(1000)\nnothing # hide","category":"page"},{"location":"user_guide/#Scenario-1:-Changing-the-treatment-values","page":"User Guide","title":"Scenario 1: Changing the treatment values","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Let us say we are interested in two ATE parameters: ATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 and ATE_T_1=1 rightarrow 0 T_2=0 rightarrow 1 (Notice how the setting for T_1 has changed).","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Let us start afresh an compute the first ATE:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Ψ = ATE(\n    target      = :Y,\n    treatment   = (T₁=(case=1, control=0), T₂=(case=1, control=0)),\n    confounders = [:W₁, :W₂]\n)\n\nate_result₁, _, cache = tmle(Ψ, η_spec, dataset)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Notice the logs are informing you of all the nuisance parameters that are being fitted.","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Let us now investigate the second ATE by using the cache:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Ψ = ATE(\n    target      = :Y,\n    treatment   = (T₁=(case=0, control=1), T₂=(case=1, control=0)),\n    confounders = [:W₁, :W₂]\n)\n\nate_result₂, _, cache = tmle!(cache, Ψ)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"You should see that the logs are actually now telling you which nuisance parameters have been reused, i.e. all of them, only the targeting step needs to be done! This is because we already had nuisance estimators that matched our target parameter.","category":"page"},{"location":"user_guide/#Scenario-2:-Changing-the-target","page":"User Guide","title":"Scenario 2: Changing the target","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Let us now imagine that we are interested in another target: Ynew, we can say so by defining a new parameter and running the TMLE procedure using the cache:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Ψ = ATE(\n    target      = :Ynew,\n    treatment   = (T₁=(case=1, control=0), T₂=(case=1, control=0)),\n    confounders = [:W₁, :W₂]\n)\n\nate_result₃, _, cache = tmle!(cache, Ψ)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"As you can see, only Q has been updated because the existing cached G already matches our target parameter and cane be reused.","category":"page"},{"location":"user_guide/#Scenario-3:-Changing-the-nuisance-parameters-specification","page":"User Guide","title":"Scenario 3: Changing the nuisance parameters specification","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Another common situation is to try a new model for a given nuisance parameter (or both). Here we can try a new regularization parameter for our logistic regression:","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"η_spec = NuisanceSpec(\n    LinearRegressor(), # Q model\n    LogisticClassifier(lambda=0.001) # Updated G model\n)\n\nate_result₄, _, cache = tmle!(cache, η_spec)\nnothing # hide","category":"page"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Since we have only updated G's specification, only this model is fitted again.","category":"page"},{"location":"user_guide/#General-behaviour","page":"User Guide","title":"General behaviour","text":"","category":"section"},{"location":"user_guide/","page":"User Guide","title":"User Guide","text":"Any change to either the Parameter of interest or the NuisanceSpec structures will trigger an update of the cache.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = TMLE","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TMLE.jl is a Julia implementation of the Targeted Minimum Loss-Based Estimation (TMLE) framework. If you are interested in efficient and unbiased estimation of some causal effect, you are in the right place. Since TMLE uses machine-learning methods to estimate nuisance parameters, the present package is based upon MLJ. This means that any model respecting the MLJ interface can be used for the estimation of nuisance parameters.","category":"page"},{"location":"#Mathematical-setting","page":"Home","title":"Mathematical setting","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Even though the TMLE framework is not restricted to causal inference, the scope of the present package is currently restricted to so called \"causal effects\" and the following causal graph is assumed throughout:","category":"page"},{"location":"","page":"Home","title":"Home","text":"<div style=\"text-align:center\">\n<img src=\"assets/causal_graph.png\" alt=\"Causal Model\" style=\"width:400px;\"/>\n</div>","category":"page"},{"location":"","page":"Home","title":"Home","text":"whith the following general interpration of variables:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Y: The Target\nT: The Treatment\nW: Confounders\nC: Extra Covariates","category":"page"},{"location":"","page":"Home","title":"Home","text":"This graph encodes a factorization of the joint probability distribution P_0 that we assume generated the observed data:","category":"page"},{"location":"","page":"Home","title":"Home","text":"P_0(Y T W C) = P_0(YT W C) cdot P_0(TW) cdot P_0(W) cdot P_0(C)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Usually, we don't have much information about P_0 and don't want to make unrealistic assumptions, thus we will simply state that P_0 in mathcalM where mathcalM is the space of all probability distributions. It turns out that most target quantities of interest in causal inference can be expressed as real-valued functions of P_0, denoted by: PsimathcalM rightarrow Re. TMLE works by finding a suitable estimator hatP_n of P_0 and then simply substituting it into Psi. Fortunately, it is often non necessary to come up with an estimator of the joint distribution P_0, instead only subparts of it are required. Those are called nuisance parameters because they are not of direct interest and for our purposes, only two nuisance parameters are necessary:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Q_0(t w c) = mathbbEYT=t W=w C=c","category":"page"},{"location":"","page":"Home","title":"Home","text":"and","category":"page"},{"location":"","page":"Home","title":"Home","text":"G_0(t w) = P(T=tW=w)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Specifying which machine-learning method to use for each of those nuisance parameter is the only ingredient you will need to run a targeted estimation for any of the following quantities. Those quantities have a causal interpretation under suitable assumptions that we do not discuss here.","category":"page"},{"location":"#The-Conditional-Mean","page":"Home","title":"The Conditional Mean","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This is simply the expected value of the target Y when the treatment is set to t.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CM_t(P) = mathbbEmathbbEYT=t W","category":"page"},{"location":"#The-Average-Treatment-Effect-(ATE)","page":"Home","title":"The Average Treatment Effect (ATE)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Probably the most famous quantity in causal inference. Under suitable assumptions, tt represents the additive effect of changing the treatment value from t_1 to t_2.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ATE_t_1 rightarrow t_2(P) = mathbbEmathbbEYT=t_2 W - mathbbEmathbbEYT=t_1 W","category":"page"},{"location":"#Interaction-Average-Treatment-Effect-(IATE)","page":"Home","title":"Interaction Average Treatment Effect (IATE)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you are interested in the interaction effect of multiple treatments, the IATE is for you. An example formula is displayed below for two interacting treatments whose values are both changed from 0 to 1:","category":"page"},{"location":"","page":"Home","title":"Home","text":"IATE_0 rightarrow 1 0 rightarrow 1(P) = mathbbEmathbbEYT_1=1 T_2=1 W - mathbbEmathbbEYT_1=1 T_2=0 W  \n- mathbbEmathbbEYT_1=0 T_2=1 W + mathbbEmathbbEYT_1=0 T_2=0 W ","category":"page"},{"location":"#Any-function-of-the-previous-Parameters","page":"Home","title":"Any function of the previous Parameters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"As a result of Julia's automatic differentiation facilities, given a set of already estimated parameters (Psi_1  Psi_k), we can automatically compute an estimator for f(Psi_1  Psi_k).","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TMLE.jl can be installed via the Package Manager and supports Julia v1.6 and greater.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg> add TMLE","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To run an estimation procedure, we need 3 ingredients:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A dataset\nA parameter of interest\nA nuisance parameters specification","category":"page"},{"location":"","page":"Home","title":"Home","text":"For instance, assume the following simple data generating process:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nW  sim mathcalUniform(0 1) \nT  sim mathcalBernoulli(logistic(1-2 cdot W)) \nY  sim mathcalNormal(1 + 3 cdot T - T cdot W 001)\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"which can be simulated in Julia by:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Distributions\nusing StableRNGs\nusing Random\nusing CategoricalArrays\nusing MLJLinearModels\nusing TMLE\nusing LogExpFunctions\n\nrng = StableRNG(123)\nn = 100\nW = rand(rng, Uniform(), n)\nT = rand(rng, Uniform(), n) .< logistic.(1 .- 2W)\nY = 1 .+ 3T .- T.*W .+ rand(rng, Normal(0, 0.01), n)\ndataset = (Y=Y, T=categorical(T), W=W)\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"And say we are interested in the ATE_0 rightarrow 1(P_0):","category":"page"},{"location":"","page":"Home","title":"Home","text":"Ψ = ATE(\n    target      = :Y,\n    treatment   = (T=(case=1, control = 0),),\n    confounders = [:W]\n)\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that in this example the ATE can be computed exactly and is given by:","category":"page"},{"location":"","page":"Home","title":"Home","text":"ATE_0 rightarrow 1(P_0) = mathbbE1 + 3 - W - mathbbE1 = 3 - mathbbEW = 25","category":"page"},{"location":"","page":"Home","title":"Home","text":"We next need to define the strategy for learning the nuisance parameters Q and G, here we keep things simple and simply use generalized linear models:","category":"page"},{"location":"","page":"Home","title":"Home","text":"η_spec = NuisanceSpec(\n    LinearRegressor(),\n    LogisticClassifier(lambda=0)\n)\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"We are now ready to run the TMLE procedure and look the associated confidence interval:","category":"page"},{"location":"","page":"Home","title":"Home","text":"result, _, _ = tmle(Ψ, η_spec, dataset)\ntest_result = OneSampleTTest(result, 2.5)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Test # hide\n@test pvalue(OneSampleTTest(result, 2.5)) > 0.05 # hide\nnothing # hide","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"EditURL = \"https://github.com/olivierlabayle/TMLE.jl/blob/main/examples/super_learning.jl\"","category":"page"},{"location":"examples/super_learning/#Becoming-a-Super-Learner","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"","category":"section"},{"location":"examples/super_learning/#What-this-tutorial-is-about","page":"Becoming a Super Learner","title":"What this tutorial is about","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Super Learning, also known as Stacking, is an emsemble technique that was first introduced by Wolpert in 1992. Instead of selecting a model based on cross-validation performance, models are combined by a meta-learner to minimize the cross-validation error. It has also been shown by van der Laan et al. that the resulting Super Learner will perform at least as well as its best performing submodel.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Why is it important for Targeted Learning?","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"The short answer is that the consistency (convergence in probability) of the targeted estimator depends on the consistency of at least one of the nuisance parameters: Q_0 or G_0 (see Mathematical setting ). By only using unrealistic models like linear models, we have little chance of satisfying the above criterion. Super Learning is a data driven way to leverage a diverse set of models and build the best performing estimator for both Q_0 or G_0.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"In the following, we investigate the benefits of Super Learning for the estimation of The Average Treatment Effect.","category":"page"},{"location":"examples/super_learning/#The-dataset","page":"Becoming a Super Learner","title":"The dataset","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"To demonstrate the benefits of Super Learning, we need to simulate a dataset more stimulating than a simple linear regression, otherwise there would be nothing to do. Here is what I could come up with:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"using DataFrames\nusing TMLE\nusing Random\nusing Distributions\nusing StableRNGs\nusing CairoMakie\nusing CategoricalArrays\nusing MLJ\nusing TMLE\nusing LogExpFunctions\n\nμY(T, W₁, W₂) = sin.(10T.*W₁).*exp.(-1 .+ 2W₁.*W₂ .- T.*W₂) .- cos.(10T.*W₂).*log.(2 .- W₁.*W₂)\nμT(W₁, W₂) = logistic.(10sin.(W₁) .- 1.5W₂)\n\nfunction hard_problem(;n=1000, doT=nothing)\n    rng = StableRNG(123)\n    W₁ = rand(rng, Uniform(), n)\n    W₂ = rand(rng, Uniform(), n)\n    T = doT === nothing ?\n        rand(rng, Uniform(), n) .< μT(W₁, W₂) :\n        repeat([doT], n)\n    Y = μY(T, W₁, W₂) .+ rand(rng, Normal(0, 0.1), n)\n    return DataFrame(W₁=W₁, W₂=W₂, T=categorical(T), Y=Y)\nend\n\nN = 10000\ndataset = hard_problem(;n=N)\nnothing # hide","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"It may seem difficult to understand it but we can still have a look at the function mathbfEYTW₁ W₂.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"function plot_μY(;npoints=100)\n    W₁ = LinRange(0, 1, npoints)\n    W₂ = LinRange(0, 1, npoints)\n    y₁ = [μY(1, w₁, w₂) for w₁ in W₁, w₂ in W₂]\n    y₀ = [μY(0, w₁, w₂) for w₁ in W₁, w₂ in W₂]\n    t = [μT(w₁, w₂) for w₁ in W₁, w₂ in W₂]\n    fig = Figure(backgroundcolor = RGBf(0.98, 0.98, 0.98), resolution = (1000, 700))\n    ax₁ = Axis3(fig[1, 1], aspect=:data, xlabel=\"W₁\", ylabel=\"W₂\", zlabel=\"μY\")\n    Label(fig[1, 1, Top()], \"μY(W₁, W₂, T=1)\", textsize = 30, tellwidth=false, tellheight=false)\n    surface!(ax₁, W₁, W₂, y₁)\n    ax₀ = Axis3(fig[1, 2], aspect=:data, xlabel=\"W₁\", ylabel=\"W₂\", zlabel=\"μY\")\n    Label(fig[1, 2, Top()], \"μY(W₁, W₂, T=0)\", textsize = 30, tellwidth=false, tellheight=false)\n    surface!(ax₀, W₁, W₂, y₀)\n    axₜ = Axis3(fig[2, :], aspect=:data, xlabel=\"W₁\", ylabel=\"W₂\", zlabel=\"μT\")\n    Label(fig[2, :, Top()], \"μT(W₁, W₂)\", textsize = 30, tellwidth=false, tellheight=false)\n    surface!(axₜ, W₁, W₂, t)\n    return fig\nend\n\nplot_μY(;npoints=100)","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Also, even though the analytical solution for the ATE may be intractable, since we known the data generating process, we can approximate it by sampling.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"function approximate_ate(;n=1000)\n    dataset_1 = hard_problem(;n=n, doT = 1)\n    dataset_0 = hard_problem(;n=n, doT = 0)\n    return mean(dataset_1.Y) - mean(dataset_0.Y)\nend\n\nψ₀ =  approximate_ate(;n=1000)","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Now that the data has been generated and we know the solution to our problem, we can dive into the estimation part.","category":"page"},{"location":"examples/super_learning/#Defining-a-Super-Learner-in-MLJ","page":"Becoming a Super Learner","title":"Defining a Super Learner in MLJ","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"In MLJ, a Super Learner can be defined using the Stack function. The three most important type of arguments for a Stack are:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"metalearner: The metalearner to be used to combine the weak learner to be defined.\nresampling: The cross-validation scheme, by default, a 6-fold cross-validation.\nmodels...: A series of named MLJ models.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"One important point is that MLJ does not provide any model by itself, those have to be loaded from external compatible libraries. You can search for available models that match your data.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"for instance, for G_0 = P(T W_1 W_2), we need classification models and we can see there are quire a bunch of them:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"G_available_models = models(matching(dataset[!, [:W₁, :W₂]], dataset.T))","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"note: Stack limitations\nFor now, there are a few limitations as to which models you can actually use within the Stack. The most important is that if the output is categorical, each model must be <: Probabilistic, which means that SVMs cannot be used as a weak learners for classification.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Let's load a few model providing libraries and define our library for G_0.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"using EvoTrees\nusing MLJLinearModels\nusing MLJModels\nusing NearestNeighborModels\n\nfunction Gmodels()\n    lambdas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0, 1., 10., 100.]\n    logistic_models = [LogisticClassifier(lambda=l) for l in lambdas]\n    logistic_models = NamedTuple{Tuple(Symbol(\"lr_$i\") for i in eachindex(lambdas))}(logistic_models)\n    evo_trees = [EvoTreeClassifier(lambda=l) for l in lambdas]\n    evo_trees = NamedTuple{Tuple(Symbol(\"tree_$i\") for i in eachindex(lambdas))}(evo_trees)\n    Ks = [5, 10, 50, 100]\n    knns = [KNNClassifier(K=k) for k in Ks]\n    knns = NamedTuple{Tuple(Symbol(\"knn_$i\") for i in eachindex(Ks))}(knns)\n    return merge(logistic_models, evo_trees, knns)\nend\n\nG_super_learner = Stack(;\n    metalearner = LogisticClassifier(lambda=0),\n    resampling  = StratifiedCV(nfolds=3),\n    measure     = log_loss,\n    Gmodels()...\n)\n\n\nnothing # hide","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"And now do the same for Q_0","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"function Qmodels()\n    lambdas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0, 1., 10., 100.]\n    linear_models = [RidgeRegressor(lambda=l) for l in lambdas]\n    linear_models = NamedTuple{Tuple(Symbol(\"lr_$i\") for i in eachindex(lambdas))}(linear_models)\n    evo_trees = [EvoTreeRegressor(lambda=l) for l in lambdas]\n    evo_trees = NamedTuple{Tuple(Symbol(\"tree_$i\") for i in eachindex(lambdas))}(evo_trees)\n    Ks = [5, 10, 50, 100]\n    knns = [KNNRegressor(K=k) for k in Ks]\n    knns = NamedTuple{Tuple(Symbol(\"knn_$i\") for i in eachindex(Ks))}(knns)\n    return merge(linear_models, evo_trees, knns)\nend\n\nQ_super_learner = Stack(;\n    metalearner = LinearRegressor(fit_intercept=false),\n    resampling=CV(nfolds=3),\n    Qmodels()...\n    )\n\nnothing # hide","category":"page"},{"location":"examples/super_learning/#Targeted-estimation","page":"Becoming a Super Learner","title":"Targeted estimation","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Let us move to the targeted estimation step itself. We define the target parameter (the ATE) and the nuisance parameters specification:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Ψ = ATE(\n    target = :Y,\n    treatment = (T=(case=1, control=0),),\n    confounders = [:W₁, :W₂]\n)\n\nη_spec = NuisanceSpec(\n    Q_super_learner,\n    G_super_learner\n)\n\nnothing # hide","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Finally run the TMLE procedure and check the result","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"tmle_result, initial_result, cache = tmle(Ψ, η_spec, dataset)\n\ntest_result = OneSampleTTest(tmle_result, ψ₀)","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Now, what if we had used linear models only instead of the Super Learner? This is easy to check","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"η_spec_linear = NuisanceSpec(\n    LinearRegressor(),\n    LogisticClassifier(lambda=0)\n)\n\ntmle_result_linear, initial_result_linear, cache = tmle(Ψ, η_spec_linear, dataset)\n\ntest_result_linear = OneSampleTTest(tmle_result_linear, ψ₀)","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"using Test # hide\npvalue(test_result) > 0.05 #hide\nnothing # hide","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"This page was generated using Literate.jl.","category":"page"}]
}
