var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = TMLE","category":"page"},{"location":"#TMLE","page":"Home","title":"TMLE","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The purpose of this package is to provide convenience methods for  Targeted Minimum Loss-Based Estimation (TMLE). TMLE is a framework for efficient estimation that was first proposed by Van der Laan et al in 2006. If you are new to TMLE, this review paper  gives a nice overview to the field. Because TMLE requires nuisance parameters  to be learnt by machine learning algorithms, this package is built on top of  MLJ. This means that any model  respecting the MLJ interface can be used to estimate the nuisance parameters.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThis package is still experimental and documentation under construction","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The package is not yet part of the registry and must be installed via github:","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> ]add https://github.com/olivierlabayle/TMLE.jl","category":"page"},{"location":"#Get-in-touch","page":"Home","title":"Get in touch","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please feel free to fill an issue if you want to report any bug or want to have additional features part of the package.  Contributing is also welcome.","category":"page"},{"location":"#Tutorials","page":"Home","title":"Tutorials","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is built on top of MLJ, if you are new to the MLJ framework,  please refer first to their documentation.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Currently, two parameters of the generating distribution are available for estimation, the Average Treatment Effect (ATE) and the Interaction Average  Treatment Effect (IATE). For both quantities, a graphical representation of the  underlying causal model in presented bellow.","category":"page"},{"location":"","page":"Home","title":"Home","text":"<img src=\"assets/causal_model.png\" alt=\"Causal Model\" style=\"width:400px;\"/>","category":"page"},{"location":"","page":"Home","title":"Home","text":"TMLE is a two steps procedure, it first starts by estimating nuisance  parameters that will be used to build the final estimator. They are called nuisance parameters because they are required for estimation but are not our target quantity of interest.  For both the ATE and IATE, the nuisance parameters that require a learning algorithm are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The conditional extectation of the target \nThe conditional density of the treatment","category":"page"},{"location":"","page":"Home","title":"Home","text":"They are typically estimated by stacking which is built into MLJ and you can find more information about it here. Stacking is not compulsory however and any model  respecting the MLJ Interface should work out of the box.","category":"page"},{"location":"","page":"Home","title":"Home","text":"In the second stage, TMLE fluctuates a nuisance parameter using a parametric model in order to solve the efficient influence curve equation. For now, this is done via a  Generalized Linear model and the nuisance parameter which is fluctuated is the conditional extectation of the target variable.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For those examples, we will need the following packages:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Random\nusing Distributions\nusing MLJ\nusing TMLE\n\nexpit(X) = 1 ./ (1 .+ exp.(-X))","category":"page"},{"location":"#ATE","page":"Home","title":"ATE","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Let's consider the following example:","category":"page"},{"location":"","page":"Home","title":"Home","text":"W = [W1, W2, W_3] is a set of binary confounding variables, W sim Bernoulli(05)\nT is a Binary variable, p(T=1W=w) = textexpit(05W_1 + 15W_2 - W_3)\nY is a Continuous variable, Y = T + 2W_1 + 3W_2 - 4W_3 + epsilon(0 1)","category":"page"},{"location":"","page":"Home","title":"Home","text":"For which the ATE can be computed explicitely and is equal to 1. In Julia such dataset can be generated like this:","category":"page"},{"location":"","page":"Home","title":"Home","text":"n = 10000\nrng = MersenneTwister(0)\n# Sampling\nUnif = Uniform(0, 1)\nW = float(rand(rng, Bernoulli(0.5), n, 3))\nt = rand(rng, Unif, n) .< expit(0.5W[:, 1] + 1.5W[:, 2] - W[:,3])\ny = t + 2W[:, 1] + 3W[:, 2] - 4W[:, 3] + rand(rng, Normal(0, 1), n)\n# W needs to respect the Tables.jl interface.\n# t is a binary categorical vector\nW = MLJ.table(W)\nt = categorical(t)","category":"page"},{"location":"","page":"Home","title":"Home","text":"We need to define 2 estimators for the nuisance parameters, usually this is  done using the Stack but here because we know the generating process we can  cheat a bit. We will use a Logistic Classifier for p(T|W) and a Constant Regressor for p(Y|W, T). This means one estimator is well specified and the other not.  The target is continuous thus we will use a Linear regression model  for the fluctuation. This is done by specifying a Normal distribution for the  Generalized Linear Model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels verbosity=0\n\ntmle = ATEEstimator(LogisticClassifier(),\n                    MLJ.DeterministicConstantRegressor(),\n                    Normal())","category":"page"},{"location":"","page":"Home","title":"Home","text":"Now, all there is to do is to fit the estimator:","category":"page"},{"location":"","page":"Home","title":"Home","text":"fitresult, _, _ = MLJ.fit(tmle, 0, t, W, y)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The fitresult contains the estimate and the associated standard error. We can see  that even if one nuisance parameter is misspecified, the double robustness of TMLE enables correct estimation of our target.","category":"page"},{"location":"#IATE","page":"Home","title":"IATE","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The IATE measures the effect of interacting causes on a target variable, it was  defined by Beentjes and Khamseh in this paper. In this case, the treatment variable T is a vector, for instance for two treatments T=(T1, T2).","category":"page"},{"location":"","page":"Home","title":"Home","text":"Let's consider the following example for which again the IATE is known:","category":"page"},{"location":"","page":"Home","title":"Home","text":"W is a binary outcome confounding variable, W sim Bernoulli(04)\nT =(T_1 T_2) are independent binary variables sampled from an expit model. p(T_1=1W=w) = textexpit(05w - 1) and, p(T_2=1W=w) = textexpit(-05w - 1)\nY is a binary variable sampled from an expit model. p(Y=1t_1 t_2 w) = textexpit(-2w + 3t_1 - 3t_2 - 1)","category":"page"},{"location":"","page":"Home","title":"Home","text":"In Julia:","category":"page"},{"location":"","page":"Home","title":"Home","text":"n = 10000\nrng = MersenneTwister(0)\np_w() = 0.4\npt1_given_w(w) = expit(0.5w .- 1)\npt2_given_w(w) = expit(-0.5w .- 1)\npy_given_t1t2w(t1, t2, w) = expit(-2w .+ 3t1 .- 3t2 .- 1)\n# Sampling\nUnif = Uniform(0, 1)\nw = rand(rng, Unif, n) .< p_w()\nt₁ = rand(rng, Unif, n) .< pt1_given_w(w)\nt₂ = rand(rng, Unif, n) .< pt2_given_w(w)\ny = rand(rng, Unif, n) .< py_given_t1t2w(t₁, t₂, w)\n# W should be a table\n# T should be a table of binary categorical variables\n# Y should be a binary categorical variable\nW = (W=convert(Array{Float64}, w),)\nT = (t₁ = categorical(t₁), t₂ = categorical(t₂))\ny = categorical(y)\n# Compute the theoretical IATE\nIATE₁ = (py_given_t1t2w(1, 1, 1) - py_given_t1t2w(1, 0, 1) - py_given_t1t2w(0, 1, 1) + py_given_t1t2w(0, 0, 1))*p_w()\nIATE₀ = (py_given_t1t2w(1, 1, 0) - py_given_t1t2w(1, 0, 0) - py_given_t1t2w(0, 1, 0) + py_given_t1t2w(0, 0, 0))*(1 - p_w())\nIATE = IATE₁ + IATE₀","category":"page"},{"location":"","page":"Home","title":"Home","text":"Again, we need to estimate the 2 nuisance parameters, this time let's use the  Stack with a few learning algorithms. The fluctuation will be a Logistic Regression, this is done by specifying a Bernoulli distribution for the  Generalized Linear Model.","category":"page"},{"location":"","page":"Home","title":"Home","text":"LogisticClassifier = @load LogisticClassifier pkg=MLJLinearModels verbosity=0\nDecisionTreeClassifier = @load DecisionTreeClassifier pkg=DecisionTree verbosity=0\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels verbosity=0\n\nstack = Stack(;metalearner=LogisticClassifier(),\n                resampling=CV(),\n                lr=LogisticClassifier(),\n                tree_2=DecisionTreeClassifier(max_depth=2),\n                tree_3=DecisionTreeClassifier(max_depth=3),\n                knn=KNNClassifier())\n\ntmle = ATEEstimator(stack,\n                    stack,\n                    Bernoulli())","category":"page"},{"location":"","page":"Home","title":"Home","text":"And fit it!","category":"page"},{"location":"","page":"Home","title":"Home","text":"fitresult, _, _ = MLJ.fit(tmle, 0, t, W, y)","category":"page"},{"location":"#API","page":"Home","title":"API","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modules = [TMLE]\nPrivate = false","category":"page"},{"location":"#TMLE.FullCategoricalJoint","page":"Home","title":"TMLE.FullCategoricalJoint","text":"FullCategoricalJoint(model)\n\nA thin wrapper around a classifier.\n\n\n\n\n\n","category":"type"},{"location":"#TMLE.TMLEstimator","page":"Home","title":"TMLE.TMLEstimator","text":"TMLEstimator(Q̅, G, F)\n\n# Scope:\n\nImplements the Targeted Minimum Loss-Based Estimator for the Interaction  Average Treatment Effect (IATE) defined by Beentjes and Khamseh in https://link.aps.org/doi/10.1103/PhysRevE.102.053314. For instance, The IATE is defined for two treatment variables as: \n\nIATE = E[E[Y|T₁=1, T₂=1, W=w] - E[E[Y|T₁=1, T₂=0, W=w]         - E[E[Y|T₁=0, T₂=1, W=w] + E[E[Y|T₁=0, T₂=0, W=w]\n\nwhere:\n\nY is the target variable (Binary)\nT = T₁, T₂ are the treatment variables (Binary)\nW are confounder variables\n\nThe TMLEstimator procedure relies on plugin estimation. Like the ATE, the IATE  requires an estimator of t,w → E[Y|T=t, W=w], an estimator of  w → p(T|w)  and an estimator of w → p(w). The empirical distribution will be used for w → p(w) all along.  The estimator of t,w → E[Y|T=t, W=w] is then fluctuated to solve the efficient influence curve equation. \n\nArguments:\n\nQ̅::MLJ.Supervised : The learner to be used\n\nfor E[Y|W, T]. Typically a MLJ.Stack.\n\nG::MLJ.Supervised : The learner to be used\n\nfor p(T|W). Typically a MLJ.Stack.\n\nfluctuation_family::Distribution : This will be used to build the fluctuation \n\nusing a GeneralizedLinearModel. Typically Normal for a continuous target  and Bernoulli for a Binary target.\n\n# Examples:\n\nTODO\n\n\n\n\n\n","category":"type"},{"location":"#MLJModelInterface.fit-Tuple{TMLEstimator, Int64, Any, Any, Union{Vector{var\"#s87\"} where var\"#s87\"<:Real, CategoricalArrays.CategoricalVector{Bool, R, V, C, U} where {R<:Integer, V, C, U}}}","page":"Home","title":"MLJModelInterface.fit","text":"MLJ.fit(tmle::TMLEstimator, \n             verbosity::Int, \n             T,\n             W, \n             y::Union{CategoricalVector{Bool}, Vector{<:Real}}\n\n\n\n\n\n","category":"method"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
