<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Becoming a Super Learner · TMLE.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link rel="canonical" href="https://TARGENE.github.io/TMLE.jl/examples/super_learning/"/><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script><link href="../../assets/logo.ico" rel="icon" type="image/x-icon"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.svg" alt="TMLE.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">TMLE.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../walk_through/">Walk Through</a></li><li><span class="tocitem">User Guide</span><ul><li><a class="tocitem" href="../../user_guide/scm/">Structural Causal Models</a></li><li><a class="tocitem" href="../../user_guide/estimands/">Estimands</a></li><li><a class="tocitem" href="../../user_guide/estimation/">Estimation</a></li><li><a class="tocitem" href="../../user_guide/adjustment/">Adjustment Methods</a></li><li><a class="tocitem" href="../../user_guide/misc/">Miscellaneous</a></li></ul></li><li><span class="tocitem">Examples</span><ul><li class="is-active"><a class="tocitem" href>Becoming a Super Learner</a><ul class="internal"><li><a class="tocitem" href="#What-this-tutorial-is-about"><span>What this tutorial is about</span></a></li><li><a class="tocitem" href="#The-dataset"><span>The dataset</span></a></li><li><a class="tocitem" href="#Defining-a-Super-Learner-in-MLJ"><span>Defining a Super Learner in MLJ</span></a></li><li><a class="tocitem" href="#Targeted-estimation"><span>Targeted estimation</span></a></li></ul></li><li><a class="tocitem" href="../double_robustness/">Model Misspecification &amp; Double Robustness</a></li></ul></li><li><a class="tocitem" href="../../resources/">Resources</a></li><li><a class="tocitem" href="../../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Examples</a></li><li class="is-active"><a href>Becoming a Super Learner</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Becoming a Super Learner</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/TARGENE/TMLE.jl/blob/main/examples/super_learning.jl#" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Becoming-a-Super-Learner"><a class="docs-heading-anchor" href="#Becoming-a-Super-Learner">Becoming a Super Learner</a><a id="Becoming-a-Super-Learner-1"></a><a class="docs-heading-anchor-permalink" href="#Becoming-a-Super-Learner" title="Permalink"></a></h1><h2 id="What-this-tutorial-is-about"><a class="docs-heading-anchor" href="#What-this-tutorial-is-about">What this tutorial is about</a><a id="What-this-tutorial-is-about-1"></a><a class="docs-heading-anchor-permalink" href="#What-this-tutorial-is-about" title="Permalink"></a></h2><p>Super Learning, also known as Stacking, is an ensemble technique that was first introduced by Wolpert in 1992. Instead of selecting a model based on cross-validation performance, models are combined by a meta-learner to minimize the cross-validation error. It has also been shown by van der Laan et al. that the resulting Super Learner will perform at least as well as its best performing submodel.</p><p>Why is it important for Targeted Learning?</p><p>The short answer is that the consistency (convergence in probability) of the targeted estimator depends on the consistency of at least one of the nuisance estimands: <span>$Q_0$</span> or <span>$G_0$</span>. By only using unrealistic models like linear models, we have little chance of satisfying the above criterion. Super Learning is a data driven way to leverage a diverse set of models and build the best performing estimator for both <span>$Q_0$</span> or <span>$G_0$</span>.</p><p>In the following, we investigate the benefits of Super Learning for the estimation of the Average Treatment Effect.</p><h2 id="The-dataset"><a class="docs-heading-anchor" href="#The-dataset">The dataset</a><a id="The-dataset-1"></a><a class="docs-heading-anchor-permalink" href="#The-dataset" title="Permalink"></a></h2><p>For this example we will use the following perinatal dataset. The (haz01, parity01) are converted to categorical values.</p><pre><code class="language-julia hljs">using CSV
using DataFrames
using TMLE
using MLJ

dataset = CSV.read(
    joinpath(pkgdir(TMLE), &quot;test&quot;, &quot;data&quot;, &quot;perinatal.csv&quot;),
    DataFrame,
    select=[:haz01, :parity01, :apgar1, :apgar5, :gagebrth, :mage, :meducyrs, :sexn],
    types=Float64
)
dataset.haz01 = categorical(Int.(dataset.haz01))
dataset.parity01 = categorical(Int.(dataset.parity01))</code></pre><p>We will also assume the following causal model:</p><pre><code class="language-julia hljs">scm = SCM(
    SE(:haz01, [:parity01, :apgar1, :apgar5, :gagebrth, :mage, :meducyrs, :sexn]),
    SE(:parity01, [:apgar1, :apgar5, :gagebrth, :mage, :meducyrs, :sexn])
)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Structural Causal Model:
-----------------------
haz01 = f₁(parity01, apgar1, apgar5, gagebrth, mage, meducyrs, sexn)
parity01 = f₂(apgar1, apgar5, gagebrth, mage, meducyrs, sexn)

</code></pre><h2 id="Defining-a-Super-Learner-in-MLJ"><a class="docs-heading-anchor" href="#Defining-a-Super-Learner-in-MLJ">Defining a Super Learner in MLJ</a><a id="Defining-a-Super-Learner-in-MLJ-1"></a><a class="docs-heading-anchor-permalink" href="#Defining-a-Super-Learner-in-MLJ" title="Permalink"></a></h2><p>In MLJ, a Super Learner can be defined using the <a href="https://alan-turing-institute.github.io/MLJ.jl/stable/model_stacking/">Stack</a> function. The three most important type of arguments for a Stack are:</p><ul><li><code>metalearner</code>: The metalearner to be used to combine the weak learner to be defined.</li><li><code>resampling</code>: The cross-validation scheme, by default, a 6-fold cross-validation.</li><li><code>models...</code>: A series of named MLJ models.</li></ul><p>One important point is that MLJ does not provide any model by itself, those have to be loaded from external compatible libraries. You can search for available models that match your data.</p><p>In our case, for both <span>$G_0$</span> and <span>$Q_0$</span> we need classification models and we can see there are quire a few of them:</p><pre><code class="language-julia hljs">G_available_models = models(matching(dataset[!, parents(scm.parity01)], dataset.parity01))</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">54-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :reporting_operations, :reports_feature_importances, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:
 (name = AdaBoostClassifier, package_name = MLJScikitLearnInterface, ... )
 (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )
 (name = BaggingClassifier, package_name = MLJScikitLearnInterface, ... )
 (name = BayesianLDA, package_name = MLJScikitLearnInterface, ... )
 (name = BayesianLDA, package_name = MultivariateStats, ... )
 (name = BayesianQDA, package_name = MLJScikitLearnInterface, ... )
 (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )
 (name = CatBoostClassifier, package_name = CatBoost, ... )
 (name = ConstantClassifier, package_name = MLJModels, ... )
 (name = DecisionTreeClassifier, package_name = BetaML, ... )
 ⋮
 (name = SGDClassifier, package_name = MLJScikitLearnInterface, ... )
 (name = SVC, package_name = LIBSVM, ... )
 (name = SVMClassifier, package_name = MLJScikitLearnInterface, ... )
 (name = SVMLinearClassifier, package_name = MLJScikitLearnInterface, ... )
 (name = SVMNuClassifier, package_name = MLJScikitLearnInterface, ... )
 (name = StableForestClassifier, package_name = SIRUS, ... )
 (name = StableRulesClassifier, package_name = SIRUS, ... )
 (name = SubspaceLDA, package_name = MultivariateStats, ... )
 (name = XGBoostClassifier, package_name = XGBoost, ... )</code></pre><div class="admonition is-info"><header class="admonition-header">Stack limitations</header><div class="admonition-body"><p>For now, there are a few limitations as to which models you can actually use within the Stack. The most important is that if the output is categorical, each model must be <code>&lt;: Probabilistic</code>, which means that SVMs cannot be used as a weak learners for classification.</p></div></div><p>Let&#39;s load a few model providing libraries and define our library for <span>$G_0$</span>.</p><pre><code class="language-julia hljs">using EvoTrees
using MLJLinearModels
using MLJModels
using NearestNeighborModels

function superlearner_models()
    lambdas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0, 1., 10., 100.]
    logistic_models = [LogisticClassifier(lambda=l) for l in lambdas]
    logistic_models = NamedTuple{Tuple(Symbol(&quot;lr_$i&quot;) for i in eachindex(lambdas))}(logistic_models)
    evo_trees = [EvoTreeClassifier(lambda=l) for l in lambdas]
    evo_trees = NamedTuple{Tuple(Symbol(&quot;tree_$i&quot;) for i in eachindex(lambdas))}(evo_trees)
    Ks = [5, 10, 50, 100]
    knns = [KNNClassifier(K=k) for k in Ks]
    knns = NamedTuple{Tuple(Symbol(&quot;knn_$i&quot;) for i in eachindex(Ks))}(knns)
    return merge(logistic_models, evo_trees, knns)
end

superlearner = Stack(;
    metalearner = LogisticClassifier(lambda=0),
    resampling  = StratifiedCV(nfolds=3),
    measure     = log_loss,
    superlearner_models()...
)</code></pre><p>and assign those models to the <code>SCM</code>:</p><pre><code class="language-julia hljs">setmodel!(scm.haz01, with_encoder(superlearner))
setmodel!(scm.parity01, superlearner)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">ProbabilisticStack(
  metalearner = LogisticClassifier(
        lambda = 0, 
        gamma = 0.0, 
        penalty = :l2, 
        fit_intercept = true, 
        penalize_intercept = false, 
        scale_penalty_with_samples = true, 
        solver = nothing), 
  resampling = StratifiedCV(
        nfolds = 3, 
        shuffle = false, 
        rng = Random._GLOBAL_RNG()), 
  measures = MLJBase.LogLoss{Float64}[LogLoss(tol = 2.220446049250313e-16)], 
  cache = true, 
  acceleration = ComputationalResources.CPU1{Nothing}(nothing), 
  lr_1 = LogisticClassifier(
        lambda = 1.0e-5, 
        gamma = 0.0, 
        penalty = :l2, 
        fit_intercept = true, 
        penalize_intercept = false, 
        scale_penalty_with_samples = true, 
        solver = nothing), 
  lr_2 = LogisticClassifier(
        lambda = 0.0001, 
        gamma = 0.0, 
        penalty = :l2, 
        fit_intercept = true, 
        penalize_intercept = false, 
        scale_penalty_with_samples = true, 
        solver = nothing), 
  lr_3 = LogisticClassifier(
        lambda = 0.001, 
        gamma = 0.0, 
        penalty = :l2, 
        fit_intercept = true, 
        penalize_intercept = false, 
        scale_penalty_with_samples = true, 
        solver = nothing), 
  lr_4 = LogisticClassifier(
        lambda = 0.01, 
        gamma = 0.0, 
        penalty = :l2, 
        fit_intercept = true, 
        penalize_intercept = false, 
        scale_penalty_with_samples = true, 
        solver = nothing), 
  lr_5 = LogisticClassifier(
         lambda = 0.1, 
         gamma = 0.0, 
         penalty = :l2, 
         fit_intercept = true, 
         penalize_intercept = false, 
         scale_penalty_with_samples = true, 
         solver = nothing), 
  lr_6 = LogisticClassifier(
         lambda = 0.0, 
         gamma = 0.0, 
         penalty = :l2, 
         fit_intercept = true, 
         penalize_intercept = false, 
         scale_penalty_with_samples = true, 
         solver = nothing), 
  lr_7 = LogisticClassifier(
         lambda = 1.0, 
         gamma = 0.0, 
         penalty = :l2, 
         fit_intercept = true, 
         penalize_intercept = false, 
         scale_penalty_with_samples = true, 
         solver = nothing), 
  lr_8 = LogisticClassifier(
         lambda = 10.0, 
         gamma = 0.0, 
         penalty = :l2, 
         fit_intercept = true, 
         penalize_intercept = false, 
         scale_penalty_with_samples = true, 
         solver = nothing), 
  lr_9 = LogisticClassifier(
         lambda = 100.0, 
         gamma = 0.0, 
         penalty = :l2, 
         fit_intercept = true, 
         penalize_intercept = false, 
         scale_penalty_with_samples = true, 
         solver = nothing), 
  tree_1 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 1.0e-5, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  tree_2 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 0.0001, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  tree_3 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 0.001, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  tree_4 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 0.01, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  tree_5 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 0.1, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  tree_6 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 0.0, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  tree_7 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 1.0, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  tree_8 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 10.0, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  tree_9 = EvoTreeClassifier(
         nrounds = 100, 
         lambda = 100.0, 
         gamma = 0.0, 
         eta = 0.1, 
         max_depth = 6, 
         min_weight = 1.0, 
         rowsample = 1.0, 
         colsample = 1.0, 
         nbins = 64, 
         alpha = 0.5, 
         tree_type = &quot;binary&quot;, 
         rng = Random.TaskLocalRNG()), 
  knn_1 = KNNClassifier(
         K = 5, 
         algorithm = :kdtree, 
         metric = Distances.Euclidean(0.0), 
         leafsize = 10, 
         reorder = true, 
         weights = NearestNeighborModels.Uniform()), 
  knn_2 = KNNClassifier(
         K = 10, 
         algorithm = :kdtree, 
         metric = Distances.Euclidean(0.0), 
         leafsize = 10, 
         reorder = true, 
         weights = NearestNeighborModels.Uniform()), 
  knn_3 = KNNClassifier(
         K = 50, 
         algorithm = :kdtree, 
         metric = Distances.Euclidean(0.0), 
         leafsize = 10, 
         reorder = true, 
         weights = NearestNeighborModels.Uniform()), 
  knn_4 = KNNClassifier(
         K = 100, 
         algorithm = :kdtree, 
         metric = Distances.Euclidean(0.0), 
         leafsize = 10, 
         reorder = true, 
         weights = NearestNeighborModels.Uniform()))</code></pre><h2 id="Targeted-estimation"><a class="docs-heading-anchor" href="#Targeted-estimation">Targeted estimation</a><a id="Targeted-estimation-1"></a><a class="docs-heading-anchor-permalink" href="#Targeted-estimation" title="Permalink"></a></h2><p>Let us move to the targeted estimation step itself. We define the target estimand (the ATE):</p><pre><code class="language-julia hljs">Ψ = ATE(
    scm,
    outcome=:haz01,
    treatment=(parity01=(case=true, control=false),),
)</code></pre><p>Finally run the TMLE procedure:</p><pre><code class="language-julia hljs">tmle_result, _ = tmle!(Ψ, dataset)

tmle_result</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">┌───────────┬───────────┬─────────────────────────┬──────────────┐
│ Estimator │  Estimate │ 95% Confidence Interval │      P-value │
├───────────┼───────────┼─────────────────────────┼──────────────┤
│      TMLE │ -0.455828 │  (-0.484301, -0.427356) │ 2.42817e-165 │
│       OSE │ -0.278072 │  (-0.608253, 0.0521088) │    0.0987458 │
│     Naive │  -0.09666 │                 nothing │      nothing │
└───────────┴───────────┴─────────────────────────┴──────────────┘
</code></pre><hr/><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl">Literate.jl</a>.</em></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../user_guide/misc/">« Miscellaneous</a><a class="docs-footer-nextpage" href="../double_robustness/">Model Misspecification &amp; Double Robustness »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Wednesday 16 August 2023 16:05">Wednesday 16 August 2023</span>. Using Julia version 1.9.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
