var documenterSearchIndex = {"docs":
[{"location":"resources/#Resources","page":"Resources","title":"Resources","text":"","category":"section"},{"location":"resources/","page":"Resources","title":"Resources","text":"Targeted Learning is a difficult topic, while it is not strictly necessary to understand the details to use this package it can certainly help. Here is an incomplete list of external resources that I found useful in my journey.","category":"page"},{"location":"resources/#Websites","page":"Resources","title":"Websites","text":"","category":"section"},{"location":"resources/","page":"Resources","title":"Resources","text":"These are two very clear introductions to causal inference and semi-parametric estimation:","category":"page"},{"location":"resources/","page":"Resources","title":"Resources","text":"Introduction to Modern Causal Inference (Alejandro Schuler, Mark J. van der Laan).\nA Ride in Targeted Learning Territory (David Benkeser, Antoine Chambaz).","category":"page"},{"location":"resources/#Text-Books","page":"Resources","title":"Text Books","text":"","category":"section"},{"location":"resources/","page":"Resources","title":"Resources","text":"Targeted Learning (Mark J. van der Laan, Sherri Rose).","category":"page"},{"location":"resources/#Journal-articles","page":"Resources","title":"Journal articles","text":"","category":"section"},{"location":"resources/","page":"Resources","title":"Resources","text":"Semiparametric doubly robust targeted double machine learning: a review (Edward H. Kennedy).","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/","page":"API Reference","title":"API Reference","text":"Modules = [TMLE]\nPrivate = false","category":"page"},{"location":"api/#HypothesisTests.OneSampleTTest","page":"API Reference","title":"HypothesisTests.OneSampleTTest","text":"OneSampleTTest(r::ComposedEstimate, Ψ₀=0)\n\nPerforms a T test on the ComposedEstimate.\n\n\n\n\n\n","category":"type"},{"location":"api/#HypothesisTests.OneSampleTTest-2","page":"API Reference","title":"HypothesisTests.OneSampleTTest","text":"OneSampleTTest(r::EICEstimate, Ψ₀=0)\n\nPerforms a T test on the EICEstimate.\n\n\n\n\n\n","category":"type"},{"location":"api/#HypothesisTests.OneSampleZTest","page":"API Reference","title":"HypothesisTests.OneSampleZTest","text":"OneSampleZTest(r::ComposedEstimate, Ψ₀=0)\n\nPerforms a T test on the ComposedEstimate.\n\n\n\n\n\n","category":"type"},{"location":"api/#HypothesisTests.OneSampleZTest-2","page":"API Reference","title":"HypothesisTests.OneSampleZTest","text":"OneSampleZTest(r::EICEstimate, Ψ₀=0)\n\nPerforms a Z test on the EICEstimate.\n\n\n\n\n\n","category":"type"},{"location":"api/#TMLE.OSE-Tuple{}","page":"API Reference","title":"TMLE.OSE","text":"OSE(;models=default_models(), resampling=nothing, ps_lowerbound=1e-8, machine_cache=false)\n\nDefines a One Step Estimator using the specified models for estimation of the nuisance parameters. The estimator is a  function that can be applied to estimate estimands for a dataset.\n\nArguments\n\nmodels: A NamedTuple{variables}(models) where the variables are the outcome variables modeled by the models.\nresampling: Outer resampling strategy. Setting it to nothing (default) falls back to vanilla estimation while \n\nany valid MLJ.ResamplingStrategy will result in CV-OSE.\n\nps_lowerbound: Lowerbound for the propensity score to avoid division by 0. The special value nothing will \n\nresult in a data adaptive definition as described in here.\n\nmachine_cache: Whether MLJ.machine created during estimation should cache data.\n\nExample\n\nusing MLJLinearModels\nmodels = (Y = LinearRegressor(), T = LogisticClassifier())\nose = OSE()\nΨ̂ₙ, cache = ose(Ψ, dataset)\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.SCM","page":"API Reference","title":"TMLE.SCM","text":"A SCM is simply a wrapper around a MetaGraph over a Directed Acyclic Graph.\n\n\n\n\n\n","category":"type"},{"location":"api/#TMLE.TMLEE-Tuple{}","page":"API Reference","title":"TMLE.TMLEE","text":"TMLEE(;models=default_models(), resampling=nothing, ps_lowerbound=1e-8, weighted=false, tol=nothing, machine_cache=false)\n\nDefines a TMLE estimator using the specified models for estimation of the nuisance parameters. The estimator is a  function that can be applied to estimate estimands for a dataset.\n\nArguments\n\nmodels: A NamedTuple{variables}(models) where the variables are the outcome variables modeled by the models.\nresampling: Outer resampling strategy. Setting it to nothing (default) falls back to vanilla TMLE while \n\nany valid MLJ.ResamplingStrategy will result in CV-TMLE.\n\nps_lowerbound: Lowerbound for the propensity score to avoid division by 0. The special value nothing will \n\nresult in a data adaptive definition as described in here.\n\nweighted: Whether the fluctuation model is a classig GLM or a weighted version. The weighted fluctuation has \n\nbeen show to be more robust to positivity violation in practice.\n\ntol: This is not used at the moment.\nmachine_cache: Whether MLJ.machine created during estimation should cache data.\n\nExample\n\nusing MLJLinearModels\ntmle = TMLEE()\nΨ̂ₙ, cache = tmle(Ψ, dataset)\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.TreatmentTransformer-Tuple{}","page":"API Reference","title":"TMLE.TreatmentTransformer","text":"TreatmentTransformer(;encoder=encoder())\n\nTreatments in TMLE are represented by CategoricalArrays. If a treatment column has type OrderedFactor, then its integer representation is used, make sure that  the levels correspond to your expectations. All other columns are one-hot encoded.\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.estimate-Tuple{TMLE.ComposedEstimate}","page":"API Reference","title":"Distributions.estimate","text":"Distributions.estimate(r::ComposedEstimate)\n\nRetrieves the final estimate: after the TMLE step.\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.estimate-Tuple{Union{TMLE.OSEstimate, TMLE.TMLEstimate}}","page":"API Reference","title":"Distributions.estimate","text":"Distributions.estimate(r::EICEstimate)\n\nRetrieves the final estimate: after the TMLE step.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.var-Tuple{TMLE.ComposedEstimate}","page":"API Reference","title":"Statistics.var","text":"var(r::ComposedEstimate)\n\nComputes the estimated variance associated with the estimate.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.var-Tuple{Union{TMLE.OSEstimate, TMLE.TMLEstimate}}","page":"API Reference","title":"Statistics.var","text":"var(r::EICEstimate)\n\nComputes the estimated variance associated with the estimate.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.StaticSCM-Tuple{Any, Any, Any}","page":"API Reference","title":"TMLE.StaticSCM","text":"A plate Structural Causal Model where:\n\nFor all outcomes: oᵢ = fᵢ(treatments, confounders, outcomeextracovariates)\nFor all treatments: tⱼ = fⱼ(confounders)\n\nExample\n\nStaticSCM([:Y], [:T₁, :T₂], [:W₁, :W₂, :W₃]; outcomeextracovariates=[:C])\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.brute_force_ordering-Tuple{Any}","page":"API Reference","title":"TMLE.brute_force_ordering","text":"brute_force_ordering(estimands; η_counts = nuisance_counts(estimands))\n\nFinds an optimal ordering of the estimands to minimize maximum cache size.  The approach is a brute force one, all permutations are generated and evaluated,  if a minimum is found fast it is immediatly returned. The theoretical complexity is in O(N!). However due to the stop fast approach and  the shuffling, this is actually expected to be much smaller than that.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.compose-Tuple{Any, Vararg{Any}}","page":"API Reference","title":"TMLE.compose","text":"compose(f, estimation_results::Vararg{EICEstimate, N}) where N\n\nProvides an estimator of f(estimation_results...).\n\nMathematical details\n\nThe following is a summary from Asymptotic Statistics, A. W. van der Vaart.\n\nConsider k TMLEs computed from a dataset of size n and embodied by Tₙ = (T₁,ₙ, ..., Tₖ,ₙ).  Since each of them is asymptotically normal, the multivariate CLT provides the joint  distribution:\n\n√n(Tₙ - Ψ₀) ↝ N(0, Σ),\n\nwhere Σ is the covariance matrix of the TMLEs influence curves.\n\nLet f:ℜᵏ→ℜᵐ, be a differentiable map at Ψ₀. Then, the delta method provides the limiting distribution of √n(f(Tₙ) - f(Ψ₀)). Because Tₙ is normal, the result is:\n\n√n(f(Tₙ) - f(Ψ₀)) ↝ N(0, ∇f(Ψ₀) ̇Σ ̇(∇f(Ψ₀))ᵀ),\n\nwhere ∇f(Ψ₀):ℜᵏ→ℜᵐ is a linear map such that by abusing notations and identifying the  function with the multiplication matrix: ∇f(Ψ₀):h ↦ ∇f(Ψ₀) ̇h. And the matrix ∇f(Ψ₀) is  the jacobian of f at Ψ₀.\n\nHence, the only thing we need to do is:\n\nCompute the covariance matrix Σ\nCompute the jacobian ∇f, which can be done using Julia's automatic differentiation facilities.\nThe final estimator is normal with mean f₀=f(Ψ₀) and variance σ₀=∇f(Ψ₀) ̇Σ ̇(∇f(Ψ₀))ᵀ\n\nArguments\n\nf: An array-input differentiable map.\nestimation_results: 1 or more EICEstimate structs.\n\nExamples\n\nAssuming res₁ and res₂ are TMLEs:\n\nf(x, y) = [x^2 - y, y - 3x]\ncompose(f, res₁, res₂)\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.from_dict!-Union{Tuple{Dict{T, Any}}, Tuple{T}} where T","page":"API Reference","title":"TMLE.from_dict!","text":"from_dict!(d::Dict)\n\nConverts a dictionary to a TMLE struct.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.groups_ordering-Tuple{Any}","page":"API Reference","title":"TMLE.groups_ordering","text":"groups_ordering(estimands)\n\nThis will order estimands based on: propensity score first, outcome mean second. This heuristic should  work reasonably well in practice. It could be optimized further by:\n\nOrganising the propensity score groups that share similar components to be close together. \nBrute forcing the ordering of these groups to find an optimal one.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.to_dict-Tuple{T} where T<:Union{TMLE.CausalATE, TMLE.CausalCM, TMLE.CausalIATE}","page":"API Reference","title":"TMLE.to_dict","text":"to_dict(Ψ::T) where T <: CausalCMCompositeEstimands\n\nConverts Ψ to a dictionary that can be serialized.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.to_dict-Tuple{T} where T<:Union{TMLE.StatisticalATE, TMLE.StatisticalCM, TMLE.StatisticalIATE}","page":"API Reference","title":"TMLE.to_dict","text":"to_dict(Ψ::T) where T <: StatisticalCMCompositeEstimand\n\nConverts Ψ to a dictionary that can be serialized.\n\n\n\n\n\n","category":"method"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"EditURL = \"../../../examples/double_robustness.jl\"","category":"page"},{"location":"examples/double_robustness/#Model-Misspecification-and-Double-Robustness","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"","category":"section"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"In this example we illustrate the double robustness property of TMLE in the classical backdoor adjustment setting for the Average Treatment Effect.","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"Let's consider the following simple data generating process:","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"beginaligned\nW sim mathcalN(0 1) \nT sim mathcalB(frac11 + e^-(03 - 05 cdot W)) \nY sim mathcalN(e^1 - 10 cdot T + W 1)\nendaligned","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"using TMLE\nusing MLJ\nusing Distributions\nusing StableRNGs\nusing LogExpFunctions\nusing MLJGLMInterface\nusing DataFrames\nusing CairoMakie\n\n\nμY(T, W) = exp.(1 .- 10T .+ 1W)\n\nfunction generate_data(;n = 1000, rng = StableRNG(123))\n    W  = rand(rng, Normal(), n)\n    μT = logistic.(0.3 .- 0.5W)\n    T  = float(rand(rng, n) .< μT)\n    ϵ = rand(rng, Normal(), n)\n    Y  = μY(T, W) .+ ϵ\n    Y₁ = μY(ones(n), W) .+ ϵ\n    Y₀ = μY(zeros(n), W) .+ ϵ\n    return DataFrame(\n        W = W,\n        T = T,\n        Tcat = categorical(T),\n        Y = Y,\n        Y₁ = Y₁,\n        Y₀ = Y₀\n    )\nend\n\nfunction plotY(data)\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel=\"W\", ylabel=\"Y\")\n    for (key, group) in pairs(groupby(data, :T))\n        scatter!(ax, group.Y, group.W, label=string(\"T=\",key.T))\n    end\n    axislegend()\n    return fig\nend\n\ndata = generate_data(;n = 1000, rng = StableRNG(123))\nplotY(data)","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"Y is thus a non linear function of T and W. Despite the simplicity of the example, it is difficult to find a closed form solution for the true Average Causal Effect. However, since we know the generating process, we can approximate it using a Monte-Carlo approximation. In the next two sections, we compare Linear inference and TMLE and see how well they cover this Monte-Carlo approximation.","category":"page"},{"location":"examples/double_robustness/#Estimation-using-a-Linear-model","page":"Model Misspecification & Double Robustness","title":"Estimation using a Linear model","text":"","category":"section"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"We first propose to estimate the effect size using the classic linear inference method. Because our model does not contain the data generating process (and is hence mis-specified), there is no guarantee that the true effect size will be covered by our confidence interval. In fact, as the sample size grows, the confidence interval will inevitably shrink and fail to cover the ground truth. This can be seen from the following animation:","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"function linear_inference(data)\n    mach = machine(LinearRegressor(), data[!, [:W, :T]], data.Y)\n    fit!(mach, verbosity=0)\n    coeftable = report(mach).coef_table\n    Trow = findfirst(x -> x == \"T\", coeftable.rownms)\n    coef = coeftable.cols[1][Trow]\n    lb = coeftable.cols[end - 1][Trow]\n    ub = coeftable.cols[end][Trow]\n    return (coef, lb, ub)\nend\n\nfunction repeat_inference(inference_method; n=1000, K=100)\n    estimates = Vector{Float64}(undef, K)\n    errors = Vector{Float64}(undef, K)\n    mcestimates = Vector{Float64}(undef, K)\n    for k in 1:K\n        data = generate_data(;n=n, rng=StableRNG(k))\n        est, lb, ub = inference_method(data)\n        estimates[k] = est\n        errors[k] = ub - est\n        mcestimates[k] = mean(data.Y₁ .- data.Y₀)\n    end\n    return estimates, errors, mcestimates\nend\n\nfunction plot_coverage(inference_method; n=1000, K=100)\n    fig = Figure()\n    title = Observable(string(\"N=\", n))\n    ax = Axis(fig[1, 1], xlabel=\"Repetition\", ylabel=\"Estimate size\", title=title)\n    ks = 1:K\n    estimates, errors, mcestimates = repeat_inference(inference_method; n=n, K=K)\n    estimates = Observable(estimates)\n    errors = Observable(errors)\n    mcestimates = Observable(mcestimates)\n    errorbars!(ax, ks, estimates, errors, color=:red, whiskerwidth = 10)\n    scatter!(ax, ks, estimates, color=:red, label=replace(string(inference_method), \"_\" => \" \"))\n    scatter!(ax, ks, mcestimates, label=\"Monte Carlo estimate\")\n    axislegend()\n    return fig, estimates, errors, mcestimates, title\nend\n\nfunction update_observables!(estimates, errors, mcestimates, title, inference_method; n=1000, K=100)\n    newestimates, newerrors, newmcestimates = repeat_inference(inference_method; n=n, K=K)\n    estimates[] = newestimates\n    errors[] = newerrors\n    mcestimates[] = newmcestimates\n    title[] = string(\"N=\", n)\nend\n\nfunction make_animation(inference_method)\n    Ns = [10_000, 25_000, 50_000, 75_000, 100_000, 250_000, 500_000]\n    fig, estimates, errors, mcestimates, title = plot_coverage(inference_method; n=10_000, K=100)\n    record(fig, \"$(inference_method).gif\", Ns; framerate = 1) do n\n        update_observables!(estimates, errors, mcestimates, title, inference_method; n=n, K=100)\n    end\nend\n\nmake_animation(linear_inference)","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"(Image: Linear Inference)","category":"page"},{"location":"examples/double_robustness/#Estimation-using-TMLE","page":"Model Misspecification & Double Robustness","title":"Estimation using TMLE","text":"","category":"section"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"To solve this issue, we will now use TMLE to estimate the Average Treatment Effect. We will keep the mis-specified linear model to estimate E[Y|T,W] but will estimate p(T|W) with a logistic regression which turns out to be the true generating model in this case. Because TMLE is double robust we see that we now have full coverage of the ground truth.","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"function tmle_inference(data)\n    Ψ = ATE(\n        outcome=:Y,\n        treatment_values=(Tcat=(case=1.0, control=0.0),),\n        treatment_confounders=(Tcat=[:W],)\n    )\n    models = (Y=with_encoder(LinearRegressor()), Tcat=LinearBinaryClassifier())\n    tmle = TMLEE(models=models)\n    result, _ = tmle(Ψ, data; verbosity=0)\n    lb, ub = confint(OneSampleTTest(result))\n    return (TMLE.estimate(result), lb, ub)\nend\n\nmake_animation(tmle_inference)","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"(Image: TMLE Inference)","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"This page was generated using Literate.jl.","category":"page"},{"location":"user_guide/misc/#Miscellaneous","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"section"},{"location":"user_guide/misc/#Adjustment-Methods","page":"Miscellaneous","title":"Adjustment Methods","text":"","category":"section"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"An adjustment method is a function that transforms a causal estimand into a statistical estimand using an associated SCM. At the moment, the only available adjustment method is the backdoor adjustment.","category":"page"},{"location":"user_guide/misc/#Backdoor-Adjustment","page":"Miscellaneous","title":"Backdoor Adjustment","text":"","category":"section"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"The adjustment set consists of all the treatment variable's parents. Additional covariates used to fit the outcome model can be provided via outcome_extra.","category":"page"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"BackdoorAdjustment(;outcome_extra_covariates=[:C])","category":"page"},{"location":"user_guide/misc/#Treatment-Transformer","page":"Miscellaneous","title":"Treatment Transformer","text":"","category":"section"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"To account for the fact that treatment variables are categorical variables we provide a MLJ compliant transformer that will either:","category":"page"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Retrieve the floating point representation of a treatment if it has a natural ordering\nOne hot encode it otherwise","category":"page"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Such transformer can be created with:","category":"page"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"TreatmentTransformer(;encoder=encoder())","category":"page"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"where encoder is a OneHotEncoder.","category":"page"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"The with_encoder(model; encoder=TreatmentTransformer()) provides a shorthand to combine a TreatmentTransformer with another MLJ model in a pipeline.","category":"page"},{"location":"user_guide/misc/","page":"Miscellaneous","title":"Miscellaneous","text":"Of course you are also free to define your own strategy!","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"CurrentModule = TMLE","category":"page"},{"location":"user_guide/estimation/#Estimation","page":"Estimation","title":"Estimation","text":"","category":"section"},{"location":"user_guide/estimation/#Estimating-a-single-Estimand","page":"Estimation","title":"Estimating a single Estimand","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"using Random\nusing Distributions\nusing DataFrames\nusing StableRNGs\nusing CategoricalArrays\nusing TMLE\nusing LogExpFunctions\nusing MLJLinearModels\nusing MLJ\n\nfunction make_dataset(;n=1000)\n    rng = StableRNG(123)\n    # Confounders\n    W₁₁= rand(rng, Uniform(), n)\n    W₁₂ = rand(rng, Uniform(), n)\n    W₂₁= rand(rng, Uniform(), n)\n    W₂₂ = rand(rng, Uniform(), n)\n    # Covariates\n    C = rand(rng, Uniform(), n)\n    # Treatment | Confounders\n    T₁ = rand(rng, Uniform(), n) .< logistic.(0.5sin.(W₁₁) .- 1.5W₁₂)\n    T₂ = rand(rng, Uniform(), n) .< logistic.(-3W₂₁ - 1.5W₂₂)\n    # Target | Confounders, Covariates, Treatments\n    Y = 1 .+ 2W₂₁ .+ 3W₂₂ .+ W₁₁ .- 4C.*T₁ .- 2T₂.*T₁.*W₁₂ .+ rand(rng, Normal(0, 0.1), n)\n    return DataFrame(\n        W₁₁ = W₁₁, \n        W₁₂ = W₁₂,\n        W₂₁ = W₂₁,\n        W₂₂ = W₂₂,\n        C   = C,\n        T₁  = categorical(T₁),\n        T₂  = categorical(T₂),\n        Y   = Y\n        )\nend\ndataset = make_dataset(n=10000)\nscm = SCM([\n    :Y  => [:T₁, :T₂, :W₁₁, :W₁₂, :W₂₁, :W₂₂, :C],\n    :T₁ => [:W₁₁, :W₁₂],\n    :T₂ => [:W₂₁, :W₂₂]\n]\n)","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Once a statistical estimand has been defined, we can proceed with estimation. At the moment, we provide 3 main types of estimators:","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Targeted Maximum Likelihood Estimator (TMLEE)\nOne-Step Estimator (OSE)\nNaive Plugin Estimator (NAIVE)","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Drawing from the example dataset and SCM from the Walk Through section, we can estimate the ATE for T₁. Let's use TMLE:","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Ψ₁ = ATE(\n    outcome=:Y, \n    treatment_values=(T₁=(case=true, control=false),),\n    treatment_confounders=(T₁=[:W₁₁, :W₁₂],),\n    outcome_extra_covariates=[:C]\n)\nmodels = (\n    Y=with_encoder(LinearRegressor()), \n    T₁=LogisticClassifier(),\n    T₂=LogisticClassifier(),\n)\ntmle = TMLEE(models=models)\nresult₁, cache = tmle(Ψ₁, dataset);\nresult₁\nnothing # hide","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"We see that both models corresponding to variables Y and T₁ were fitted in the process but that the model for T₂ was not because it was not necessary to estimate this estimand.","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"The cache contains estimates for the nuisance functions that were necessary to estimate the ATE. For instance, we can see what is the value of epsilon corresponding to the clever covariate.","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"ϵ = last_fluctuation_epsilon(cache)","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"The result₁ structure corresponds to the estimation result and should report 3 main elements:","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"A point estimate.\nA 95% confidence interval.\nA p-value (Corresponding to the test that the estimand is different than 0).","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"This is only summary statistics but since both the TMLE and OSE are asymptotically linear estimators, standard Z/T tests from HypothesisTests.jl can be performed.","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"tmle_test_result₁ = OneSampleTTest(result₁)","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"We could now get an interest in the Average Treatment Effect of T₂ that we will estimate with an OSE:","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Ψ₂ = ATE(\n    outcome=:Y, \n    treatment_values=(T₂=(case=true, control=false),),\n    treatment_confounders=(T₂=[:W₂₁, :W₂₂],),\n    outcome_extra_covariates=[:C]\n)\nose = OSE(models=models)\nresult₂, cache = ose(Ψ₂, dataset;cache=cache);\nresult₂\nnothing # hide","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Again, required nuisance functions are fitted and stored in the cache.","category":"page"},{"location":"user_guide/estimation/#CV-Estimation","page":"Estimation","title":"CV-Estimation","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Both TMLE and OSE can be used with sample-splitting, which, for an additional computational cost, further reduces the assumptions we need to make regarding our data generating process (see here). Note that this sample-splitting procedure should not be confused with the sample-splitting happening in Super Learning. Using both CV-TMLE and Super-Learning will result in two nested sample-splitting loops.","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"To leverage sample-splitting, simply specify a resampling strategy when building an estimator:","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"cvtmle = TMLEE(models=models, resampling=CV())\ncvresult₁, _ = cvtmle(Ψ₁, dataset);","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Similarly, one could build CV-OSE:","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"cvose = OSE(models=models, resampling=CV(nfolds=3))","category":"page"},{"location":"user_guide/estimation/#Caching-model-fits","page":"Estimation","title":"Caching model fits","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Let's now see how the cache can be reused with a new estimand, say the Total Average Treatment Effect of both T₁ and T₂.","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Ψ₃ = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=true, control=false), \n        T₂=(case=true, control=false)\n    ),\n    treatment_confounders=(\n        T₁=[:W₁₁, :W₁₂], \n        T₂=[:W₂₁, :W₂₂],\n    ),\n    outcome_extra_covariates=[:C]\n)\nresult₃, cache = tmle(Ψ₃, dataset; cache=cache);\nresult₃\nnothing # hide","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"This time only the model for Y is fitted again while reusing the models for T₁ and T₂. Finally, let's see what happens if we estimate the IATE between T₁ and T₂.","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"Ψ₄ = IATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=true, control=false), \n        T₂=(case=true, control=false)\n    ),\n    treatment_confounders=(\n        T₁=[:W₁₁, :W₁₂], \n        T₂=[:W₂₁, :W₂₂],\n    ),\n    outcome_extra_covariates=[:C]\n)\nresult₄, cache = tmle(Ψ₄, dataset; cache=cache);\nresult₄\nnothing # hide","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"All nuisance functions have been reused, only the fluctuation is fitted!","category":"page"},{"location":"user_guide/estimation/#Composing-Estimands","page":"Estimation","title":"Composing Estimands","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"By leveraging the multivariate Central Limit Theorem and Julia's automatic differentiation facilities, we can estimate any estimand which is a function of already estimated estimands. By default, TMLE.jl will use Zygote but since we are using AbstractDifferentiation.jl you can change the backend to your favorite AD system.","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"For instance, by definition of the IATE, we should be able to retrieve:","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"IATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 = ATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 - ATE_T_1=0 T_2=0 rightarrow 1 - ATE_T_1=0 rightarrow 1 T_2=0","category":"page"},{"location":"user_guide/estimation/","page":"Estimation","title":"Estimation","text":"first_ate = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=true, control=false), \n        T₂=(case=false, control=false)),\n    treatment_confounders=(\n        T₁=[:W₁₁, :W₁₂], \n        T₂=[:W₂₁, :W₂₂],\n    ),\n)\nfirst_ate_result, cache = tmle(first_ate, dataset, cache=cache, verbosity=0);\n\nsecond_ate = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=false, control=false), \n        T₂=(case=true, control=false)),\n    treatment_confounders=(\n        T₁=[:W₁₁, :W₁₂], \n        T₂=[:W₂₁, :W₂₂],\n    ),\n    )\nsecond_ate_result, cache = tmle(second_ate, dataset, cache=cache, verbosity=0);\n\ncomposed_iate_result = compose(\n    (x, y, z) -> x - y - z, \n    result₃, first_ate_result, second_ate_result\n)\nisapprox(\n    estimate(result₄),\n    estimate(composed_iate_result),\n    atol=0.1\n)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"CurrentModule = TMLE","category":"page"},{"location":"walk_through/#Walk-Through","page":"Walk Through","title":"Walk Through","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The goal of this section is to provide a comprehensive (but non-exhaustive) illustration of the estimation process provided in TMLE.jl. For an in-depth explanation, please refer to the User Guide.","category":"page"},{"location":"walk_through/#The-Dataset","page":"Walk Through","title":"The Dataset","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"TMLE.jl is compatible with any dataset respecting the Tables.jl interface, that is for instance, a NamedTuple, a DataFrame, an Arrow.Table etc... In this section, we will be working with the same dataset all along.","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"⚠️ One thing to note is that treatment variables as well as binary outcomes must be encoded as categorical variables in the dataset (see MLJ Working with categorical data).","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The dataset is generated as follows:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"using TMLE\nusing Random\nusing Distributions\nusing DataFrames\nusing StableRNGs\nusing CategoricalArrays\nusing TMLE\nusing LogExpFunctions\nusing MLJLinearModels\n\nfunction make_dataset(;n=1000)\n    rng = StableRNG(123)\n    # Confounders\n    W₁₁= rand(rng, Uniform(), n)\n    W₁₂ = rand(rng, Uniform(), n)\n    W₂₁= rand(rng, Uniform(), n)\n    W₂₂ = rand(rng, Uniform(), n)\n    # Covariates\n    C = rand(rng, Uniform(), n)\n    # Treatment | Confounders\n    T₁ = rand(rng, Uniform(), n) .< logistic.(0.5sin.(W₁₁) .- 1.5W₁₂)\n    T₂ = rand(rng, Uniform(), n) .< logistic.(-3W₂₁ - 1.5W₂₂)\n    # Target | Confounders, Covariates, Treatments\n    Y = 1 .+ 2W₂₁ .+ 3W₂₂ .+ W₁₁ .- 4C.*T₁ .- 2T₂.*T₁.*W₁₂ .+ rand(rng, Normal(0, 0.1), n)\n    return DataFrame(\n        W₁₁ = W₁₁, \n        W₁₂ = W₁₂,\n        W₂₁ = W₂₁,\n        W₂₂ = W₂₂,\n        C   = C,\n        T₁  = categorical(T₁),\n        T₂  = categorical(T₂),\n        Y   = Y\n        )\nend\ndataset = make_dataset()\nnothing # hide","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Even though the role of a variable (treatment, outcome, confounder, ...) is relative to the problem setting, this dataset can intuitively be decomposed into:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"1 Outcome variable (Y).\n2 Treatment variables (T₁ T₂) with confounders (W₁₁ W₁₂) and (W₂₁ W₂₂) respectively.\n1 Outcome extra covariate variable (C).","category":"page"},{"location":"walk_through/#The-Structural-Causal-Model","page":"Walk Through","title":"The Structural Causal Model","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The modeling stage starts from the definition of a Structural Causal Model (SCM). This is simply a list of relationships between the random variables in our dataset. See Structural Causal Models for an in-depth explanation. For our purposes, because we know the data generating process, we can define it as follows:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"scm = SCM([\n    :Y  => [:T₁, :T₂, :W₁₁, :W₁₂, :W₂₁, :W₂₂, :C],\n    :T₁ => [:W₁₁, :W₁₂],\n    :T₂ => [:W₂₁, :W₂₂]\n]\n)","category":"page"},{"location":"walk_through/#The-Causal-Estimands","page":"Walk Through","title":"The Causal Estimands","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"From the previous causal model we can ask multiple causal questions, all represented by distinct causal estimands. The set of available estimands types can be listed as follow:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"AVAILABLE_ESTIMANDS","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"At the moment there are 3 main causal estimands in TMLE.jl, we provide below a few examples.","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The Counterfactual Mean:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"cm = CM(\n    outcome = :Y,\n    treatment_values = (T₁=true,) \n)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The Average Treatment Effect:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"total_ate = ATE(\n    outcome = :Y,\n    treatment_values = (\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    ) \n)\nmarginal_ate_t1 = ATE(\n    outcome = :Y,\n    treatment_values = (T₁=(case=1, control=0),) \n)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The Interaction Average Treatment Effect:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"iate = IATE(\n    outcome = :Y,\n    treatment_values = (\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    ) \n)","category":"page"},{"location":"walk_through/#Identification","page":"Walk Through","title":"Identification","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Identification is the process by which a Causal Estimand is turned into a Statistical Estimand, that is, a quantity we may estimate from data. This is done via the identify function which also takes in the SCM:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"statistical_iate = identify(iate, scm)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Alternatively, you can also directly define the statistical parameters (see Estimands).","category":"page"},{"location":"walk_through/#Estimation","page":"Walk Through","title":"Estimation","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Then each parameter can be estimated by building an estimator (which is simply a function) and evaluating it on data. For illustration, we will keep the models simple. We define a Targeted Maximum Likelihood Estimator:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"models = (\n    Y  = with_encoder(LinearRegressor()),\n    T₁ = LogisticClassifier(),\n    T₂ = LogisticClassifier()\n)\ntmle = TMLEE(models=models)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Because we haven't identified the cm causal estimand yet, we need to provide the scm as well to the estimator:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"result, cache = tmle(cm, scm, dataset);\nresult","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Statistical Estimands can be estimated without a SCM, let's use the One-Step estimator:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"ose = OSE(models=models)\nresult, cache = ose(statistical_iate, dataset)\nresult","category":"page"},{"location":"walk_through/#Hypothesis-Testing","page":"Walk Through","title":"Hypothesis Testing","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Both TMLE and OSE asymptotically follow a Normal distribution. It means we can perform standard T/Z tests of null hypothesis. TMLE.jl extends the method provided by the HypothesisTests.jl package that can be used as follows.","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"OneSampleTTest(result)","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"CurrentModule = TMLE","category":"page"},{"location":"user_guide/scm/#Structural-Causal-Models","page":"Structural Causal Models","title":"Structural Causal Models","text":"","category":"section"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"Even if you don't have to, it can be useful to define a Structural Causal Model (SCM) for your problem. A SCM is a directed acyclic graph that describes the causal relationships between the random variables under study.","category":"page"},{"location":"user_guide/scm/#Incremental-Construction","page":"Structural Causal Models","title":"Incremental Construction","text":"","category":"section"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"All models are wrong? Well maybe not the following:","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"using TMLE # hide\nscm = SCM()","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"This model does not say anything about the random variables and is thus not really useful. Let's assume that we are interested in an outcome Y and that this outcome is determined by 8 other random variables. We can add this assumption to the model","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"add_equation!(scm, :Y => [:T₁, :T₂, :W₁₁, :W₁₂, :W₂₁, :W₂₂, :W, :C])","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"Let's now assume that we have a more complete knowledge of the problem and we also know how T₁ and T₂ depend on the rest of the variables in the system.","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"add_equations!(scm, :T₁ => [:W₁₁, :W₁₂, :W], :T₂ => [:W₂₁, :W₂₂, :W])","category":"page"},{"location":"user_guide/scm/#One-Step-Construction","page":"Structural Causal Models","title":"One Step Construction","text":"","category":"section"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"Instead of constructing the SCM incrementally, one can provide all the specified equations at once:","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"scm = SCM([\n    :Y  => [:T₁, :T₂, :W₁₁, :W₁₂, :W₂₁, :W₂₂, :W, :C],\n    :T₁ => [:W₁₁, :W₁₂, :W],\n    :T₂ => [:W₂₁, :W₂₂, :W]\n])","category":"page"},{"location":"user_guide/scm/#Classic-Structural-Causal-Models","page":"Structural Causal Models","title":"Classic Structural Causal Models","text":"","category":"section"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"There are many cases where we are interested in estimating the causal effect of a some treatment variables on a some outcome variables. If all treatment variables share the same set of confounders, we can quickly define the associated SCM with the StaticSCM interface:","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"scm = StaticSCM(\n    outcomes=[:Y₁, :Y₂], \n    treatments=[:T₁, :T₂], \n    confounders=[:W₁, :W₂];\n)","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"where outcome_extra_covariates is a set of extra variables that are causal of the outcomes but are not of direct interest in the study.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"CurrentModule = TMLE","category":"page"},{"location":"user_guide/estimands/#Estimands","page":"Estimands","title":"Estimands","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Most causal questions can be translated into a causal estimand. Usually either an interventional or counterfactual quantity. What would have been the outcome if I had set this variable to this value? When identified, this causal estimand translates to a statistical estimand which can be estimated from data. From a mathematical standpoint, an estimand (Psi) is a functional, that is a function that takes as input a probability distribution (from a model mathcalM), and outputs a real number or vector of real numbers.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Psi mathcalM rightarrow mathbbR^p","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"At the moment, most of the work in this package has been focused on estimands that are composite functions of the counterfactual mean which is easily identified via backdoor adjustment and for which the gradient is well known.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"In what follows, P is a probability distribution generating an outcome Y, a random vector of \"treatment\" variables textbfT and a random vector of \"confounding\" variables textbfW. For the examples, we will assume two treatment variables T₁ and T₂ taking either values 0 or 1. The SCM is given by:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"using TMLE\nscm = StaticSCM(\n    outcomes=[:Y], \n    treatments=[:T₁, :T₂], \n    confounders=[:W]\n)","category":"page"},{"location":"user_guide/estimands/#The-Counterfactual-Mean-(CM)","page":"Estimands","title":"The Counterfactual Mean (CM)","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Question:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"What would have been the mean of Y had we set textbfT=textbft?","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Estimand:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"CM_textbft(P) = mathbbEYdo(textbfT=textbft)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Statistical Estimand (via backdoor adjustment):","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"CM_textbft(P) = mathbbE_textbfWmathbbEYtextbfT=textbft textbfW","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"TMLE.jl Example","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A causal estimand is given by:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"causalΨ = CM(outcome=:Y, treatment_values=(T₁=1, T₂=0))","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A corresponding statistical estimand can be identified via backdoor adjustment using the scm:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = identify(causalΨ, scm)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"or defined directly:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = CM(\n    outcome=:Y, \n    treatment_values=(T₁=1, T₂=0),\n    treatment_confounders=(T₁=[:W], T₂=[:W])\n)","category":"page"},{"location":"user_guide/estimands/#The-Average-Treatment-Effect","page":"Estimands","title":"The Average Treatment Effect","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Question:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"What would be the average difference on Y if we switch the treatment levels from textbft_1 to textbft_2?","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Estimand:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"ATE_textbft_1 rightarrow textbft_2(P) = mathbbEYdo(textbfT=textbft_2) - mathbbEYdo(textbfT=textbft_1)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Statistical Estimand (via backdoor adjustment):","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"beginaligned\nATE_textbft_1 rightarrow textbft_2(P) = CM_textbft_2(P) - CM_textbft_1(P) \n= mathbbE_textbfWmathbbEYtextbfT=textbft_2 textbfW - mathbbEYtextbfT=textbft_1 textbfW\nendaligned","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"TMLE.jl Example","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A causal estimand is given by:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"causalΨ = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    )\n)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A corresponding statistical estimand can be identified via backdoor adjustment using the scm:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = identify(causalΨ, scm)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"or defined directly:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    ),\n    treatment_confounders=(T₁=[:W], T₂=[:W])\n)","category":"page"},{"location":"user_guide/estimands/#The-Interaction-Average-Treatment-Effect","page":"Estimands","title":"The Interaction Average Treatment Effect","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Question:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Interactions can be defined up to any order but we restrict the interpretation to two variables. Is the Total Average Treatment Effect of T₁ and T₂ different from the sum of their respective marginal Average Treatment Effects? Is there a synergistic additive effect between T₁ and T₂ on Y.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"For a general higher-order definition, please refer to Higher-order interactions in statistical physics and machine learning: A model-independent solution to the inverse problem at equilibrium.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Estimand:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"For two points interaction with both treatment and control levels 0 and 1 for ease of notation:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"IATE_0 rightarrow 1 0 rightarrow 1(P) = mathbbEYdo(T_1=1 T_2=1) - mathbbEYdo(T_1=1 T_2=0)  \n- mathbbEYdo(T_1=0 T_2=1) + mathbbEYdo(T_1=0 T_2=0) ","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Statistical Estimand (via backdoor adjustment):","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"IATE_0 rightarrow 1 0 rightarrow 1(P) = mathbbE_textbfWmathbbEYT_1=1 T_2=1 textbfW - mathbbEYT_1=1 T_2=0 textbfW  \n- mathbbEYT_1=0 T_2=1 textbfW + mathbbEYT_1=0 T_2=0 textbfW ","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"TMLE.jl Example","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A causal estimand is given by:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"causalΨ = IATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    )\n)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A corresponding statistical estimand can be identified via backdoor adjustment using the scm:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = identify(causalΨ, scm)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"or defined directly:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = IATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    ),\n    treatment_confounders=(T₁=[:W], T₂=[:W])\n)","category":"page"},{"location":"user_guide/estimands/#Any-function-of-the-previous-Estimands","page":"Estimands","title":"Any function of the previous Estimands","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"As a result of Julia's automatic differentiation facilities, given a set of already estimated estimands (Psi_1  Psi_k), we can automatically compute an estimator for f(Psi_1  Psi_k). This is done via the compose function:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"compose(f, args...)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"where args are asymptotically linear estimates (see Composing Estimands).","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = TMLE","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TMLE.jl is a Julia implementation of the Targeted Minimum Loss-Based Estimation (TMLE) framework. If you are interested in efficient and unbiased estimation of causal effects, you are in the right place. Since TMLE uses machine-learning methods to estimate nuisance estimands, the present package is based upon MLJ.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TMLE.jl can be installed via the Package Manager and supports Julia v1.6 and greater.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg> add TMLE","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To run an estimation procedure, we need 3 ingredients:","category":"page"},{"location":"","page":"Home","title":"Home","text":"A dataset: here a simulation dataset.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For illustration, assume we know the actual data generating process is as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nW  sim mathcalUniform(0 1) \nT  sim mathcalBernoulli(logistic(1-2 cdot W)) \nY  sim mathcalNormal(1 + 3 cdot T - T cdot W 001)\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"Because we know the data generating process, we can simulate some data accordingly:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TMLE\nusing Distributions\nusing StableRNGs\nusing Random\nusing CategoricalArrays\nusing MLJLinearModels\nusing LogExpFunctions\n\nrng = StableRNG(123)\nn = 100\nW = rand(rng, Uniform(), n)\nT = rand(rng, Uniform(), n) .< logistic.(1 .- 2W)\nY = 1 .+ 3T .- T.*W .+ rand(rng, Normal(0, 0.01), n)\ndataset = (Y=Y, T=categorical(T), W=W)\nnothing # hide","category":"page"},{"location":"","page":"Home","title":"Home","text":"A quantity of interest: here the Average Treatment Effect (ATE).","category":"page"},{"location":"","page":"Home","title":"Home","text":"The Average Treatment Effect of T on Y confounded by W is defined as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Ψ = ATE(\n    outcome=:Y, \n    treatment_values=(T=(case=true, control = false),), \n    treatment_confounders=(T=[:W],)\n)","category":"page"},{"location":"","page":"Home","title":"Home","text":"An estimator: here a Targeted Maximum Likelihood Estimator (TMLE).","category":"page"},{"location":"","page":"Home","title":"Home","text":"models = (Y=with_encoder(LinearRegressor()), T = LogisticClassifier())\ntmle = TMLEE(models=models)\nresult, _ = tmle(Ψ, dataset, verbosity=0);\nresult","category":"page"},{"location":"","page":"Home","title":"Home","text":"We are comforted to see that our estimator covers the ground truth! 🥳","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Test # hide\n@test pvalue(OneSampleTTest(result, 2.5)) > 0.05 # hide\nnothing # hide","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"EditURL = \"../../../examples/super_learning.jl\"","category":"page"},{"location":"examples/super_learning/#Becoming-a-Super-Learner","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"","category":"section"},{"location":"examples/super_learning/#What-this-tutorial-is-about","page":"Becoming a Super Learner","title":"What this tutorial is about","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Super Learning, also known as Stacking, is an ensemble technique that was first introduced by Wolpert in 1992. Instead of selecting a model based on cross-validation performance, models are combined by a meta-learner to minimize the cross-validation error. It has also been shown by van der Laan et al. that the resulting Super Learner will perform at least as well as its best performing submodel (at least asymptotically).","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Why is it important for Targeted Learning?","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"The short answer is that the consistency (convergence in probability) of the targeted estimator depends on the consistency of at least one of the nuisance estimands: Q_0 or G_0. By only using unrealistic models like linear models, we have little chance of satisfying the above criterion. Super Learning is a data driven way to leverage a diverse set of models and build the best performing estimator for both Q_0 or G_0.","category":"page"},{"location":"examples/super_learning/#The-dataset","page":"Becoming a Super Learner","title":"The dataset","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Let's consider the case where Y is categorical. In TMLE.jl, this could be useful to learn:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"The propensity score\nThe outcome model when the outcome is binary","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"We will use the following moons dataset:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"using MLJ\n\nX, y = MLJ.make_moons(1000)\nnothing # hide","category":"page"},{"location":"examples/super_learning/#Defining-a-Super-Learner-in-MLJ","page":"Becoming a Super Learner","title":"Defining a Super Learner in MLJ","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"In MLJ, a Super Learner can be defined using the Stack function. The three most important type of arguments for a Stack are:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"metalearner: The metalearner to be used to combine the weak learner to be defined. Typically a generalized linear model.\nresampling: The cross-validation scheme, by default, a 6-fold cross-validation. Since we are working with categorical","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"data it is a good idea to make sure the splits are balanced. We will thus use a StratifiedCV resampling strategy.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"models...: A series of named MLJ models.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"One important point is that MLJ does not provide any model by itself, juat the API, models have to be loaded from external compatible libraries. You can search for available models that match your data.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"models(matching(X, y))","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"note: Stack limitation\nThe Stack cannot contain <:Deterministic models for classification.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Let's load a few packages providing models and build our first Stack:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"using MLJXGBoostInterface\nusing MLJLinearModels\nusing NearestNeighborModels\n\nresampling = StratifiedCV()\nmetalearner = LogisticClassifier()\n\nstack = Stack(\n    metalearner = metalearner,\n    resampling  = resampling,\n    lr          = LogisticClassifier(),\n    knn         = KNNClassifier(K=3)\n)","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"This Stack only contains 2 different models: a logistic classifier and a KNN classifier. A Stack is just like any MLJ model, it can be wrapped in a machine and fitted:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"mach = machine(stack, X, y)\nfit!(mach, verbosity=0)","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Or evaluated. Because the Stack contains a cross-validation procedure, this will result in two nested levels of resampling.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"evaluate!(mach, measure=log_loss, resampling=resampling)","category":"page"},{"location":"examples/super_learning/#A-more-advanced-Stack","page":"Becoming a Super Learner","title":"A more advanced Stack","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"What are good Stack members? Virtually anything, provided they are MLJ models. Here are a few examples:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"You can use the stack to \"select\" model hyper-parameters. e.g. KNNClassifier(K=3) or KNNClassifier(K=2)?\nYou can also use self-tuning models. Note that because","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"these models resort to cross-validation, fitting the stack will result in two nested levels of sample-splitting.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"The following self-tuned XGBoost will vary some hyperparameters in an internal sample-splitting procedure in order to optimize the Log-Loss. It will then be combined with the rest of the models in the Stack's own sample-splitting procedure. Finally, evaluation is performed in an outer sample-split.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"xgboost = XGBoostClassifier(tree_method=\"hist\")\nself_tuning_xgboost = TunedModel(\n    model = xgboost,\n    resampling = resampling,\n    tuning = Grid(goal=20),\n    range = [\n        range(xgboost, :max_depth, lower=3, upper=7),\n        range(xgboost, :lambda, lower=1e-5, upper=10, scale=:log)\n        ],\n    measure = log_loss,\n    cache=false\n)\n\nstack = Stack(\n    metalearner         = metalearner,\n    resampling          = resampling,\n    self_tuning_xgboost = self_tuning_xgboost,\n    lr                  = LogisticClassifier(),\n    knn_2               = KNNClassifier(K=2),\n    knn_3               = KNNClassifier(K=3),\n    cache               = false\n)\n\nmach = machine(stack, X, y, cache=false)\nevaluate!(mach, measure=log_loss, resampling=resampling)","category":"page"},{"location":"examples/super_learning/#Diagnostic","page":"Becoming a Super Learner","title":"Diagnostic","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Optionally, one can also investigate how sucessful the weak learners were in the Stack's internal cross-validation. This is done by specifying the measures keyword argument.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Here we look at both the Log-Loss and the AUC.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"stack.measures = [log_loss, auc]\nfit!(mach, verbosity=0)\nreport(mach).cv_report","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"One can look at the fitted parameters for the metalearner as well:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"fitted_params(mach).metalearner","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"This page was generated using Literate.jl.","category":"page"}]
}
