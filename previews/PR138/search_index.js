var documenterSearchIndex = {"docs":
[{"location":"examples/interactions_correlated/#Interaction-Estimation","page":"Interaction Estimation","title":"Interaction Estimation","text":"","category":"section"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"In this example we aim to estimate the average interaction effect of two, potentially correlated, treatment variables T1 and T2 on an outcome Y.","category":"page"},{"location":"examples/interactions_correlated/#Data-Generating-Process","page":"Interaction Estimation","title":"Data Generating Process","text":"","category":"section"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Let's consider the following structural causal model where the shaded nodes represent the observed variables.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"(Image: interaction-graph)","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"In other words, only one confounding variable is observed (W1). This would be a major problem if we wanted to estimate the average treatment effect of T1 or T2 on Y separately. However, here, we are interested in interactions and thus W1 is a sufficient adjustment set. This artificial situation is ubiquitous in genetics, where two main sources of confounding exist. Ancestry, can be estimated (here W1) and linkage disequilibrium is usually more challenging to address (here W2).","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Let us first define some helper functions and import some libraries.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"using Distributions\nusing Random\nusing DataFrames\nusing Statistics\nusing CategoricalArrays\nusing TMLE\nusing CairoMakie\nusing MLJXGBoostInterface\nusing MLJBase\nusing MLJLinearModels\nusing MLJTuning\nusing StatisticalMeasures\n\nfunction estimate_across_correlation_levels(estimators, σs; n=1000)\n    results = Dict(key => [] for key in keys(estimators))\n    for σ in σs\n        dataset = generate_dataset(n=n, σ=σ)\n        for (estimator_key, estimator) in estimators\n            result, _ = estimator(Ψ, dataset; verbosity=0)\n            push!(results[estimator_key], result)\n        end\n    end\n    return results\nend\n\nfunction estimate_across_sample_sizes_and_correlation_levels(estimators, ns, σs)\n    results = []\n    for n in ns\n        results_at_n = estimate_across_correlation_levels(estimators, σs; n=n)\n        push!(results, results_at_n)\n    end\n    return results\nend\n\nfunction plot_across_sample_sizes_and_correlation_levels(results, ns, σs; estimator=\"TMLE_SL\", title=\"Estimation via TMLE (GLMs)\")\n    fig = Figure(size=(800, 800))\n    for (index, n) in enumerate(ns)\n        results_at_n = results[index][estimator]\n        Ψ̂s = TMLE.estimate.(results_at_n)\n        errors = last.(confint.(significance_test.(results_at_n))) .- Ψ̂s\n        ax = if n == last(ns)\n            Axis(fig[index, 1], xlabel=\"σ\", ylabel=\"AIE\\n(n=$n)\")\n        else\n            Axis(fig[index, 1], ylabel=\"AIE\\n(n=$n)\", xticklabelsvisible=false)\n        end\n        errorbars!(ax, σs, Ψ̂s, errors, color = :blue, whiskerwidth = 10)\n        scatter!(ax, σs, Ψ̂s, color=:red, markersize=10)\n        hlines!(ax, [-1.5], color=:green, linestyle=:dash)\n    end\n    Label(fig[0, :], title; tellwidth=false, fontsize=24)\n    return fig\nend\n\nRandom.seed!(123)\n\nμT(w) = [sum(w), sum(w)]\n\nμY(t, w) = 1 + 10t[1] - 3t[2] * t[1] * w","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"We assume that W1 and W2, the confounding variables, follow a uniform distribution.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"generate_W(n) = rand(Uniform(0, 1), 2, n)","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"T1 and T2 are generated via a copula method through a multivariate normal to induce some statistical dependence (via σ).","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"function generate_T(W, n; σ=0.5, threshold=0)\n    covariance = [\n        1. σ\n        σ 1.\n    ]\n    T = zeros(Bool, 2, n)\n    for i in 1:n\n        dTi = MultivariateNormal(μT(W[:, i]), covariance)\n        T[:, i] = rand(dTi) .> threshold\n    end\n    return T\nend","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Finally, Y is generated through a simple linear model with an interaction term.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"function generate_Y(T, W1, n; σY=1)\n    Y = zeros(Float64, n)\n    for i in 1:n\n        dY = Normal(μY(T[:, i], W1[i]), σY)\n        Y[i] = rand(dY)\n    end\n    return Y\nend","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Importantly, the average interaction effect between T1 and T2 is thus -3 mathbbEW = -15.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"We will generate a full dataset with the following function.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"function generate_dataset(;n=1000, σ=0.5, threshold=0., σY=1)\n\n    W = generate_W(n)\n    T = generate_T(W, n; σ=σ, threshold=threshold)\n\n    W = permutedims(W)\n    W1 = W[:, 1]\n    W2 = W[:, 2]\n\n    Y = generate_Y(T, W1, n; σY=σY)\n\n    T = permutedims(T)\n    T1 = categorical(T[:, 1])\n    T2 = categorical(T[:, 2])\n\n    return DataFrame(W1=W1, W2=W2, T1=T1, T2=T2, Y=Y)\nend\n\ndataset = generate_dataset()\n\nfirst(dataset, 5)","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Let's verify that each treatment level is sufficiently present in the dataset (≈positivity).","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"combine(groupby(dataset, [:T1, :T2]), proprow => :JOINT_TREATMENT_FREQ)","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"And that T1 and T2 are indeed correlated.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"treatment_correlation(dataset) = cor(unwrap.(dataset.T1), unwrap.(dataset.T2))\n@assert treatment_correlation(dataset) > 0.2 #hide\ntreatment_correlation(dataset)","category":"page"},{"location":"examples/interactions_correlated/#Estimation","page":"Interaction Estimation","title":"Estimation","text":"","category":"section"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"We can now proceed to estimation, for instance using TMLE with linear models.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"First, let's define the effect of interest. Interactions are defined via the AIE function (note that we only set W1 as a confounder).","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Ψ = AIE(\n    outcome=:Y,\n    treatment_values= (\n        T1=(case=1, control=0),\n        T2=(case=1, control=0)\n    ),\n    treatment_confounders = [:W1]\n)\nlinear_models = default_models(G=LogisticClassifier(lambda=0), Q_continuous=LinearRegressor())\nestimator = Tmle(models=linear_models, weighted=true)\nresult, _ = estimator(Ψ, dataset; verbosity=0)\n@assert pvalue(significance_test(result, -1.5)) > 0.05 #hide\nsignificance_test(result)","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"The true effect size is thus covered by our confidence interval.","category":"page"},{"location":"examples/interactions_correlated/#Varying-levels-of-correlation","page":"Interaction Estimation","title":"Varying levels of correlation","text":"","category":"section"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"We will now vary the correlation level between T1 and T2 to observe how it affects the estimation results across samples sizes. We will also look at three different modelling strategies:","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Generalized linear models (GLMs)\nXGBoost\nSuper Learning (SL) via a model selection approach","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"First, let's see how the parameter σ affects the correlation between T1 and T2.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"function plot_correlations(;σs = 0.1:0.1:1, n=1000, threshold=0., σY=1.)\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel=\"σ\", ylabel=\"Correlation(T1, T2)\")\n    correlations = map(σs) do σ\n        dataset = generate_dataset(;n=n, σ=σ, threshold=threshold, σY=σY)\n        return treatment_correlation(dataset)\n    end\n    scatter!(ax, σs, correlations, color=:blue)\n    return fig\nend\n\nσs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.999]\nplot_correlations(;σs=σs, n=10_000)","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"As expected, the correlation between T1 and T2 increases with σ. Let's see how this affects estimation.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"We first define our Super Learners, they compare L2 penalized GLM and XGboost models for various penalization parameters λ on a holdout set and select the best model.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"lambdas = 10 .^ range(1, stop=-4, length=5)\nlinear_regressors = [RidgeRegressor(lambda=λ) for λ in lambdas]\nlogistic_classifiers = [LogisticClassifier(lambda=λ) for λ in lambdas]\nxgboost_classifiers = [XGBoostClassifier(tree_method=\"hist\", lambda=λ, nthread=1) for λ in lambdas]\nxgboost_regressors = [XGBoostRegressor(tree_method=\"hist\", lambda=λ, nthread=1) for λ in lambdas]\n\nsl_regressor = TunedModel(\n    models=vcat(linear_regressors, xgboost_regressors),\n    resampling=Holdout(),\n    measure=rmse,\n    check_measure=false\n)\n\nsl_classifier = TunedModel(\n    models=vcat(logistic_classifiers, xgboost_classifiers),\n    resampling=Holdout(),\n    measure=log_loss,\n    check_measure=false\n)","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Now define the sample sizes and correlation levels we want to explore.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"ns = [1000, 10_000, 100_000, 500_000]\nσs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 0.999]","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"and estimate (this will take a little while).","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"estimators = Dict(\n    \"TMLE_GLM\"  => Tmle(models=linear_models, weighted=true),\n    \"TMLE_XGBOOST\" => Tmle(\n        models=default_models(G=XGBoostClassifier(tree_method=\"hist\", nthread=1), Q_continuous=XGBoostRegressor(tree_method=\"hist\", nthread=1)),\n        weighted=true,\n    ),\n    \"TMLE_SL\" => Tmle(\n        models=default_models(G=sl_classifier, Q_continuous=sl_regressor),\n        weighted=true,\n    )\n)\n\nresults = estimate_across_sample_sizes_and_correlation_levels(estimators, ns, σs)","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"Let us first focus on results obtained with the GLM estimator. In small sample sizes, coverage is almost perfect across all correlation levels. However, as sample size increases, the confidence intervals shrink and start to miss the ground truth. The phenomenon is more pronounced for larger correlations. This could be due to model misspecification bias which can be verified by using a more flexible modelling strategy, here we use XGBoost.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"plot_across_sample_sizes_and_correlation_levels(results, ns, σs; estimator=\"TMLE_GLM\", title=\"Estimation via TMLE (GLMs)\")","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"As expected, XGBoost improves estimation performance in the asymptotic regime, however, the performance is the small sample size regime is deteriorated, likely due to over-fitting. To find the sweet spot between GLM and XGBoost, we can resort to model selection to adaptively select the best model (sometimes this is called discrete super learning).","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"plot_across_sample_sizes_and_correlation_levels(results, ns, σs; estimator=\"TMLE_XGBOOST\", title=\"Estimation via TMLE (XGBoost)\")","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"As we can see, the performance is now good across all sample sizes. Furthermore, the correlation between T1 and T2 seems harmless except when σ > 0.9. The confidence interval is then quite large which will result in a loss of power.","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"plot_across_sample_sizes_and_correlation_levels(results, ns, σs; estimator=\"TMLE_SL\", title=\"Estimation via TMLE (SL)\")","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"","category":"page"},{"location":"examples/interactions_correlated/","page":"Interaction Estimation","title":"Interaction Estimation","text":"This page was generated using Literate.jl.","category":"page"},{"location":"user_guide/scm/#Structural-Causal-Models","page":"Structural Causal Models","title":"Structural Causal Models","text":"","category":"section"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"Even if you don't have to, it can be useful to define a Structural Causal Model (SCM) for your problem. A SCM is a directed acyclic graph that describes the causal relationships between the random variables under study.","category":"page"},{"location":"user_guide/scm/#Incremental-Construction","page":"Structural Causal Models","title":"Incremental Construction","text":"","category":"section"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"All models are wrong? Well maybe not the following:","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"using TMLE # hide\nscm = SCM()","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"This model does not say anything about the random variables and is thus not really useful. Let's assume that we are interested in an outcome Y and that this outcome is determined by 8 other random variables. We can add this assumption to the model","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"add_equation!(scm, :Y => [:T₁, :T₂, :W₁₁, :W₁₂, :W₂₁, :W₂₂, :W, :C])","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"Let's now assume that we have a more complete knowledge of the problem and we also know how T₁ and T₂ depend on the rest of the variables in the system.","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"add_equations!(scm, :T₁ => [:W₁₁, :W₁₂, :W], :T₂ => [:W₂₁, :W₂₂, :W])","category":"page"},{"location":"user_guide/scm/#One-Step-Construction","page":"Structural Causal Models","title":"One Step Construction","text":"","category":"section"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"Instead of constructing the SCM incrementally, one can provide all the specified equations at once:","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"scm = SCM([\n    :Y  => [:T₁, :T₂, :W₁₁, :W₁₂, :W₂₁, :W₂₂, :W, :C],\n    :T₁ => [:W₁₁, :W₁₂, :W],\n    :T₂ => [:W₂₁, :W₂₂, :W]\n])","category":"page"},{"location":"user_guide/scm/#Classic-Structural-Causal-Models","page":"Structural Causal Models","title":"Classic Structural Causal Models","text":"","category":"section"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"There are many cases where we are interested in estimating the causal effect of a some treatment variables on a some outcome variables. If all treatment variables share the same set of confounders, we can quickly define the associated SCM with the StaticSCM interface:","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"scm = StaticSCM(\n    outcomes=[:Y₁, :Y₂], \n    treatments=[:T₁, :T₂], \n    confounders=[:W₁, :W₂];\n)","category":"page"},{"location":"user_guide/scm/","page":"Structural Causal Models","title":"Structural Causal Models","text":"where outcome_extra_covariates is a set of extra variables that are causal of the outcomes but are not of direct interest in the study.","category":"page"},{"location":"contributing/#Contributing","page":"Contributing & Reporting","title":"Contributing","text":"","category":"section"},{"location":"contributing/#Reporting","page":"Contributing & Reporting","title":"Reporting","text":"","category":"section"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"If you have a question that is not answered by the documentation, would like a new feature or find a bug, please open an issue on Github. Be specific, in the latter case, a minimal example reproducing the bug is ideal.","category":"page"},{"location":"contributing/#Contributing-2","page":"Contributing & Reporting","title":"Contributing","text":"","category":"section"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"The package is not fully mature and some of the API is subject to change. If you would like to contribute a new estimand, estimator, extension, or example for the documentation, feel free to get in touch. Breaking changes are welcome if they extend the capabilities of the package.","category":"page"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"Here is some general information about the code base.","category":"page"},{"location":"contributing/#New-Estimands","page":"Contributing & Reporting","title":"New Estimands","text":"","category":"section"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"Implementing a new estimand requires both the definition of the estimand (see src/counterfactual_mean_based/estimands.jl for examples) and the associated estimators.","category":"page"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"The entry point estimators are defined within src/counterfactual_mean_based/estimators.jl. This is because there has been a focus on implementationsestimands based on the counterfactual mean EY(T). However the estimation procedures are still quite general and could serve as a backbone for future estimands. For instance:","category":"page"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"get_relevant_factors: returns the nuisance parameters for a given parameter Ψ.\ngradient_and_estimate: computes the gradient for a given parameter Ψ.","category":"page"},{"location":"contributing/#New-Estimators","page":"Contributing & Reporting","title":"New Estimators","text":"","category":"section"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"There are several interesting directions to complement this package with new estimators.","category":"page"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"C-TMLE: collaborative TMLE is a powerful estimation strategy for which a rough template is in place (src/counterfactual_mean_based/collaborative_template.jl). Many strategies following the template can then be implemented. Get in touch if needed for general directions.\nRiesz Representer Estimation: Estimation of the nuisance functions is typically made by estimation of the propensity score. In cases of positivity violations this can lead to numerical instability. It turns out that the inverse of the propensity score can be estimated directly (see here).","category":"page"},{"location":"contributing/#General-Code-Pattern","page":"Contributing & Reporting","title":"General Code Pattern","text":"","category":"section"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"The package is centered around the following statistical concepts which are embodied by Julia structs:","category":"page"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"Estimand: A quantity of interest, one-dimensional like the Average Treatment Effect (ATE) or infinite dimensional like a conditional distribution (ConditionalDistribution).\nEstimator: A method that uses data to obtain an estimate of the estimand. Tmle and Ose are valid estimators for the ATE and the MLConditionalDistributionEstimator is a valid estimator for a ConditionalDistribution.\nEstimate: Calling an estimator on an estimand with a dataset yields an estimate. For example a TMLEstimate is obtained by using a Tmle for the ATE. A MLConditionalDistribution is obtained from MLConditionalDistributionEstimator for a ConditionalDistribution.","category":"page"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"The general pattern is thus:","category":"page"},{"location":"contributing/","page":"Contributing & Reporting","title":"Contributing & Reporting","text":"estimand = Estimand(kwargs...)\nestimator = Estimator(kwargs...)\nestimate = estimator(estimand, dataset; kwargs...)","category":"page"},{"location":"walk_through/#Walk-Through","page":"Walk Through","title":"Walk Through","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The goal of this section is to provide a comprehensive (but non-exhaustive) illustration of the estimation process provided in TMLE.jl. For an in-depth explanation, please refer to the User Guide.","category":"page"},{"location":"walk_through/#The-Dataset","page":"Walk Through","title":"The Dataset","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"TMLE.jl is compatible with any dataset wrapped in a DataFrame, note that it is possible to wrap an Arrow Table for instance, in a Dataframe object. In this section, we will be working with the same dataset all along.","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"⚠️ One thing to note is that treatment variables as well as binary outcomes must be encoded as categorical variables in the dataset (see MLJ Working with categorical data).","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The dataset is generated as follows:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"using TMLE\nusing Random\nusing Distributions\nusing DataFrames\nusing StableRNGs\nusing CategoricalArrays\nusing TMLE\nusing LogExpFunctions\nusing MLJLinearModels\n\nfunction make_dataset(;n=1000)\n    rng = StableRNG(123)\n    # Confounders\n    W₁₁= rand(rng, Uniform(), n)\n    W₁₂ = rand(rng, Uniform(), n)\n    W₂₁= rand(rng, Uniform(), n)\n    W₂₂ = rand(rng, Uniform(), n)\n    # Covariates\n    C = rand(rng, Uniform(), n)\n    # Treatment | Confounders\n    T₁ = rand(rng, Uniform(), n) .< logistic.(0.5sin.(W₁₁) .- 1.5W₁₂)\n    T₂ = rand(rng, Uniform(), n) .< logistic.(-3W₂₁ - 1.5W₂₂)\n    # Target | Confounders, Covariates, Treatments\n    Y = 1 .+ 2W₂₁ .+ 3W₂₂ .+ W₁₁ .- 4C.*T₁ .- 2T₂.*T₁.*W₁₂ .+ rand(rng, Normal(0, 0.1), n)\n    return DataFrame(\n        W₁₁ = W₁₁, \n        W₁₂ = W₁₂,\n        W₂₁ = W₂₁,\n        W₂₂ = W₂₂,\n        C   = C,\n        T₁  = categorical(T₁),\n        T₂  = categorical(T₂),\n        Y   = Y\n        )\nend\ndataset = make_dataset()\nnothing # hide","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Even though the role of a variable (treatment, outcome, confounder, ...) is relative to the problem setting, this dataset can intuitively be decomposed into:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"1 Outcome variable (Y).\n2 Treatment variables (T₁ T₂) with confounders (W₁₁ W₁₂) and (W₂₁ W₂₂) respectively.\n1 Outcome extra covariate variable (C).","category":"page"},{"location":"walk_through/#The-Structural-Causal-Model","page":"Walk Through","title":"The Structural Causal Model","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The modeling stage starts from the definition of a Structural Causal Model (SCM). This is simply a list of relationships between the random variables in our dataset. See Structural Causal Models for an in-depth explanation. For our purposes, because we know the data generating process, we can define it as follows:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"scm = SCM([\n    :Y  => [:T₁, :T₂, :W₁₁, :W₁₂, :W₂₁, :W₂₂, :C],\n    :T₁ => [:W₁₁, :W₁₂],\n    :T₂ => [:W₂₁, :W₂₂]\n]\n)","category":"page"},{"location":"walk_through/#The-Causal-Estimands","page":"Walk Through","title":"The Causal Estimands","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"From the previous causal model we can ask multiple causal questions, all represented by distinct causal estimands. The set of available estimands types can be listed as follow:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"AVAILABLE_ESTIMANDS","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"At the moment there are 3 main causal estimands in TMLE.jl, we provide below a few examples.","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The Counterfactual Mean:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"cm = CM(\n    outcome = :Y,\n    treatment_values = (T₁=true,) \n)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The Average Treatment Effect:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"total_ate = ATE(\n    outcome = :Y,\n    treatment_values = (\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    ) \n)\nmarginal_ate_t1 = ATE(\n    outcome = :Y,\n    treatment_values = (T₁=(case=1, control=0),) \n)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"The Average Interaction Effect:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"aie = AIE(\n    outcome = :Y,\n    treatment_values = (\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    ) \n)","category":"page"},{"location":"walk_through/#Identification","page":"Walk Through","title":"Identification","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Identification is the process by which a Causal Estimand is turned into a Statistical Estimand, that is, a quantity we may estimate from data. This is done via the identify function which also takes in the SCM:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"statistical_aie = identify(aie, scm)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Alternatively, you can also directly define the statistical parameters (see Estimands).","category":"page"},{"location":"walk_through/#Estimation","page":"Walk Through","title":"Estimation","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Then each parameter can be estimated by building an estimator (which is simply a function) and evaluating it on data. For illustration, we will keep the models simple. We define a Targeted Maximum Likelihood Estimator:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"tmle = Tmle()","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Because we haven't identified the cm causal estimand yet, we need to provide the scm as well to the estimator:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"result, cache = tmle(cm, scm, dataset);\nresult","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Statistical Estimands can be estimated without a SCM, let's use the One-Step estimator:","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"ose = Ose()\nresult, cache = ose(statistical_aie, dataset)\nresult","category":"page"},{"location":"walk_through/#Hypothesis-Testing","page":"Walk Through","title":"Hypothesis Testing","text":"","category":"section"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"Both TMLE and OSE asymptotically follow a Normal distribution. It means we can perform standard T/Z tests of null hypothesis. TMLE.jl extends the method provided by the HypothesisTests.jl package that can be used as follows.","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"OneSampleTTest(result)","category":"page"},{"location":"walk_through/","page":"Walk Through","title":"Walk Through","text":"If the estimate is high-dimensional, a OneSampleHotellingT2Test should be performed instead. Alternatively, the significance_test function will automatically select the appropriate test for the estimate and return its result.","category":"page"},{"location":"user_guide/resampling/#Resampling-Strategies","page":"Resampling Strategies","title":"Resampling Strategies","text":"","category":"section"},{"location":"user_guide/resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"In causal inference, the treatment variables play a crucial role, it is thus usually a good idea to stratify cross-validation schemes by these variables. This is the purpose of the following resampling strategy which is an instance of a MLJBase.ResamplingStrategy. It applies a stratified cross-validation strategy based on both treatments and outcome (if it is Finite) variables.","category":"page"},{"location":"user_guide/resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"resampling = CausalStratifiedCV(resampling=StratifiedCV(nfolds=3))","category":"page"},{"location":"user_guide/resampling/","page":"Resampling Strategies","title":"Resampling Strategies","text":"For ease of use the treatment variables are detected automatically from the parameter of interest Ψ.","category":"page"},{"location":"examples/double_robustness/#Model-Misspecification-and-Double-Robustness","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"","category":"section"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"In this example we illustrate the double robustness property of TMLE in the classical backdoor adjustment setting for the Average Treatment Effect.","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"Let's consider the following simple data generating process:","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"beginaligned\nW sim mathcalN(0 1) \nT sim mathcalB(frac11 + e^-(03 - 05 cdot W)) \nY sim mathcalN(e^1 - 10 cdot T + W 1)\nendaligned","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"using TMLE\nusing MLJBase\nusing Distributions\nusing StableRNGs\nusing LogExpFunctions\nusing MLJGLMInterface\nusing DataFrames\nusing CairoMakie\n\n\nμY(T, W) = exp.(1 .- 10T .+ 1W)\n\nfunction generate_data(;n = 1000, rng = StableRNG(123))\n    W  = rand(rng, Normal(), n)\n    μT = logistic.(0.3 .- 0.5W)\n    T  = float(rand(rng, n) .< μT)\n    ϵ = rand(rng, Normal(), n)\n    Y  = μY(T, W) .+ ϵ\n    Y₁ = μY(ones(n), W) .+ ϵ\n    Y₀ = μY(zeros(n), W) .+ ϵ\n    return DataFrame(\n        W = W,\n        T = T,\n        Tcat = categorical(T),\n        Y = Y,\n        Y₁ = Y₁,\n        Y₀ = Y₀\n    )\nend\n\nfunction plotY(data)\n    fig = Figure()\n    ax = Axis(fig[1, 1], xlabel=\"W\", ylabel=\"Y\")\n    for (key, group) in pairs(groupby(data, :T))\n        scatter!(ax, group.Y, group.W, label=string(\"T=\",key.T))\n    end\n    axislegend()\n    return fig\nend\n\ndata = generate_data(;n = 1000, rng = StableRNG(123))\nplotY(data)","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"Y is thus a non linear function of T and W. Despite the simplicity of the example, it is difficult to find a closed form solution for the true Average Causal Effect. However, since we know the generating process, we can approximate it using a Monte-Carlo approximation. In the next two sections, we compare Linear inference and TMLE and see how well they cover this Monte-Carlo approximation.","category":"page"},{"location":"examples/double_robustness/#Estimation-using-a-Linear-model","page":"Model Misspecification & Double Robustness","title":"Estimation using a Linear model","text":"","category":"section"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"We first propose to estimate the effect size using the classic linear inference method. Because our model does not contain the data generating process (and is hence mis-specified), there is no guarantee that the true effect size will be covered by our confidence interval. In fact, as the sample size grows, the confidence interval will inevitably shrink and fail to cover the ground truth. This can be seen from the following animation:","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"function linear_inference(data)\n    mach = machine(LinearRegressor(), data[!, [:W, :T]], data.Y)\n    fit!(mach, verbosity=0)\n    coeftable = report(mach).coef_table\n    Trow = findfirst(x -> x == \"T\", coeftable.rownms)\n    coef = coeftable.cols[1][Trow]\n    lb = coeftable.cols[end - 1][Trow]\n    ub = coeftable.cols[end][Trow]\n    return (coef, lb, ub)\nend\n\nfunction repeat_inference(inference_method; n=1000, K=100)\n    estimates = Vector{Float64}(undef, K)\n    errors = Vector{Float64}(undef, K)\n    mcestimates = Vector{Float64}(undef, K)\n    for k in 1:K\n        data = generate_data(;n=n, rng=StableRNG(k))\n        est, lb, ub = inference_method(data)\n        estimates[k] = est\n        errors[k] = ub - est\n        mcestimates[k] = mean(data.Y₁ .- data.Y₀)\n    end\n    return estimates, errors, mcestimates\nend\n\nfunction plot_coverage(inference_method; n=1000, K=100)\n    fig = Figure()\n    title = Observable(string(\"N=\", n))\n    ax = Axis(fig[1, 1], xlabel=\"Repetition\", ylabel=\"Estimate size\", title=title)\n    ks = 1:K\n    estimates, errors, mcestimates = repeat_inference(inference_method; n=n, K=K)\n    estimates = Observable(estimates)\n    errors = Observable(errors)\n    mcestimates = Observable(mcestimates)\n    errorbars!(ax, ks, estimates, errors, color=:red, whiskerwidth = 10)\n    scatter!(ax, ks, estimates, color=:red, label=replace(string(inference_method), \"_\" => \" \"))\n    scatter!(ax, ks, mcestimates, label=\"Monte Carlo estimate\")\n    axislegend()\n    return fig, estimates, errors, mcestimates, title\nend\n\nfunction update_observables!(estimates, errors, mcestimates, title, inference_method; n=1000, K=100)\n    newestimates, newerrors, newmcestimates = repeat_inference(inference_method; n=n, K=K)\n    estimates[] = newestimates\n    errors[] = newerrors\n    mcestimates[] = newmcestimates\n    title[] = string(\"N=\", n)\nend\n\nfunction make_animation(inference_method)\n    Ns = [10_000, 25_000, 50_000, 75_000, 100_000, 250_000, 500_000]\n    fig, estimates, errors, mcestimates, title = plot_coverage(inference_method; n=10_000, K=100)\n    record(fig, \"$(inference_method).gif\", Ns; framerate = 1) do n\n        update_observables!(estimates, errors, mcestimates, title, inference_method; n=n, K=100)\n    end\nend\n\nmake_animation(linear_inference)","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"(Image: Linear Inference)","category":"page"},{"location":"examples/double_robustness/#Estimation-using-TMLE","page":"Model Misspecification & Double Robustness","title":"Estimation using TMLE","text":"","category":"section"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"To solve this issue, we will now use TMLE to estimate the Average Treatment Effect. We will keep the mis-specified linear model to estimate E[Y|T,W] but will estimate p(T|W) with a logistic regression which turns out to be the true generating model in this case. Because TMLE is double robust we see that we now have full coverage of the ground truth.","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"function tmle_inference(data)\n    Ψ = ATE(\n        outcome=:Y,\n        treatment_values=(Tcat=(case=1.0, control=0.0),),\n        treatment_confounders=(Tcat=[:W],)\n    )\n    models = Dict(\n        :Y    => with_encoder(LinearRegressor()),\n        :Tcat => with_encoder(LinearBinaryClassifier())\n    )\n    tmle = Tmle(models=models)\n    result, _ = tmle(Ψ, data; verbosity=0)\n    lb, ub = confint(OneSampleTTest(result))\n    return (TMLE.estimate(result), lb, ub)\nend\n\nmake_animation(tmle_inference)","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"(Image: TMLE Inference)","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"","category":"page"},{"location":"examples/double_robustness/","page":"Model Misspecification & Double Robustness","title":"Model Misspecification & Double Robustness","text":"This page was generated using Literate.jl.","category":"page"},{"location":"user_guide/acceleration/#Accelerations","page":"Accelerations","title":"Accelerations","text":"","category":"section"},{"location":"user_guide/acceleration/","page":"Accelerations","title":"Accelerations","text":"Targeted estimators are expensive by nature, in particular when resorting to cross-validation and collaborative TMLE. However many of the operations can be conducted in parallel. To facilitate this, an acceleration parameter can be provided to the estimation call.","category":"page"},{"location":"user_guide/acceleration/","page":"Accelerations","title":"Accelerations","text":"At the moment, only single CPU and multi-threading modes are supported. For a multi-threaded call:","category":"page"},{"location":"user_guide/acceleration/","page":"Accelerations","title":"Accelerations","text":"tmle = Tmle(resampling=CausalStratifiedCV())\ntmle(Ψ, dataset; acceleration=CPUThreads())","category":"page"},{"location":"user_guide/acceleration/","page":"Accelerations","title":"Accelerations","text":"In this case, fitting across multiple folds will happen on all available threads. Similarly the outcome mean and propensity score model will be estimated in parallel.","category":"page"},{"location":"user_guide/acceleration/","page":"Accelerations","title":"Accelerations","text":"note: Note\nAs noted here, non native Julia MLJ models may not be suitable for multi-threading.","category":"page"},{"location":"user_guide/estimation/#Estimators","page":"Estimators","title":"Estimators","text":"","category":"section"},{"location":"user_guide/estimation/#Constructing-and-Using-Estimators","page":"Estimators","title":"Constructing and Using Estimators","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Once a statistical estimand has been defined, we can proceed with estimation. There are two semi-parametric efficient estimators in TMLE.jl:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"The Targeted Maximum-Likelihood Estimator (Tmle)\nThe One-Step Estimator (Ose)","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"While they have similar asymptotic properties, their finite sample performance may be different. They also have a very distinguishing feature, the TMLE is a plugin estimator, which means it respects the natural bounds of the estimand of interest. In contrast, the OSE may in theory report values outside these bounds. In practice, this is not often the case and the estimand of interest may not impose any restriction on its domain.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Drawing from the example dataset and SCM from the Walk Through section, we can estimate the ATE for T₁. Let's use TMLE:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Ψ₁ = ATE(\n    outcome=:Y, \n    treatment_values=(T₁=(case=true, control=false),),\n    treatment_confounders=(T₁=[:W₁₁, :W₁₂],),\n    outcome_extra_covariates=[:C]\n)\ntmle = Tmle()\nresult₁, cache = tmle(Ψ₁, dataset);\nresult₁\nnothing # hide","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"The cache (see below) contains estimates for the nuisance functions that were necessary to estimate the ATE.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"The result₁ structure corresponds to the estimation result and will display the result of a T-Test including:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"A point estimate.\nA 95% confidence interval.\nA p-value (Corresponding to the test that the estimand is different than 0).","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Both the TMLE and OSE are asymptotically linear estimators, standard Z/T tests from HypothesisTests.jl can be performed and confint and pvalue methods used.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"tmle_test_result₁ = pvalue(OneSampleTTest(result₁))","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Let us now turn to the Average Treatment Effect of T₂, we will estimate it with a Ose:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Ψ₂ = ATE(\n    outcome=:Y, \n    treatment_values=(T₂=(case=true, control=false),),\n    treatment_confounders=(T₂=[:W₂₁, :W₂₂],),\n    outcome_extra_covariates=[:C]\n)\nose = Ose()\nresult₂, cache = ose(Ψ₂, dataset;cache=cache);\nresult₂\nnothing # hide","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Again, required nuisance functions are fitted and stored in the cache.","category":"page"},{"location":"user_guide/estimation/#Specifying-Models","page":"Estimators","title":"Specifying Models","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"By default, TMLE.jl uses generalized linear models for the estimation of relevant and nuisance factors such as the outcome mean and the propensity score. However, this is not the recommended usage since the estimators' performance is closely related to how well we can estimate these factors. More sophisticated models can be provided using the models keyword argument of each estimator which is a Dict{Symbol, Model} mapping variables' names to their respective model.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Rather than specifying a specific model for each variable it may be easier to override the default models using the default_models function:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"For example one can override all default models with XGBoost models from MLJXGBoostInterface:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"using MLJXGBoostInterface\nxgboost_regressor = XGBoostRegressor()\nxgboost_classifier = XGBoostClassifier()\nmodels = default_models(\n    Q_binary     = xgboost_classifier,\n    Q_continuous = xgboost_regressor,\n    G            = xgboost_classifier\n)\ntmle_gboost = Tmle(models=models)","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"The advantage of using default_models is that it will automatically prepend each model with a ContinuousEncoder to make sure the correct types are passed to the downstream models.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Super Learning (Stack) as well as variable specific models can be defined as well. Here is a more customized version:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"lr = LogisticClassifier(lambda=0.)\nstack_binary = Stack(\n    metalearner=lr,\n    xgboost=xgboost_classifier,\n    lr=lr\n)\n\nmodels = default_models( # For all non-specified variables use the following defaults\n        Q_binary     = stack_binary, # A Super Learner\n        Q_continuous = xgboost_regressor, # An XGBoost\n        # T₁ with XGBoost prepended with a Continuous Encoder\n        T₁           = xgboost_classifier\n        # Unspecified G defaults to Logistic Regression\n)\n\ntmle_custom = Tmle(models=models)","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Notice that with_encoder is simply a shorthand to construct a pipeline with a ContinuousEncoder and that the resulting models is simply a Dict.","category":"page"},{"location":"user_guide/estimation/#CV-Estimation","page":"Estimators","title":"CV-Estimation","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Canonical TMLE/OSE are essentially using the dataset twice, once for the estimation of the nuisance functions and once for the estimation of the parameter of interest. This means that there is a risk of over-fitting and residual bias (see here for some discussion). One way to address this limitation is to use a technique called sample-splitting / cross-validation. In order to activate the sample-splitting mode, simply provide a MLJ.ResamplingStrategy using the resampling keyword argument:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Tmle(resampling=StratifiedCV());","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"or","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Ose(resampling=StratifiedCV(nfolds=3));","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"There are some practical considerations","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Choice of resampling Strategy: The theory behind sample-splitting requires the nuisance functions to be sufficiently well estimated on each and every fold. A practical aspect of it is that each fold should contain a sample representative of the dataset. In particular, when the treatment and outcome variables are categorical it is important to make sure the proportions are preserved. This is typically done using StratifiedCV.\nComputational Complexity: Sample-splitting results in K fits of the nuisance functions, drastically increasing computational complexity. In particular, if the nuisance functions are estimated using (P-fold) Super-Learning, this will result in two nested cross-validation loops and K times P fits.\nCaching of Nuisance Functions: Because the resampling strategy typically needs to preserve the outcome and treatment proportions, very little reuse of cached models is possible (see Using the Cache).","category":"page"},{"location":"user_guide/estimation/#C-TMLE","page":"Estimators","title":"C-TMLE","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Collaborative TMLE (C-TMLE) is an estimation strategy which optimises all nuisance parameters in order to improve estimation performance. For some background material, see this paper and the original paper. In the example of the average treatment effect, the propensity score is optimised alongside the targeted step optimising the outcome mean. A sequence of nested estimators is built and the propensity score candidate minimising the cross-validated loss of the associated targeted outcome mean model is selected.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"There exist many strategies to optimise the propensity score, at the moment we provide two such strategies in this package. Since C-TMLE is an expensive estimation strategy, an additional patience variable can be used for early stopping. If the cross-validated loss of the targeted outcome mean model is not reduced in patience iterations, the procedure returns the current best candidate.","category":"page"},{"location":"user_guide/estimation/#Greedy-Strategy","page":"Estimators","title":"Greedy Strategy","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"This is the original implementation of the C-TMLE template, it operates as a forward variable selection. Initially, the propensity score consists in a marginal model p(TW)=p(T). Then, at each iteration, remaining confounding variables are temptatively added to the propensity score model one at a time. The variable minimising the targeted outcome mean model's loss is retained for the iteration.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"greedy_strategy = GreedyStrategy(patience=10)\ngreedy_ctmle = Tmle(collaborative_strategy=greedy_strategy)","category":"page"},{"location":"user_guide/estimation/#Adaptive-Correlation-Strategy","page":"Estimators","title":"Adaptive Correlation Strategy","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"This is a scalable version of the C-TMLE template which also operates as a forward selection method. However, instead of iterating through all potential confounders at each iteration, the confounder most associated with the residuals of the last targeted outcome mean is selected.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"adapt_cor_strategy = AdaptiveCorrelationStrategy(patience=10)\nadapt_cor_ctmle = Tmle(collaborative_strategy=adapt_cor_strategy)","category":"page"},{"location":"user_guide/estimation/#Using-the-Cache","page":"Estimators","title":"Using the Cache","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"TMLE and OSE are expensive procedures, it may therefore be useful to store some information for further reuse. This is the purpose of the cache object, which is produced as a byproduct of the estimation process. ","category":"page"},{"location":"user_guide/estimation/#Reusing-Models","page":"Estimators","title":"Reusing Models","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"The cache contains in particular the machine-learning models that were fitted in the process and which can sometimes be reused to estimate other quantities of interest. For example, say we are now interested in the Joint Average Treatment Effect of both T₁ and T₂. We can provide the cache to the next round of estimation as follows.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Ψ₃ = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=true, control=false), \n        T₂=(case=true, control=false)\n    ),\n    treatment_confounders=(\n        T₁=[:W₁₁, :W₁₂], \n        T₂=[:W₂₁, :W₂₂],\n    ),\n    outcome_extra_covariates=[:C]\n)\nresult₃, cache = tmle(Ψ₃, dataset; cache=cache);\nresult₃\nnothing # hide","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Only the conditional distribution of Y given T₁ and T₂ is fitted as it is absent from the cache. However, the propensity scores corresponding to T₁ and T₂ have been reused. Finally, let's see what happens if we estimate the interaction effect between T₁ and T₂ on Y.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Ψ₄ = AIE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=true, control=false), \n        T₂=(case=true, control=false)\n    ),\n    treatment_confounders=(\n        T₁=[:W₁₁, :W₁₂], \n        T₂=[:W₂₁, :W₂₂],\n    ),\n    outcome_extra_covariates=[:C]\n)\nresult₄, cache = tmle(Ψ₄, dataset; cache=cache);\nresult₄\nnothing # hide","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"All nuisance functions have been reused, only the fluctuation is fitted!","category":"page"},{"location":"user_guide/estimation/#Accessing-Fluctuations'-Reports","page":"Estimators","title":"Accessing Fluctuations' Reports","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"The cache also holds the last targeted factor that was estimated if TMLE was used. Some key information related to the targeting steps can be accessed, for example:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"gradients(cache);\nestimates(cache);\nepsilons(cache)","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"correspond to the gradients, point estimates and epsilons obtained after each targeting step which was performed (usually only one).","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"One can for instance check that the mean of the gradient is close to zero.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"mean(last(gradients(cache)))","category":"page"},{"location":"user_guide/estimation/#Joint-Estimands-and-Composition","page":"Estimators","title":"Joint Estimands and Composition","text":"","category":"section"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"As explained in Joint And Composed Estimands, a joint estimand is simply a collection of estimands. Here, we will illustrate that an Average Interaction Effect is also defined as a difference in partial Average Treatment Effects.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"More precisely, we would like to see if the left-hand side of this equation is equal to the right-hand side:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"AIE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 = ATE_T_1=0 rightarrow 1 T_2=0 rightarrow 1 - ATE_T_1=0 T_2=0 rightarrow 1 - ATE_T_1=0 rightarrow 1 T_2=0","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"For that, we need to define a joint estimand of three components:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"ATE₁ = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=true, control=false), \n        T₂=(case=false, control=false)),\n    treatment_confounders=(\n        T₁=[:W₁₁, :W₁₂], \n        T₂=[:W₂₁, :W₂₂],\n    ),\n)\nATE₂ = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=false, control=false), \n        T₂=(case=true, control=false)),\n    treatment_confounders=(\n        T₁=[:W₁₁, :W₁₂], \n        T₂=[:W₂₁, :W₂₂],\n    ),\n    )\njoint_estimand = JointEstimand(Ψ₃, ATE₁, ATE₂)","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"where the interaction Ψ₃ was defined earlier. This joint estimand can be estimated like any other estimand using our estimator of choice:","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"joint_estimate, cache = tmle(joint_estimand, dataset, cache=cache, verbosity=0);\njoint_estimate","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"The printed output is the result of a Hotelling's T2 Test which is the multivariate counterpart of the Student's T Test. It tells us whether any of the component of this joint estimand is different from 0.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"Then we can formally test our hypothesis by leveraging the multivariate Central Limit Theorem and Julia's automatic differentiation.","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"composed_result = compose(x -> x[1] - x[2] - x[3], joint_estimate)\nisapprox(\n    estimate(result₄),\n    first(estimate(composed_result)),\n    atol=0.1\n)","category":"page"},{"location":"user_guide/estimation/","page":"Estimators","title":"Estimators","text":"By default, TMLE.jl will use Zygote but since we are using DifferentiationInterface.jl you can change the backend to your favorite AD system.","category":"page"},{"location":"integrations/#Integrations","page":"Integrations","title":"Integrations","text":"","category":"section"},{"location":"integrations/#CausalTables.jl","page":"Integrations","title":"CausalTables.jl","text":"","category":"section"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"The CausalTables.jl package provides a simple Tables-compliant interface allowing users to wrap data and structural causal information together into one object. It also allows users to easily simulate data from a known structural causal model for experimentation purposes.","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"TMLE.jl estimators can take CausalTable objects as input, in which case the user does not need to identify a statistical estimand from a causal one – it will be identified automatically from the CausalTable. ","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"Using CausalTables.jl, one can define a structural causal model where the distribution of each variable is known, and sample from it using the rand function. This yields a CausalTable object that stores the underlying causal structure (the same information as that contained in an SCM object in TMLE.jl). ","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"Estimating a causal quantity in this scenario is now simpler: one does not need to use the identify function or define the variables needed in the statistical estimand; just call the estimator with the CausalTable object!","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"using TMLE\nusing CausalTables\nusing Distributions\n# Sample a random dataset endowed with causal structure\n# using the CausalTables.jl package\nscm = StructuralCausalModel(@dgp(\n        W ~ Beta(2,2),\n        A ~ Binomial.(1, W),\n        Y ~ Normal.(A .+ W, 0.5)\n    ); treatment = :A, response = :Y)\n\nct = rand(scm, 100)\n\n# Define a causal estimand and estimate it using TMLE\nΨ = ATE(outcome = :Y, treatment_values = (A = (case = 1, control = 0),))\nestimator = Tmle()\nΨ̂, cache = estimator(Ψ, ct; verbosity=0)","category":"page"},{"location":"integrations/#Serialization-to-JSON-/-YAML","page":"Integrations","title":"Serialization to JSON / YAML","text":"","category":"section"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"Estimands and estimates can be serialized to disk in JSON or YAML format using TMLE.write_json or TMLE.write_yaml. Let's serialize the estimand and estimate from the previous example.","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"using JSON\nusing YAML\n\nTMLE.write_json(\"estimand.json\", Ψ)\nTMLE.write_yaml(\"estimate.yaml\", Ψ̂)","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"Now let's deserialize the estimand and run TMLE again","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"deserialized_Ψ = TMLE.read_json(\"estimand.json\")\nΨ̂_from_serialized_Ψ, cache = estimator(deserialized_Ψ, ct; cache=cache, verbosity=0)\n","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"The new estimate should match the previously serialized one","category":"page"},{"location":"integrations/","page":"Integrations","title":"Integrations","text":"deserialized_Ψ̂ = TMLE.read_yaml(\"estimate.yaml\")\nTMLE.estimate(deserialized_Ψ̂) ≈ TMLE.estimate(Ψ̂_from_serialized_Ψ)","category":"page"},{"location":"examples/super_learning/#Becoming-a-Super-Learner","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"","category":"section"},{"location":"examples/super_learning/#What-this-tutorial-is-about","page":"Becoming a Super Learner","title":"What this tutorial is about","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Super Learning, also known as Stacking, is an ensemble technique that was first introduced by Wolpert in 1992. Instead of selecting a model based on cross-validation performance, models are combined by a meta-learner to minimize the cross-validation error. It has also been shown by van der Laan et al. that the resulting Super Learner will perform at least as well as its best performing submodel (at least asymptotically).","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Why is it important for Targeted Learning?","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"The short answer is that the consistency (convergence in probability) of the targeted estimator depends on the consistency of at least one of the nuisance estimands: Q_0 or G_0. By only using unrealistic models like linear models, we have little chance of satisfying the above criterion. Super Learning is a data driven way to leverage a diverse set of models and build the best performing estimator for both Q_0 or G_0.","category":"page"},{"location":"examples/super_learning/#The-dataset","page":"Becoming a Super Learner","title":"The dataset","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Let's consider the case where Y is categorical. In TMLE.jl, this could be useful to learn:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"The propensity score\nThe outcome model when the outcome is binary","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"We will use the following moons dataset:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"using MLJBase\nusing MLJTuning\nusing StatisticalMeasures\nusing MLJXGBoostInterface\nusing MLJLinearModels\nusing NearestNeighborModels\n\nX, y = MLJBase.make_moons(1000)\nnothing # hide","category":"page"},{"location":"examples/super_learning/#Defining-a-Super-Learner-in-MLJ","page":"Becoming a Super Learner","title":"Defining a Super Learner in MLJ","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"In MLJ, a Super Learner can be defined using the Stack function. The three most important type of arguments for a Stack are:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"metalearner: The metalearner to be used to combine the weak learner to be defined. Typically a generalized linear model.\nresampling: The cross-validation scheme, by default, a 6-fold cross-validation. Since we are working with categorical","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"data it is a good idea to make sure the splits are balanced. We will thus use a StratifiedCV resampling strategy.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"models...: A series of named MLJ models.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"One important point is that MLJ does not provide any model by itself, juat the API, models have to be loaded from external compatible libraries.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"note: Stack limitation\nThe Stack cannot contain <:Deterministic models for classification.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Let's load a few packages providing models and build our first Stack:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"resampling = StratifiedCV()\nmetalearner = LogisticClassifier()\n\nstack = Stack(\n    metalearner = metalearner,\n    resampling  = resampling,\n    lr          = LogisticClassifier(),\n    knn         = KNNClassifier(K=3)\n)","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"This Stack only contains 2 different models: a logistic classifier and a KNN classifier. A Stack is just like any MLJ model, it can be wrapped in a machine and fitted:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"mach = machine(stack, X, y)\nfit!(mach, verbosity=0)","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Or evaluated. Because the Stack contains a cross-validation procedure, this will result in two nested levels of resampling.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"evaluate!(mach, measure=log_loss, resampling=resampling)","category":"page"},{"location":"examples/super_learning/#A-more-advanced-Stack","page":"Becoming a Super Learner","title":"A more advanced Stack","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"What are good Stack members? Virtually anything, provided they are MLJ models. Here are a few examples:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"You can use the stack to \"select\" model hyper-parameters. e.g. KNNClassifier(K=3) or KNNClassifier(K=2)?\nYou can also use self-tuning models. Note that because","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"these models resort to cross-validation, fitting the stack will result in two nested levels of sample-splitting.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"The following self-tuned XGBoost will vary some hyperparameters in an internal sample-splitting procedure in order to optimize the Log-Loss. It will then be combined with the rest of the models in the Stack's own sample-splitting procedure. Finally, evaluation is performed in an outer sample-split.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"xgboost = XGBoostClassifier(tree_method=\"hist\", nthread=1)\nself_tuning_xgboost = TunedModel(\n    model = xgboost,\n    resampling = resampling,\n    tuning = Grid(goal=20),\n    range = [\n        range(xgboost, :max_depth, lower=3, upper=7),\n        range(xgboost, :lambda, lower=1e-5, upper=10, scale=:log)\n        ],\n    measure = log_loss,\n    cache=false\n)\n\nstack = Stack(\n    metalearner         = metalearner,\n    resampling          = resampling,\n    self_tuning_xgboost = self_tuning_xgboost,\n    lr                  = LogisticClassifier(),\n    knn_2               = KNNClassifier(K=2),\n    knn_3               = KNNClassifier(K=3),\n    cache               = false\n)\n\nmach = machine(stack, X, y, cache=false)\nevaluate!(mach, measure=log_loss, resampling=resampling)","category":"page"},{"location":"examples/super_learning/#Diagnostic","page":"Becoming a Super Learner","title":"Diagnostic","text":"","category":"section"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Optionally, one can also investigate how sucessful the weak learners were in the Stack's internal cross-validation. This is done by specifying the measures keyword argument.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"Here we look at both the Log-Loss and the AUC.","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"stack.measures = [log_loss, auc]\nfit!(mach, verbosity=0)\nreport(mach).cv_report","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"One can look at the fitted parameters for the metalearner as well:","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"fitted_params(mach).metalearner","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"","category":"page"},{"location":"examples/super_learning/","page":"Becoming a Super Learner","title":"Becoming a Super Learner","text":"This page was generated using Literate.jl.","category":"page"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"","category":"section"},{"location":"api/#TMLE.AdaptiveCorrelationStrategy","page":"API Reference","title":"TMLE.AdaptiveCorrelationStrategy","text":"AdaptiveCorrelationStrategy()\n\nThis strategy adaptively selects the confounding variable that is the most correlated with the last residuals of the outcome mean estimator.\n\n\n\n\n\n","category":"type"},{"location":"api/#TMLE.CausalStratifiedCV-Tuple{}","page":"API Reference","title":"TMLE.CausalStratifiedCV","text":"CausalStratifiedCV(;resampling=StratifiedCV())\n\nApplies a stratified cross-validation strategy based on both treatments and outcome (if it is Finite) variables.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.Configuration-Tuple{}","page":"API Reference","title":"TMLE.Configuration","text":"Configuration(;estimands, scm=nothing, adjustment=nothing) = Configuration(estimands, scm, adjustment)\n\nA Configuration is a set of estimands to be estimated. If the set of estimands contains causal (identifiable) estimands,  these will be identified using the provided scm and adjustment method.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.Ose-Tuple{}","page":"API Reference","title":"TMLE.Ose","text":"Ose(;models=default_models(), resampling=nothing, ps_lowerbound=1e-8, machine_cache=false)\n\nDefines a One Step Estimator using the specified models for estimation of the nuisance parameters. The estimator is a  function that can be applied to estimate estimands for a dataset.\n\nConstructor Arguments\n\nmodels: A Dict(variable => model, ...) where the variables are the outcome variables modeled by the models.\nresampling: Outer resampling strategy. Setting it to nothing (default) falls back to vanilla estimation while \n\nany valid MLJ.ResamplingStrategy will result in CV-OSE.\n\nps_lowerbound: Lowerbound for the propensity score to avoid division by 0. The special value nothing will \n\nresult in a data adaptive definition as described in here.\n\nmachine_cache: Whether MLJ.machine created during estimation should cache data.\n\nRun Argument\n\nΨ: parameter of interest\ndataset: A DataFrame \ncache (default: Dict()): A dictionary to store nuisance function fits.\nverbosity (default: 1): Verbosity level.\nacceleration (default: CPU1()): acceleration strategy for parallelised estimation of nuisance functions.\n\nExample\n\nusing MLJLinearModels\nmodels = Dict(:Y => LinearRegressor(), :T => LogisticClassifier())\nose = Ose()\nΨ̂ₙ, cache = ose(Ψ, dataset)\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.SCM","page":"API Reference","title":"TMLE.SCM","text":"A SCM is simply a wrapper around a MetaGraph over a Directed Acyclic Graph.\n\n\n\n\n\n","category":"type"},{"location":"api/#TMLE.Tmle-Tuple{}","page":"API Reference","title":"TMLE.Tmle","text":"Tmle(;models=default_models(), resampling=nothing, ps_lowerbound=1e-8, weighted=false, tol=nothing, machine_cache=false)\n\nDefines a TMLE estimator using the specified models for estimation of the nuisance parameters. The estimator is a  function that can be applied to estimate estimands for a dataset.\n\nConstructor Arguments\n\nmodels (default: default_models()): A Dict(variable => model, ...) where the variables are the outcome variables modeled by the models.\ncollaborative_strategy (default: nothing): A collaborative strategy to use for the estimation. Then the resampling strategy is used  to evaluate the candidates.\nresampling (default: default_resampling(collaborative_strategy)): Outer resampling strategy. Setting it to nothing (default) falls back to vanilla TMLE while \n\nany valid MLJ.ResamplingStrategy will result in CV-TMLE.\n\nps_lowerbound (default: 1e-8): Lowerbound for the propensity score to avoid division by 0. The special value nothing will \n\nresult in a data adaptive definition as described in here.\n\nweighted (default: false): Whether the fluctuation model is a classig GLM or a weighted version. The weighted fluctuation has \n\nbeen show to be more robust to positivity violation in practice.\n\ntol (default: nothing): Convergence threshold for the TMLE algorithm iterations. If nothing (default), 1/(sample size) will be used. See also max_iter.\nmax_iter (default: 1): Maximum number of iterations for the TMLE algorithm.\nmachine_cache (default: false): Whether MLJ.machine created during estimation should cache data.\n\nRun Argument\n\nΨ: parameter of interest\ndataset: A DataFrame \ncache (default: Dict()): A dictionary to store nuisance function fits.\nverbosity (default: 1): Verbosity level.\nacceleration (default: CPU1()): acceleration strategy for parallelised estimation of nuisance functions.\n\nExample\n\nusing MLJLinearModels\ntmle = Tmle()\nΨ̂ₙ, cache = tmle(Ψ, dataset)\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.estimate-Tuple{TMLE.JointEstimate}","page":"API Reference","title":"Distributions.estimate","text":"Distributions.estimate(r::JointEstimate)\n\nRetrieves the final estimate: after the TMLE step.\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.estimate-Tuple{Union{TMLE.OSEstimate, TMLE.TMLEstimate}}","page":"API Reference","title":"Distributions.estimate","text":"Distributions.estimate(r::EICEstimate)\n\nRetrieves the final estimate: after the TMLE step.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.StaticSCM-Tuple{Any, Any, Any}","page":"API Reference","title":"TMLE.StaticSCM","text":"A plate Structural Causal Model where:\n\nFor all outcomes: oᵢ = fᵢ(treatments, confounders, outcomeextracovariates)\nFor all treatments: tⱼ = fⱼ(confounders)\n\nExample\n\nStaticSCM([:Y], [:T₁, :T₂], [:W₁, :W₂, :W₃]; outcomeextracovariates=[:C])\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.brute_force_ordering-Tuple{Any}","page":"API Reference","title":"TMLE.brute_force_ordering","text":"brute_force_ordering(estimands; η_counts = nuisance_function_counts(estimands))\n\nFinds an optimal ordering of the estimands to minimize maximum cache size.  The approach is a brute force one, all permutations are generated and evaluated,  if a minimum is found fast it is immediatly returned. The theoretical complexity is in O(N!). However due to the stop fast approach and  the shuffling, this is actually expected to be much smaller than that.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.compose-Tuple{Any, TMLE.JointEstimate}","page":"API Reference","title":"TMLE.compose","text":"compose(f, estimation_results::Vararg{EICEstimate, N}) where N\n\nProvides an estimator of f(estimation_results...).\n\nMathematical details\n\nThe following is a summary from Asymptotic Statistics, A. W. van der Vaart.\n\nConsider k TMLEs computed from a dataset of size n and embodied by Tₙ = (T₁,ₙ, ..., Tₖ,ₙ).  Since each of them is asymptotically normal, the multivariate CLT provides the joint  distribution:\n\n√n(Tₙ - Ψ₀) ↝ N(0, Σ),\n\nwhere Σ is the covariance matrix of the TMLEs influence curves.\n\nLet f:ℜᵏ→ℜᵐ, be a differentiable map at Ψ₀. Then, the delta method provides the limiting distribution of √n(f(Tₙ) - f(Ψ₀)). Because Tₙ is normal, the result is:\n\n√n(f(Tₙ) - f(Ψ₀)) ↝ N(0, ∇f(Ψ₀) ̇Σ ̇(∇f(Ψ₀))ᵀ),\n\nwhere ∇f(Ψ₀):ℜᵏ→ℜᵐ is a linear map such that by abusing notations and identifying the  function with the multiplication matrix: ∇f(Ψ₀):h ↦ ∇f(Ψ₀) ̇h. And the matrix ∇f(Ψ₀) is  the jacobian of f at Ψ₀.\n\nHence, the only thing we need to do is:\n\nCompute the covariance matrix Σ\nCompute the jacobian ∇f, which can be done using Julia's automatic differentiation facilities.\nThe final estimator is normal with mean f₀=f(Ψ₀) and variance σ₀=∇f(Ψ₀) ̇Σ ̇(∇f(Ψ₀))ᵀ\n\nArguments\n\nf: An array-input differentiable map.\nestimation_results: 1 or more EICEstimate structs.\n\nExamples\n\nAssuming res₁ and res₂ are TMLEs:\n\nf(x, y) = [x^2 - y, y - 3x]\ncompose(f, res₁, res₂)\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.default_models-Tuple{}","page":"API Reference","title":"TMLE.default_models","text":"default_models(;Q_binary=LinearBinaryClassifier(), Q_continuous=LinearRegressor(), G=LinearBinaryClassifier()) = (\n\nCreate a Dictionary containing default models to be used by downstream estimators.  Each provided model is prepended (in a MLJ.Pipeline) with an MLJ.ContinuousEncoder.\n\nBy default:     - Qbinary is a LinearBinaryClassifier     - Qcontinuous is a LinearRegressor     - G is a LinearBinaryClassifier\n\nExample\n\nThe following changes the default Q_binary to a LogisticClassifier and provides a RidgeRegressor for special_y. \n\nusing MLJLinearModels\nmodels = default_models(\n    Q_binary  = LogisticClassifier(),\n    special_y = RidgeRegressor()\n)\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.epsilons-Tuple{Any}","page":"API Reference","title":"TMLE.epsilons","text":"epsilons(cache)\n\nRetrieves the fluctuations' epsilons corresponding to each targeting step from the cache.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.estimates-Tuple{Any}","page":"API Reference","title":"TMLE.estimates","text":"estimates(cache)\n\nRetrieves the estimates corresponding to each targeting step from the cache.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.factorialEstimand-Tuple{Union{typeof(AIE), typeof(ATE), typeof(CM)}, Any, Any}","page":"API Reference","title":"TMLE.factorialEstimand","text":"factorialEstimand(\n    constructor::Union{typeof(CM), typeof(ATE), typeof(AIE)},\n    treatments, outcome; \n    confounders=nothing,\n    dataset=nothing, \n    outcome_extra_covariates=(),\n    positivity_constraint=nothing,\n    freq_table=nothing,\n    verbosity=1\n)\n\nGenerates a factorial JointEstimand with components of type constructor (CM, ATE, AIE). \n\nFor the ATE and the AIE, the generated components are restricted to the Cartesian Product of single treatment levels transitions. For example, consider two treatment variables T₁ and T₂ each taking three possible values (0, 1, 2).  For each treatment variable, the single treatment levels transitions are defined by (0 → 1, 1 → 2).  Then, the Cartesian Product of these transitions is taken, resulting in a 2 x 2 = 4 dimensional joint estimand:\n\n(T₁: 0 → 1, T₂: 0 → 1)\n(T₁: 0 → 1, T₂: 1 → 2)\n(T₁: 1 → 2, T₂: 0 → 1)\n(T₁: 1 → 2, T₂: 1 → 2)\n\nReturn\n\nA JointEstimand with causal or statistical components.\n\nArgs\n\nconstructor: CM, ATE or AIE.\ntreatments: An AbstractDictionary/NamedTuple of treatment levels (e.g. (T=(0, 1, 2),)) or a treatment iterator, then a dataset must be provided to infer the levels from it.\noutcome: The outcome variable.\nconfounders=nothing: The generated components will inherit these confounding variables. If nothing, causal estimands are generated.\noutcome_extra_covariates=(): The generated components will inherit these outcome_extra_covariates.\ndataset: An optional dataset to enforce a positivity constraint and infer treatment levels.\npositivity_constraint=nothing: Only components that pass the positivity constraint are added to the JointEstimand. A dataset must then be provided.\nfreq_table: This is only to be used by factorialEstimands to avoid unecessary computations.\nverbosity=1: Verbosity level.\n\nExamples:\n\nAn Average Treatment Effect with causal components:\n\nfactorialEstimand(ATE, (T₁ = (0, 1), T₂=(0, 1, 2)), :Y₁)\n\nAn Average Interaction Effect with statistical components:\n\nfactorial(AIE, (T₁ = (0, 1, 2), T₂=(0, 1, 2)), :Y₁, confounders=[:W₁, :W₂])\n\nWith a dataset, the treatment levels can be infered and a positivity constraint enforced:\n\nInteractions:\n\nfactorialEstimand(ATE, [:T₁, :T₂], :Y₁, \n    confounders=[:W₁, :W₂], \n    dataset=dataset, \n    positivity_constraint=0.1\n)\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.factorialEstimands-Tuple{Union{typeof(AIE), typeof(ATE), typeof(CM)}, Any, Any}","page":"API Reference","title":"TMLE.factorialEstimands","text":"factorialEstimands(     constructor::Union{typeof(ATE), typeof(AIE)},     dataset, treatments, outcomes;      confounders=nothing,      outcomeextracovariates=(),     positivity_constraint=nothing,     verbosity=1     )\n\nGenerates a JointEstimand for each outcome in outcomes. See factorialEstimand.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.gradients-Tuple{Any}","page":"API Reference","title":"TMLE.gradients","text":"gradients(cache)\n\nRetrieves the gradients corresponding to each targeting step from the cache.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.groups_ordering-Tuple{Any}","page":"API Reference","title":"TMLE.groups_ordering","text":"groups_ordering(estimands)\n\nThis will order estimands based on: propensity score first, outcome mean second. This heuristic should  work reasonably well in practice. It could be optimized further by:\n\nOrganising the propensity score groups that share similar components to be close together. \nBrute forcing the ordering of these groups to find an optimal one.\n\n\n\n\n\n","category":"method"},{"location":"api/#TMLE.significance_test","page":"API Reference","title":"TMLE.significance_test","text":"significance_test(estimate::EICEstimate, Ψ₀=0)\n\nPerforms a TTest\n\n\n\n\n\n","category":"function"},{"location":"api/#TMLE.significance_test-2","page":"API Reference","title":"TMLE.significance_test","text":"significance_test(estimate::JointEstimate, Ψ₀=zeros(size(estimate.estimate, 1)))\n\nPerforms a TTest if the estimate is one dimensional and a HotellingT2Test otherwise.\n\n\n\n\n\n","category":"function"},{"location":"estimators_cheatsheet/#Estimators'-Cheatsheet","page":"Estimators' Cheat Sheet","title":"Estimators' Cheatsheet","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"This section is an effort to succinctly summarize the definition of semi-parametric estimators available in this package. As such, it is not self-contained, rather, it is intended as a mathematical memo that can be quickly searched. Gradients, One-Step and Targeted Maximum-Likelihood estimators are provided for the Counterfactual Mean, Average Treatment Effect and Average Interaction Effect. Estimators are presented in both their canonical and cross-validated versions.","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"One major difficulty I personally faced when entering the field, was the overwhelming notational burden. Unfortunately, this burden is necessary to understand how the various mathematical objects are handled by the procedures presented below. It is thus worth the effort to make sure you understand what each notation means. The reward? After reading this document, you should be able to implement any estimator present in this page.","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Finally, if you find inconsistencies or imprecision, please report it, so we can keep improving!","category":"page"},{"location":"estimators_cheatsheet/#Where-it-all-begins","page":"Estimators' Cheat Sheet","title":"Where it all begins","text":"","category":"section"},{"location":"estimators_cheatsheet/#Notations","page":"Estimators' Cheat Sheet","title":"Notations","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"This is the notation we use throughout:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"The observed data: We assume we observe the realization of a random vector boldZ_n = (Z_1  Z_n). The components of boldZ are assumed independent and identically distributed according to mathbbP, i.e. forall i in 1  nZ_i sim mathbbP. Note that each Z_i is usually a vector as well, for us: Z_i = (W_i T_i Y_i).\nThe Empirical Distribution : mathbbP_n(A) = frac1nsum_i=1^n 1_A(Z_i). For each set A, it computes the mean number of points falling into A.\nExpected value of a function f of the data Z with respect to a distribution mathbbP: mathbbPf equiv mathbbE_mathbbPf(Z). Note that for the empirical distribution mathbbP_n, this is simply: mathbbP_nf = frac1nsum_i=1^nf(Z_i), in other words, the sample mean.\nThe Estimand: The unknown finite dimensional quantity we are interested in. Psi is defined as a functional, i.e. mathbbP mapsto Psi(mathbbP) in mathbbR^d. In this document d=1.\nThe Gradient of Psi at the distribution mathbbP : phi_mathbbP, a function of the data, i.e. Z mapsto phi_mathbbP(Z). The gradient satisfies two properties: (i) mathbbPphi_mathbbP = 0, and (ii) Varphi_mathbbP  infty.\nAn estimator, is a function of the data boldZ_n, and thus a random variable, that seeks to approximate an unknown quantity. For instance, hatPsi_n denotes an estimator for Psi. Notice that the empirical distribution is an estimator for mathbbP, but the hat is omitted to distinguish it from other estimators that will be defined later on.","category":"page"},{"location":"estimators_cheatsheet/#The-Counterfactual-Mean","page":"Estimators' Cheat Sheet","title":"The Counterfactual Mean","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Let boldZ=(W T Y)_i=1n be a dataset generated according to the following structural causal model:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\nW = f_W(U_W) \nT = f_T(W U_T) \nY = f_Y(T W U_Y)\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"This is certainly the most common scenario in causal inference where Y is an outcome of interest, T a set of treatment variables and W a set of confounding variables. We are generally interested in the effect of T on Y. In this document we will consider the counterfactual mean as a workhorse. We will show that the estimators for the Average Treatment Effect and Average Interaction Effect can easily be derived from it. Under usual conditions, the counterfactual mean identifies to the following statistical estimand:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"CM_t(mathbbP) = Psi_t(mathbbP) = mathbbE_mathbbPmathbbE_mathbbPY  W T = t = int mathbbE_mathbbPY  W=w T = t dmathbbP(w)","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"From this definition, we can see that Psi_t depends on mathbbP only through two relevant factors:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\nQ_Y(W T) = mathbbE_mathbbPY  W T \nQ_W(W) = mathbbP(W)\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"So that Psi(mathbbP) = Psi(Q_Y Q_W) (the t subscript is dropped as it is unambiguous). This makes explicit that we don't need to estimate mathbbP but only the relevant factors (Q_Y Q_W) to obtain a plugin estimate of Psi(mathbbP). Finally, it will be useful to define an additional factor:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"g(W T) = mathbbP(T  W) ","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"This is because the gradient of Psi:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"phi_CM_t mathbbP(W T Y) = fracmathbb1(T = t)g(W t)(Y  Q_Y(W t)) + Q_Y(W t)  Psi(mathbbP) ","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"which is the foundation of semi-parametric estimation, depends on this so-called nuisance parameter.","category":"page"},{"location":"estimators_cheatsheet/#Average-Treatment-Effect-and-Average-Interaction-Effect","page":"Estimators' Cheat Sheet","title":"Average Treatment Effect and Average Interaction Effect","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"They are simple linear combinations of counterfactual means and so are their gradients.","category":"page"},{"location":"estimators_cheatsheet/#Average-Treatment-Effect","page":"Estimators' Cheat Sheet","title":"Average Treatment Effect","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"In all generality, for two values of a categorical treatment variable T t_control rightarrow t_case, the Average Treatment Effect (ATE) is defined by:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"ATE_t_case t_control(mathbbP) = (CM_t_case - CM_t_control)(mathbbP) ","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"And it's associated gradient is:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\nphi_ATE(W T Y) = (phi_CM_t_case - phi_CM_t_control)(W T Y) \n=  H(W T)(Y  Q_Y(W T)) + C(W)  ATE_t_case t_control(mathbbP)\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"with:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\n  begincases\n    H(W T) = frac mathbb1(T = t_case) - mathbb1(T = t_control)g(W T) \n    C(W) = (Q_Y(W t_case) - Q_Y(W t_control))\n  endcases\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"H is also known as the clever covariate in the Targeted Learning literature (see later).","category":"page"},{"location":"estimators_cheatsheet/#Average-Interaction-Effect","page":"Estimators' Cheat Sheet","title":"Average Interaction Effect","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"For simplicity, we only consider two treatments T = (T_1 T_2) such that T_1 t_1control rightarrow t_1case and T_2 t_2control rightarrow t_2case. The Average Interaction Effect (AIE) of (T_1 T_2) is defined as:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\nAIE(mathbbP) = (CM_t_1 case t_2 case - CM_t_1 case t_2 control \n- CM_t_1 control t_2 case + CM_t_1 control t_2 control)(mathbbP)\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"And its gradient is given by:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\nphi_AIE (W T Y) = (phi_CM_t_1case t_2case - phi_CM_t_1case t_2control \n- phi_CM_t_1control t_2case + phi_CM_t_1control t_2control)(W T Y) \n=  H(W T)(Y  Q_Y(W T)) + C(W)  ATE_t_case t_control(mathbbP)\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"with:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\n  begincases\n    H(W T) = frac(mathbb1(T_1 = t_1 case) - mathbb1(T_1 = t_1 control))(mathbb1(T_2 = t_2 case) - mathbb1(T_2 = t_2 control))g(W T) \n    C(W) = (Q_Y(W t_1 case t_2 case) - Q_Y(W t_1 case t_2 control) - Q_Y(W t_1 control t_2 case) + Q_Y(W t_1 control t_2 control))\n  endcases\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"For higher-order interactions the two factors H and C can be similarly inferred.","category":"page"},{"location":"estimators_cheatsheet/#Asymptotic-Analysis-of-Plugin-Estimators","page":"Estimators' Cheat Sheet","title":"Asymptotic Analysis of Plugin Estimators","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"The theory is based on the von Mises expansion (functional equivalent of Taylor's expansion) which reduces due to the fact that the gradient satisfies: mathbbE_mathbbPphi_mathbbP(Z) = 0.","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\nPsi(hatmathbbP)  Psi(mathbbP) = int phi_hatmathbbP(z) mathrmd(hatmathbbP  mathbbP)(z) + R_2(hatmathbbP mathbbP) \n= - int phi_hatmathbbP(z) mathrmdmathbbP(z) + R_2(hatmathbbP mathbbP)\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"This suggests that a plugin estimator, one that simply evaluates Psi at an estimator hatmathbbP of mathbbP, is biased. This bias can be elegantly decomposed in four terms by reworking the previous expression:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Psi(hatmathbbP)  Psi(mathbbP) = mathbbP_nphi_mathbbP(Z) - mathbbP_nphi_mathbbhatP(Z) + (mathbbP_n  mathbbP)(ϕ_mathbbhatP(Z)  phi_mathbbP(Z)) + R_2(mathbbhatP mathbbP)","category":"page"},{"location":"estimators_cheatsheet/#1.-The-Asymptotically-Linear-Term","page":"Estimators' Cheat Sheet","title":"1. The Asymptotically Linear Term","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"mathbbP_nphi_mathbbP(Z)","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"By the central limit theorem, it is asymptotically normal with variance Varphin, it will be used to construct a confidence interval for our final estimate.","category":"page"},{"location":"estimators_cheatsheet/#2.-The-First-Order-Bias-Term","page":"Estimators' Cheat Sheet","title":"2. The First-Order Bias Term","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"- mathbbP_nphi_mathbbhatP(Z)","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"This is the term both the One-Step estimator and the Targeted Maximum-Likelihood estimator deal with.","category":"page"},{"location":"estimators_cheatsheet/#3.-The-Empirical-Process-Term","page":"Estimators' Cheat Sheet","title":"3. The Empirical Process Term","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"(mathbbP_n  mathbbP)(ϕ_mathbbhatP(Z)  phi_mathbbP(Z))","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"This can be shown to be of order o_mathbbP(frac1sqrtn) if phi_hatmathbbP converges to phi_mathbbP in L_2(mathbbP) norm, that is:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"int (phi_hatmathbbP(Z) - phi_mathbbP(Z) )^2 dmathbbP(Z) = o_mathbbPleft(frac1sqrtnright)","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"and, any of the following holds:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"phi (or equivalently its components) is [Donsker], i.e., not too complex.\nThe estimator is constructed using sample-splitting (see cross-validated estimators).","category":"page"},{"location":"estimators_cheatsheet/#4.-The-Exact-Second-Order-Remainder-Term","page":"Estimators' Cheat Sheet","title":"4. The Exact Second-Order Remainder Term","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"R_2(mathbbhatP mathbbP)","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"This term is usually more complex to analyse. Note however, that it is entirely defined by the von Mises expansion, and for the counterfactual mean, it can be shown that if g(WT) geq frac1eta (positivity constraint):","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"R_2(mathbbhatP mathbbP) leq eta hatQ_n Y - Q_Y cdot hatg_n - g","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"and thus, if the estimators hatQ_n Y and hatg_n converge at a rate o_mathbbP(n^-frac14), the second-order remainder will be o_mathbbP(frac1sqrtn). This is the case for many popular estimators like random forests, neural networks, etc...","category":"page"},{"location":"estimators_cheatsheet/#Asymptotic-Linearity-and-Inference","page":"Estimators' Cheat Sheet","title":"Asymptotic Linearity and Inference","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"According to the previous section, the OSE and TMLE will be asymptotically linear with efficient influence curve the gradient phi:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"sqrtn(hatPsi - Psi) = frac1sqrtn sum_i=1^n phi(Z_i) + o_mathbbPleft(frac1sqrtnright)","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"By the Central Limit Theorem and Slutsky's Theorem we have:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"sqrtn(hatPsi - Psi) leadsto mathcalN(0 Std(phi))","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Now, if we consider S_n = hatStd(phi_hatmathbbP) as a consistent estimator for Std(phi), we have by Slutsky's Theorem again, the following pivot:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"fracsqrtn(hatPsi - Psi)S_n leadsto mathcalN(0 1)","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"which can be used to build confidence intervals:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"undersetn to inftylim P(hatPsi_n - fracS_nsqrtnz_alpha leq Psi leq hatPsi_n + fracS_nsqrtnz_alpha) = 1 - 2alpha","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Here, z_alpha denotes the alpha-quantile function of the standard normal distribution","category":"page"},{"location":"estimators_cheatsheet/#One-Step-Estimator","page":"Estimators' Cheat Sheet","title":"One-Step Estimator","text":"","category":"section"},{"location":"estimators_cheatsheet/#Canonical-OSE","page":"Estimators' Cheat Sheet","title":"Canonical OSE","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"The One-Step estimator is very intuitive, it simply corrects the initial plugin estimator by adding in the residual bias term. As such, it corrects for the bias in the estimand's space. Let hatP= (hatQ_nY hatQ_nW) be an estimator of the relevant factors of mathbbP as well as hatg_n an estimator of the nuisance function g. The OSE is:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"hatPsi_n OSE = Psi(hatP) + mathbbP_nphi_hatP","category":"page"},{"location":"estimators_cheatsheet/#CV-OSE","page":"Estimators' Cheat Sheet","title":"CV-OSE","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Instead of assuming Donsker conditions limiting the complexity of the algorithms used, we can use sample splitting techniques. For this, we split the data into K folds of (roughly) equal size. For a given sample i, we denote by k(i) the fold it belongs to (called validation fold/set) and by -k(i) the union of all remaining folds (called training set). Similarly, we denote by hatQ^k an estimator for Q obtained from samples in the validation fold k and hatQ^-k an estimator for Q obtained from samples in the (training) fold 1  K-k.","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"The cross-validated One-Step estimator can be compactly written as an average over the folds of sub one-step estimators:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\nhatPsi_n CV-OSE = sum_k=1^K fracN_kn (Psi(hatQ_Y^-k hatQ_W^k) + hatmathbbP_n^k phi^-k) \n= frac1n sum_k=1^K sum_i k(i) = k (hatQ_Y^-k(W_i T_i) + phi^-k(W_i T_i Y_i))\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Where the first equation is the general form of the estimator while the second one corresponds to the counterfactual mean. The important thing to note is that for each sub one-step estimator, the sum runs over the validation samples while hatQ_Y^-k and hatphi^-k are estimated using the training samples.","category":"page"},{"location":"estimators_cheatsheet/#Targeted-Maximum-Likelihood-Estimator","page":"Estimators' Cheat Sheet","title":"Targeted Maximum-Likelihood Estimator","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Unlike the One-Step estimator, the Targeted Maximum-Likelihood Estimator corrects the bias term in distribution space. That is, it moves the initial estimate hatmathbbP^0=hatmathbbP to a corrected hatmathbbP^* (notice the new superscript notation). Then the plugin principle can be applied and the targeted estimator is simply hatPsi_n TMLE = Psi(hatmathbbP^*). This means TMLE always respects the natural range of the estimand, giving it an upper hand on the One-Step estimator.","category":"page"},{"location":"estimators_cheatsheet/#Canonical-TMLE","page":"Estimators' Cheat Sheet","title":"Canonical TMLE","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"The way hatmathbbP is modified is by means of a parametric sub-model also known as a fluctuation. The choice of fluctuation depends on the target parameter of interest. It can be shown that for the conditional mean, it is sufficient to fluctuate hatQ_n Y only once using the following fluctuations:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"hatQ_Y epsilon(W T) = hatQ_n Y(T W) + epsilon hatH(T W), for continuous outcomes Y.\nhatQ_Y epsilon(W T) = frac11 + e^-(logit(hatQ_n Y(T W)) + epsilon hatH(T W)), for binary outcomes Y.","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"where hatH(T W) = frac1(T=t)hatg_n(W) is known as the clever covariate. The value of epsilon is obtained by minimizing the loss L associated with Q_Y, that is the mean-squared error for continuous outcomes and negative log-likelihood for binary outcomes. This can easily be done via linear and logistic regression respectively, using the initial fit as off-set.","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"note: Note\nFor the ATE and AIE, just like the gradient is linear in Psi, the clever covariate used to fluctuate the initial hatQ_n Y is as presented in Average Treatment Effect and Average Interaction Effect","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"If we denote by epsilon^* the value of epsilon minimizing the loss, the TMLE is:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"hatPsi_n TMLE = Psi(hatQ_n Y epsilon^* hatQ_n W)","category":"page"},{"location":"estimators_cheatsheet/#CV-TMLE","page":"Estimators' Cheat Sheet","title":"CV-TMLE","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Using the same notation as the cross-validated One-Step estimator, the fluctuated distribution is obtained by solving:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"epsilon^* = undersetepsilonarg min frac1n sum_k=1^K sum_i k(i) = k L(Y_i hatQ_Y epsilon^-k(W_i T_i Y_i))","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"where hatQ_Y epsilon and L are the respective fluctuations and loss for continuous and binary outcomes. This leads to a targeted hatQ_nY^* such that:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"forall i in 1  n hatQ_n Y^*(W_i T_i) = hatQ_Y epsilon^*^-k(i)(W_i T_i)","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"That is, the predictions of hatQ_n Y^* for sample i are based on the out of fold predictions of hatQ_Y^-k(i) and the \"pooled\" fluctuation given by epsilon^*.","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Then, the CV-TMLE is:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"beginaligned\nhatPsi_n CV-TMLE = sum_k=1^K fracN_kn Psi(hatQ_n Y^* hatQ_W^k) \n= frac1n sum_k=1^K sum_i k(i) = k hatQ_n Y^*(W_i T_i)\nendaligned","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Notice that, while hatPsi_n CV-TMLE is not a plugin estimator anymore, it still respects the natural range of the parameter because it is an average of plugin estimators.","category":"page"},{"location":"estimators_cheatsheet/#References","page":"Estimators' Cheat Sheet","title":"References","text":"","category":"section"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"The content of this page is largely inspired from:","category":"page"},{"location":"estimators_cheatsheet/","page":"Estimators' Cheat Sheet","title":"Estimators' Cheat Sheet","text":"Semiparametric doubly robust targeted double machine learning: a review.\nIntroduction to Modern Causal Inference.\nTargeted Learning, Causal Inference for Observational and Experimental Data.\nSTATS 361: Causal Inference","category":"page"},{"location":"user_guide/estimands/#Estimands","page":"Estimands","title":"Estimands","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Most causal questions can be translated into a causal estimand. Usually either an interventional or counterfactual quantity. What would have been the outcome if I had set this variable to this value? When identified, this causal estimand translates to a statistical estimand which can be estimated from data. From a mathematical standpoint, an estimand (Psi) is a functional, that is a function that takes as input a probability distribution (from a model mathcalM), and outputs a real number or vector of real numbers.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Psi mathcalM rightarrow mathbbR^p","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"At the moment, most of the work in this package has been focused on estimands that are composite functions of the counterfactual mean which is easily identified via backdoor adjustment and for which the gradient is well known.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"In what follows, P is a probability distribution generating an outcome Y, a random vector of \"treatment\" variables textbfT and a random vector of \"confounding\" variables textbfW. For the examples, we will assume two treatment variables T₁ and T₂ taking either values 0 or 1. The SCM is given by:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"using TMLE\nscm = StaticSCM(\n    outcomes=[:Y], \n    treatments=[:T₁, :T₂], \n    confounders=[:W]\n)","category":"page"},{"location":"user_guide/estimands/#The-Counterfactual-Mean-(CM)","page":"Estimands","title":"The Counterfactual Mean (CM)","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Question:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"What would have been the mean of Y had we set textbfT=textbft?","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Estimand:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"CM_textbft(P) = mathbbEYdo(textbfT=textbft)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Statistical Estimand (via backdoor adjustment):","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"CM_textbft(P) = mathbbE_textbfWmathbbEYtextbfT=textbft textbfW","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"TMLE.jl Example","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A causal estimand is given by:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"causalΨ = CM(outcome=:Y, treatment_values=(T₁=1, T₂=0))","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A corresponding statistical estimand can be identified via backdoor adjustment using the scm:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = identify(causalΨ, scm)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"or defined directly:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = CM(\n    outcome=:Y, \n    treatment_values=(T₁=1, T₂=0),\n    treatment_confounders=(T₁=[:W], T₂=[:W])\n)","category":"page"},{"location":"user_guide/estimands/#The-Average-Treatment-Effect","page":"Estimands","title":"The Average Treatment Effect","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Question:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"What would be the average difference on Y if we switch the treatment levels from textbft_1 to textbft_2?","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Estimand:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"ATE_textbft_1 rightarrow textbft_2(P) = mathbbEYdo(textbfT=textbft_2) - mathbbEYdo(textbfT=textbft_1)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Statistical Estimand (via backdoor adjustment):","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"beginaligned\nATE_textbft_1 rightarrow textbft_2(P) = CM_textbft_2(P) - CM_textbft_1(P) \n= mathbbE_textbfWmathbbEYtextbfT=textbft_2 textbfW - mathbbEYtextbfT=textbft_1 textbfW\nendaligned","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"TMLE.jl Example","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A causal estimand is given by:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"causalΨ = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    )\n)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A corresponding statistical estimand can be identified via backdoor adjustment using the scm:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = identify(causalΨ, scm)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"or defined directly:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = ATE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    ),\n    treatment_confounders=(T₁=[:W], T₂=[:W])\n)","category":"page"},{"location":"user_guide/estimands/#The-Average-Interaction-Effect","page":"Estimands","title":"The Average Interaction Effect","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Question:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Interactions can be defined up to any order but we restrict the interpretation to two variables. Is the Total Average Treatment Effect of T₁ and T₂ different from the sum of their respective marginal Average Treatment Effects? Is there a synergistic additive effect between T₁ and T₂ on Y.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"For a general higher-order definition, please refer to Higher-order interactions in statistical physics and machine learning: A model-independent solution to the inverse problem at equilibrium.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Causal Estimand:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"For two points interaction with both treatment and control levels 0 and 1 for ease of notation:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"AIE_0 rightarrow 1 0 rightarrow 1(P) = mathbbEYdo(T_1=1 T_2=1) - mathbbEYdo(T_1=1 T_2=0)  \n- mathbbEYdo(T_1=0 T_2=1) + mathbbEYdo(T_1=0 T_2=0) ","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Statistical Estimand (via backdoor adjustment):","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"AIE_0 rightarrow 1 0 rightarrow 1(P) = mathbbE_textbfWmathbbEYT_1=1 T_2=1 textbfW - mathbbEYT_1=1 T_2=0 textbfW  \n- mathbbEYT_1=0 T_2=1 textbfW + mathbbEYT_1=0 T_2=0 textbfW ","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"TMLE.jl Example","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A causal estimand is given by:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"causalΨ = AIE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    )\n)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A corresponding statistical estimand can be identified via backdoor adjustment using the scm:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = identify(causalΨ, scm)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"or defined directly:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"statisticalΨ = AIE(\n    outcome=:Y, \n    treatment_values=(\n        T₁=(case=1, control=0), \n        T₂=(case=1, control=0)\n    ),\n    treatment_confounders=(T₁=[:W], T₂=[:W])\n)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Factorial Treatments","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"It is possible to generate a JointEstimand containing all linearly independent AIEs from a set of treatment values or from a dataset. For that purpose, use the factorialEstimand function.","category":"page"},{"location":"user_guide/estimands/#Joint-And-Composed-Estimands","page":"Estimands","title":"Joint And Composed Estimands","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"A JointEstimand is simply a list of one dimensional estimands that are grouped together. For instance for a treatment T taking three possible values (0 1 2) we can define the two following Average Treatment Effects and a corresponding JointEstimand:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"ATE₁ = ATE(\n    outcome = :Y, \n    treatment_values = (T = (control = 0, case = 1),),\n    treatment_confounders = [:W]\n    )\nATE₂ = ATE(\n    outcome = :Y, \n    treatment_values = (T = (control = 1, case = 2),),\n    treatment_confounders = [:W]\n    )\njoint_estimand = JointEstimand(ATE₁, ATE₂)","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"You can easily generate joint estimands corresponding to Counterfactual Means, Average Treatment Effects or Average Interaction Effects by using the factorialEstimand function.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"To estimate a joint estimand you can use any of the estimators defined in this package exactly as you would do it for a one dimensional estimand.","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"There are two main use cases for them that we now describe.","category":"page"},{"location":"user_guide/estimands/#Joint-Testing","page":"Estimands","title":"Joint Testing","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"In some cases, like in factorial analyses where multiple versions of a treatment are tested, it may be of interest to know if any version of the versions has had an effect. This can be done via a Hotelling's T2 Test, which is simply a multivariate generalisation of the Student's T test. This is the default returned by the significance_test function provided in TMLE.jl and the result of the test is also printed to the REPL for any joint estimate.","category":"page"},{"location":"user_guide/estimands/#Composition","page":"Estimands","title":"Composition","text":"","category":"section"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"Once you have estimated a JointEstimand and have a JointEstimate, you may be interested to ask further questions. For instance whether two treatment versions have the same effect. This question is typically answered by testing if the difference in Average Treatment Effect is 0. Using the Delta Method and Julia's automatic differentiation, you don't need to explicitly define a semi-parametric estimator for it. You can simply call compose:","category":"page"},{"location":"user_guide/estimands/","page":"Estimands","title":"Estimands","text":"ATEdiff = compose(x -> x[2] - x[1], joint_estimate)","category":"page"},{"location":"resources/#Resources","page":"Learning Resources","title":"Resources","text":"","category":"section"},{"location":"resources/","page":"Learning Resources","title":"Learning Resources","text":"Targeted Learning is a difficult topic, while it is not strictly necessary to understand the details to use this package it can certainly help. Here is an incomplete list of external resources that I found useful in my journey.","category":"page"},{"location":"resources/#Websites","page":"Learning Resources","title":"Websites","text":"","category":"section"},{"location":"resources/","page":"Learning Resources","title":"Learning Resources","text":"These are two very clear introductions to causal inference and semi-parametric estimation:","category":"page"},{"location":"resources/","page":"Learning Resources","title":"Learning Resources","text":"Introduction to Modern Causal Inference (Alejandro Schuler, Mark J. van der Laan).\nA Ride in Targeted Learning Territory (David Benkeser, Antoine Chambaz).","category":"page"},{"location":"resources/#Youtube","page":"Learning Resources","title":"Youtube","text":"","category":"section"},{"location":"resources/","page":"Learning Resources","title":"Learning Resources","text":"Targeted Learning Webinar Series\nTL Briefs","category":"page"},{"location":"resources/#Books-and-Lecture-Notes","page":"Learning Resources","title":"Books and Lecture Notes","text":"","category":"section"},{"location":"resources/","page":"Learning Resources","title":"Learning Resources","text":"Targeted Learning (Mark J. van der Laan, Sherri Rose).\nSTATS 361: Causal Inference","category":"page"},{"location":"resources/#Journal-articles","page":"Learning Resources","title":"Journal articles","text":"","category":"section"},{"location":"resources/","page":"Learning Resources","title":"Learning Resources","text":"Semiparametric doubly robust targeted double machine learning: a review (Edward H. Kennedy).","category":"page"},{"location":"user_guide/missingness/#Missingness","page":"Missingness","title":"Missingness","text":"","category":"section"},{"location":"user_guide/missingness/","page":"Missingness","title":"Missingness","text":"Strictly speaking, there is not much missingness support at the moment, rows containing missing values in the variables necessary for estimation are dropped.","category":"page"},{"location":"user_guide/missingness/","page":"Missingness","title":"Missingness","text":"However, when no cross-validation scheme is used we make sure the propensity score estimation uses as much data as possible. This is because the propensity score g(T W) = p(TW) only depends on (T W). That is, missing values in the outcome Y or extra covariates C should not impact its estimation. ","category":"page"},{"location":"user_guide/missingness/","page":"Missingness","title":"Missingness","text":"If you estimate the effect of some treatment on multiple outcomes, this can be handy because you can make maximal use of the propensity score's related data and reuse it across all outcomes (with the cache).","category":"page"},{"location":"#Home","page":"Home","title":"Home","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Most scientific questions are causal and can be answered by a finite dimensional causal estimand. Perhaps the most famous of them is the Average Treatment Effect. If certain conditions are met (no unobserved confounders, overlap), statistical methods can be employed to estimate these causal estimands from data. Semi-parametric methods have gained interest in the past decade because they are more likely to capture the true generating process than their restricted parametric counterparts. This is particularly true in modern days data science where datasets are increasingly complex and unlikely to be well represented by parametric models. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"TMLE.jl implements such semi-parametric methods. Precisely, it is an implementation of the Targeted Minimum Loss-Based Estimation (TMLE) framework. If you are interested in leveraging the power of modern machine-learning methods while preserving interpretability and statistical inference guarantees, you are in the right place. TMLE.jl is compatible with any MLJ compliant algorithm and any dataset wrapped in a DataFrame object.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The following plot illustrates the bias reduction achieved by TMLE over a mis-specified parametric linear model in the presence of confounding. Note that in this case, TMLE also uses mis-specified models but still achieves a lower bias due to the targeting step.","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: Home Illustration)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"TMLE.jl can be installed via the Package Manager and supports Julia v1.10 and greater.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg> add TMLE","category":"page"},{"location":"#Quick-Start","page":"Home","title":"Quick Start","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To run an estimation procedure, we need 3 ingredients:","category":"page"},{"location":"#1.-A-dataset:-here-a-simulation-dataset","page":"Home","title":"1. A dataset: here a simulation dataset","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"For illustration, assume we know the actual data generating process is as follows:","category":"page"},{"location":"","page":"Home","title":"Home","text":"beginaligned\nW  sim mathcalUniform(0 1) \nT  sim mathcalBernoulli(logistic(1-2 cdot W)) \nY  sim mathcalNormal(1 + 3 cdot T - T cdot W 001)\nendaligned","category":"page"},{"location":"","page":"Home","title":"Home","text":"Because we know the data generating process, we can simulate some data accordingly:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using TMLE\nusing Distributions\nusing StableRNGs\nusing Random\nusing CategoricalArrays\nusing MLJLinearModels\nusing LogExpFunctions\nusing DataFrames\n\nrng = StableRNG(123)\nn = 100\nW = rand(rng, Uniform(), n)\nT = rand(rng, Uniform(), n) .< logistic.(1 .- 2W)\nY = 1 .+ 3T .- T.*W .+ rand(rng, Normal(0, 0.01), n)\ndataset = DataFrame(Y=Y, T=categorical(T), W=W)\nnothing # hide","category":"page"},{"location":"#2.-A-quantity-of-interest:-here-the-Average-Treatment-Effect-(ATE)","page":"Home","title":"2. A quantity of interest: here the Average Treatment Effect (ATE)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The Average Treatment Effect of T on Y confounded by W is defined as:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Ψ = ATE(\n    outcome=:Y, \n    treatment_values=(T=(case=true, control = false),), \n    treatment_confounders=(T=[:W],)\n)","category":"page"},{"location":"#3.-An-estimator:-here-a-Targeted-Maximum-Likelihood-Estimator-(TMLE)","page":"Home","title":"3. An estimator: here a Targeted Maximum Likelihood Estimator (TMLE)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"tmle = Tmle()\nresult, _ = tmle(Ψ, dataset, verbosity=0);\nresult","category":"page"},{"location":"","page":"Home","title":"Home","text":"We are comforted to see that our estimator covers the ground truth! 🥳","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Test # hide\n@test pvalue(OneSampleTTest(result, 2.5)) > 0.05 # hide\nnothing # hide","category":"page"},{"location":"#Scope-and-Distinguishing-Features","page":"Home","title":"Scope and Distinguishing Features","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The goal of this package is to provide an entry point for semi-parametric asymptotic unbiased and efficient estimation in Julia. The two main general estimators that are known to achieve these properties are the One-Step estimator and the Targeted Maximum-Likelihood estimator. Most of the current effort has been centered around estimands that are composite of the counterfactual mean.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Distinguishing Features:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Estimands: Counterfactual Mean, Average Treatment Effect, Interactions, Any composition thereof\nEstimators: TMLE, CV-TMLE, C-TMLE, One-Step, CV-One-Step.\nMachine-Learning: Any MLJ compatible model\nDataset: Any dataset wrapped in a DataFrame.\nFactorial Treatment Variables:\nMultiple treatments\nCategorical treatment values","category":"page"},{"location":"#Citing-TMLE.jl","page":"Home","title":"Citing TMLE.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"If you use TMLE.jl for your own work and would like to cite us, here are the BibTeX and APA formats:","category":"page"},{"location":"","page":"Home","title":"Home","text":"BibTeX","category":"page"},{"location":"","page":"Home","title":"Home","text":"@software{Labayle_TMLE_jl,\n    author = {Labayle, Olivier and Khamseh, Ava and Ponting, Chris and Beentjes, Sjoerd},\n    title = {{TMLE.jl}},\n    url = {https://github.com/olivierlabayle/TMLE.jl}\n}","category":"page"},{"location":"","page":"Home","title":"Home","text":"APA","category":"page"},{"location":"","page":"Home","title":"Home","text":"Labayle, O., Beentjes, S., Khamseh, A., & Ponting, C. TMLE.jl [Computer software]. https://github.com/olivierlabayle/TMLE.jl","category":"page"}]
}
